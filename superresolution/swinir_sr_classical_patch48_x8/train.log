22-11-05 11:47:37.286 :   task: swinir_sr_classical_patch48_x8
  model: plain
  gpu_ids: [0, 1]
  dist: False
  scale: 8
  n_channels: 3
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netE: None
    task: superresolution/swinir_sr_classical_patch48_x8
    log: superresolution/swinir_sr_classical_patch48_x8
    options: superresolution/swinir_sr_classical_patch48_x8/options
    models: superresolution/swinir_sr_classical_patch48_x8/models
    images: superresolution/swinir_sr_classical_patch48_x8/images
    pretrained_optimizerG: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/WIN/train-HR
      dataroot_L: trainsets/WIN/train-LRx8
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 16
      dataloader_batch_size: 24
      phase: train
      scale: 8
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/WIN/test-HR-100
      dataroot_L: testsets/WIN/test-LRx8-100
      phase: test
      scale: 8
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 8
    in_chans: 3
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 8
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/swinir/train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  use_static_graph: False
  num_gpu: 2
  rank: 0
  world_size: 1

22-11-05 11:47:37.622 : Number of train images: 7,142, iters: 298
22-11-05 11:47:40.886 :   task: swinir_sr_classical_patch48_x8
  model: plain
  gpu_ids: [0, 1]
  dist: False
  scale: 8
  n_channels: 3
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netE: None
    task: superresolution/swinir_sr_classical_patch48_x8
    log: superresolution/swinir_sr_classical_patch48_x8
    options: superresolution/swinir_sr_classical_patch48_x8/options
    models: superresolution/swinir_sr_classical_patch48_x8/models
    images: superresolution/swinir_sr_classical_patch48_x8/images
    pretrained_optimizerG: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/WIN/train-HR
      dataroot_L: trainsets/WIN/train-LRx8
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 16
      dataloader_batch_size: 24
      phase: train
      scale: 8
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/WIN/test-HR-100
      dataroot_L: testsets/WIN/test-LRx8-100
      phase: test
      scale: 8
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 8
    in_chans: 3
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 8
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/swinir/train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  use_static_graph: False
  num_gpu: 2
  rank: 0
  world_size: 1

22-11-05 11:47:41.043 : Number of train images: 7,142, iters: 298
22-11-05 11:47:44.837 : 
Networks name: SwinIR
Params number: 12047911
Net structure:
SwinIR(
  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.003)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.006)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.011)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.014)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.017)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.020)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.023)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.026)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.029)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.031)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.034)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.037)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.040)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.043)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.046)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.049)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.051)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.054)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.057)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.060)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.063)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.066)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.069)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.071)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.074)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.077)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.080)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.083)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.086)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.089)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.091)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.094)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.097)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): PixelShuffle(upscale_factor=2)
    (4): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-11-05 11:47:45.287 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.000 | -0.192 |  0.192 |  0.111 | torch.Size([180, 3, 3, 3]) || conv_first.weight
 | -0.001 | -0.191 |  0.192 |  0.115 | torch.Size([180]) || conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.090 |  0.058 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.093 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.077 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.089 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.081 |  0.078 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.066 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.086 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.081 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.083 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.088 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.056 |  0.061 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.099 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.083 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.083 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.090 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.070 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.083 |  0.078 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.080 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.098 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.091 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.064 |  0.067 |  0.021 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.083 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.082 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.087 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.084 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.061 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.086 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.074 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.084 |  0.075 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.083 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.068 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.081 |  0.096 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.080 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.081 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.087 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.058 |  0.055 |  0.019 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.092 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.086 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.106 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.087 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.070 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.082 |  0.097 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.088 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.081 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.081 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.067 |  0.057 |  0.019 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.087 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.084 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.094 |  0.078 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.080 |  0.100 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.060 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.091 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.083 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.086 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.086 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.070 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.085 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.089 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.078 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.085 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 | -0.001 | -0.025 |  0.025 |  0.013 | torch.Size([180]) || layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.072 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.095 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.083 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.092 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.076 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.069 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.083 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.073 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 | -0.000 | -2.000 |  0.087 |  0.021 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.092 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.071 |  0.055 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.095 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.077 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.086 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.077 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.058 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.086 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.074 |  0.074 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.094 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.078 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.072 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.093 |  0.095 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.085 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.085 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.091 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.060 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.086 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.085 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.087 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.086 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 | -0.001 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.095 |  0.076 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.101 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.085 |  0.106 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.081 |  0.077 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.092 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.078 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.083 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.086 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.089 |  0.097 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.091 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.069 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.088 |  0.103 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.080 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.086 |  0.079 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.085 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.065 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.078 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.083 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.085 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.096 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.075 |  0.084 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.089 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.084 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.084 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.083 |  0.098 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.067 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.093 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.080 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.091 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.084 |  0.095 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 | -0.002 | -0.025 |  0.024 |  0.014 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.067 |  0.098 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.089 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.082 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.083 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.084 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.069 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.084 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.090 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.081 |  0.093 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.084 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.056 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.084 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.091 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.079 |  0.100 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.091 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.068 |  0.059 |  0.019 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.087 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.073 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.078 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.087 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.068 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.082 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.074 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.075 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.085 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.074 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.085 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.086 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.089 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.085 |  0.096 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 | -0.001 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.4.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.074 |  0.078 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.093 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.082 |  0.073 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.086 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.083 |  0.076 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.058 |  0.071 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.090 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.081 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.095 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.081 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.068 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.092 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.083 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.092 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.089 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.065 |  0.068 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.082 |  0.097 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.079 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.097 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.080 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.081 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.089 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.095 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.085 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.094 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.071 |  0.072 |  0.019 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.085 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.076 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.081 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.090 |  0.106 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 | -0.001 | -0.024 |  0.025 |  0.013 | torch.Size([180]) || layers.5.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || norm.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 |  0.000 | -0.025 |  0.023 |  0.015 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 | -0.001 | -0.024 |  0.024 |  0.014 | torch.Size([64]) || conv_before_upsample.0.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 | -0.001 | -0.042 |  0.042 |  0.024 | torch.Size([256]) || upsample.0.bias
 | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.2.weight
 | -0.000 | -0.041 |  0.042 |  0.023 | torch.Size([256]) || upsample.2.bias
 | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.4.weight
 | -0.002 | -0.041 |  0.042 |  0.024 | torch.Size([256]) || upsample.4.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([3, 64, 3, 3]) || conv_last.weight
 | -0.010 | -0.041 |  0.030 |  0.036 | torch.Size([3]) || conv_last.bias

22-11-05 11:47:47.544 : 
Networks name: SwinIR
Params number: 12047911
Net structure:
SwinIR(
  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.003)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.006)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.011)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.014)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.017)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.020)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.023)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.026)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.029)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.031)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.034)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.037)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.040)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.043)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.046)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.049)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.051)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.054)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.057)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.060)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.063)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.066)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.069)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.071)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.074)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.077)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.080)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.083)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.086)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.089)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.091)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.094)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.097)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): PixelShuffle(upscale_factor=2)
    (4): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-11-05 11:47:47.634 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.002 | -0.192 |  0.192 |  0.112 | torch.Size([180, 3, 3, 3]) || conv_first.weight
 | -0.000 | -0.189 |  0.190 |  0.110 | torch.Size([180]) || conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.065 |  0.061 |  0.021 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.083 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.080 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.086 |  0.076 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.088 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.065 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.092 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.092 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.089 |  0.102 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.084 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.065 |  0.067 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.089 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.084 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.081 |  0.079 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.082 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.077 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.081 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.092 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.093 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.080 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.069 |  0.075 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.087 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.078 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.088 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.087 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.060 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.094 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.088 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.095 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.086 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 | -0.001 | -0.025 |  0.024 |  0.013 | torch.Size([180]) || layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.074 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.085 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.094 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.083 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.080 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.072 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.085 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.087 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.089 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.082 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.081 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.103 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.077 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.083 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.086 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.066 |  0.076 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.084 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.079 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.091 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.084 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.063 |  0.086 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.082 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.085 |  0.074 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.099 |  0.092 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.084 |  0.091 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.068 |  0.056 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.083 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.081 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.079 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.082 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 | -0.001 | -0.024 |  0.025 |  0.014 | torch.Size([180]) || layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.060 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.086 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.081 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.082 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.083 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.083 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.096 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.082 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.083 |  0.099 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.095 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.066 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.080 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.085 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.083 |  0.078 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.084 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.071 |  0.070 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.093 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.082 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.086 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.082 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.060 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.081 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.082 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.084 |  0.095 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.097 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.065 |  0.057 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.090 |  0.100 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.085 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.085 |  0.078 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.087 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 | -0.002 | -0.024 |  0.024 |  0.013 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.066 |  0.064 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.092 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.086 |  0.095 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.086 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.089 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.059 |  0.065 |  0.019 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.080 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.087 |  0.093 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.080 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.089 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.068 |  0.056 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.087 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.077 |  0.100 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.091 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.087 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.064 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.100 |  0.106 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.083 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.083 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.089 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.063 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.085 |  0.097 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.088 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.083 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.098 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.066 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.076 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.075 |  0.074 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.085 |  0.094 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.089 |  0.091 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 | -0.001 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.065 |  0.062 |  0.019 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.089 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.095 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.098 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.085 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.068 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.089 |  0.095 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.090 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.102 |  0.077 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.083 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.063 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.088 |  0.078 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.077 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.076 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.096 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.065 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.087 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.089 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.084 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.081 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.059 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.091 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.083 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.087 |  0.094 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.083 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.073 |  0.065 |  0.019 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.093 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.078 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.080 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.082 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 |  0.001 | -0.024 |  0.025 |  0.014 | torch.Size([180]) || layers.4.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.070 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.090 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.079 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.085 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.090 |  0.092 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.064 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.094 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.079 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.088 |  0.078 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.087 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.060 |  0.069 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.087 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.085 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.081 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.106 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.072 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.087 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.082 |  0.073 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.078 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.076 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.067 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.088 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.080 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.082 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.084 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.077 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.083 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.080 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.092 |  0.095 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.091 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 |  0.001 | -0.024 |  0.025 |  0.014 | torch.Size([180]) || layers.5.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || norm.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.001 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || conv_after_body.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 | -0.000 | -0.025 |  0.024 |  0.014 | torch.Size([64]) || conv_before_upsample.0.bias
 | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 | -0.001 | -0.042 |  0.042 |  0.024 | torch.Size([256]) || upsample.0.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.2.weight
 | -0.001 | -0.041 |  0.041 |  0.024 | torch.Size([256]) || upsample.2.bias
 | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.4.weight
 | -0.000 | -0.041 |  0.041 |  0.025 | torch.Size([256]) || upsample.4.bias
 |  0.001 | -0.042 |  0.041 |  0.024 | torch.Size([3, 64, 3, 3]) || conv_last.weight
 |  0.004 | -0.015 |  0.018 |  0.017 | torch.Size([3]) || conv_last.bias

22-11-05 11:50:47.695 :   task: swinir_sr_classical_patch48_x8
  model: plain
  gpu_ids: [0, 1]
  dist: False
  scale: 8
  n_channels: 3
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netE: None
    task: superresolution/swinir_sr_classical_patch48_x8
    log: superresolution/swinir_sr_classical_patch48_x8
    options: superresolution/swinir_sr_classical_patch48_x8/options
    models: superresolution/swinir_sr_classical_patch48_x8/models
    images: superresolution/swinir_sr_classical_patch48_x8/images
    pretrained_optimizerG: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/WIN/train-HR
      dataroot_L: trainsets/WIN/train-LRx8
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 16
      dataloader_batch_size: 24
      phase: train
      scale: 8
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/WIN/test-HR-100
      dataroot_L: testsets/WIN/test-LRx8-100
      phase: test
      scale: 8
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 8
    in_chans: 3
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 8
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/swinir/train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  use_static_graph: False
  num_gpu: 2
  rank: 0
  world_size: 1

22-11-05 11:50:47.797 : Number of train images: 7,142, iters: 298
22-11-05 11:50:49.270 :   task: swinir_sr_classical_patch48_x8
  model: plain
  gpu_ids: [0, 1]
  dist: False
  scale: 8
  n_channels: 3
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netE: None
    task: superresolution/swinir_sr_classical_patch48_x8
    log: superresolution/swinir_sr_classical_patch48_x8
    options: superresolution/swinir_sr_classical_patch48_x8/options
    models: superresolution/swinir_sr_classical_patch48_x8/models
    images: superresolution/swinir_sr_classical_patch48_x8/images
    pretrained_optimizerG: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/WIN/train-HR
      dataroot_L: trainsets/WIN/train-LRx8
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 16
      dataloader_batch_size: 24
      phase: train
      scale: 8
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/WIN/test-HR-100
      dataroot_L: testsets/WIN/test-LRx8-100
      phase: test
      scale: 8
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 8
    in_chans: 3
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 8
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/swinir/train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  use_static_graph: False
  num_gpu: 2
  rank: 0
  world_size: 1

22-11-05 11:50:49.948 : Number of train images: 7,142, iters: 298
22-11-05 11:50:55.085 : 
Networks name: SwinIR
Params number: 12047911
Net structure:
SwinIR(
  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.003)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.006)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.011)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.014)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.017)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.020)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.023)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.026)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.029)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.031)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.034)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.037)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.040)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.043)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.046)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.049)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.051)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.054)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.057)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.060)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.063)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.066)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.069)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.071)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.074)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.077)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.080)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.083)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.086)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.089)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.091)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.094)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.097)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): PixelShuffle(upscale_factor=2)
    (4): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-11-05 11:50:55.383 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.002 | -0.192 |  0.192 |  0.111 | torch.Size([180, 3, 3, 3]) || conv_first.weight
 | -0.005 | -0.192 |  0.191 |  0.111 | torch.Size([180]) || conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.059 |  0.068 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.088 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.079 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.093 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.092 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.057 |  0.075 |  0.021 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.093 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.081 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.084 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.080 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.064 |  0.084 |  0.021 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.085 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.082 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.087 |  0.078 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.079 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.063 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.088 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.090 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.087 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.085 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.068 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.089 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.083 |  0.074 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.086 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.081 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.079 |  0.065 |  0.021 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.090 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.080 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.086 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.093 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 | -0.001 | -0.024 |  0.025 |  0.014 | torch.Size([180]) || layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.058 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.088 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.082 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.085 |  0.091 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.091 |  0.092 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.060 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.088 |  0.106 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.086 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.087 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.087 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.064 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.091 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.083 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.082 |  0.095 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.077 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.056 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.081 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.079 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.084 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.093 |  0.102 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.060 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.087 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.080 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.081 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.101 |  0.099 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.067 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.082 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.078 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.082 |  0.097 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.078 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 | -0.000 | -0.024 |  0.024 |  0.014 | torch.Size([180]) || layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.066 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.086 |  0.079 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.082 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.087 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.093 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.078 |  0.058 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.080 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.075 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.078 |  0.076 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.077 |  0.092 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.063 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.095 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.075 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.086 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.106 |  0.092 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.073 |  0.066 |  0.019 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.086 |  0.079 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.088 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.084 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.083 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.075 |  0.081 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.085 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.076 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.081 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.084 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.058 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.081 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.079 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.084 |  0.097 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.084 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 |  0.001 | -0.024 |  0.025 |  0.014 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.062 |  0.075 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.084 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.085 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.087 |  0.093 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.091 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.058 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.091 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.096 |  0.098 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.080 |  0.092 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.087 |  0.077 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.069 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.088 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.080 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.084 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.088 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.065 |  0.074 |  0.019 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.082 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.086 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.091 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.087 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.067 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.080 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.081 |  0.074 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.089 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.082 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.060 |  0.056 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.086 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.085 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.078 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.089 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 | -0.001 | -0.024 |  0.025 |  0.014 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.067 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.082 |  0.102 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.080 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.086 |  0.078 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.085 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.060 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.091 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.078 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.087 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.087 |  0.096 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.066 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.085 |  0.097 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.081 |  0.074 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.082 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.088 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.064 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.088 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.077 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.078 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.085 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.066 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.100 |  0.079 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.079 |  0.094 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.080 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.082 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.060 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.094 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.078 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.086 |  0.094 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.091 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 | -0.001 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.4.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.060 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.085 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.078 |  0.073 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.087 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.080 |  0.095 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.069 |  0.081 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.088 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.083 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.080 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.086 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.076 |  0.059 |  0.021 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.085 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.077 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.088 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.084 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.056 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.090 |  0.093 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.085 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.087 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.085 |  0.095 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.074 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.083 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.076 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.086 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.091 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.070 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.084 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.076 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.096 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.080 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.5.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || norm.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.000 | -0.025 |  0.024 |  0.014 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 |  0.001 | -0.025 |  0.024 |  0.014 | torch.Size([64]) || conv_before_upsample.0.bias
 | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 | -0.001 | -0.041 |  0.041 |  0.024 | torch.Size([256]) || upsample.0.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.2.weight
 |  0.000 | -0.041 |  0.041 |  0.024 | torch.Size([256]) || upsample.2.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.4.weight
 |  0.000 | -0.042 |  0.041 |  0.025 | torch.Size([256]) || upsample.4.bias
 | -0.001 | -0.042 |  0.042 |  0.024 | torch.Size([3, 64, 3, 3]) || conv_last.weight
 |  0.005 | -0.030 |  0.024 |  0.030 | torch.Size([3]) || conv_last.bias

22-11-05 11:50:56.912 : 
Networks name: SwinIR
Params number: 12047911
Net structure:
SwinIR(
  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.003)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.006)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.011)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.014)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.017)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.020)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.023)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.026)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.029)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.031)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.034)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.037)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.040)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.043)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.046)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.049)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.051)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.054)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.057)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.060)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.063)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.066)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.069)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.071)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.074)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.077)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.080)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.083)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.086)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.089)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.091)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.094)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.097)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): PixelShuffle(upscale_factor=2)
    (4): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-11-05 11:50:57.094 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.001 | -0.192 |  0.192 |  0.111 | torch.Size([180, 3, 3, 3]) || conv_first.weight
 |  0.013 | -0.192 |  0.192 |  0.104 | torch.Size([180]) || conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.074 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.087 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.089 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.082 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.086 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.061 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.086 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.082 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.084 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.089 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.078 |  0.065 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.084 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.080 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.096 |  0.099 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.083 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.072 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.088 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.080 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.085 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.088 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.063 |  0.053 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.083 |  0.095 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.075 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.082 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.091 |  0.090 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 | -0.000 | -0.065 |  0.058 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.102 |  0.095 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.081 |  0.093 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.079 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.079 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 | -0.000 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.085 |  0.077 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.084 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.078 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.089 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.084 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.086 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.098 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.089 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.084 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.087 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.068 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.090 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.079 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.078 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.079 |  0.078 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.064 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.083 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.077 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.082 |  0.093 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.084 |  0.078 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.084 |  0.058 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.088 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.092 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.078 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.084 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.072 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.087 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.079 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.082 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.088 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 | -0.002 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.059 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.085 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.088 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.090 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.091 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.068 |  0.070 |  0.021 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.093 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.084 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.084 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.087 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.062 |  0.073 |  0.019 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -2.000 |  0.088 |  0.021 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.080 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.078 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.091 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.062 |  0.078 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.085 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.085 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.081 |  0.090 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.088 |  0.077 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.063 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.094 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.082 |  0.075 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.081 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.080 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.064 |  0.083 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.085 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.089 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.101 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.084 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 | -0.001 | -0.024 |  0.025 |  0.014 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.074 |  0.063 |  0.021 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.083 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.082 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.086 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.083 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 | -0.001 | -0.061 |  0.055 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.089 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.078 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.088 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.089 |  0.099 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.061 |  0.060 |  0.019 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.086 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.082 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.091 |  0.095 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.092 |  0.092 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.058 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.087 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.079 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.087 |  0.077 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.080 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.061 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.084 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.076 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.106 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.086 |  0.098 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.061 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.082 |  0.080 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.085 |  0.088 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.087 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.078 |  0.078 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 |  0.001 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.075 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.092 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.082 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.084 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.086 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.065 |  0.063 |  0.019 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.087 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.093 |  0.090 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.082 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.080 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.058 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.088 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.088 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.086 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.082 |  0.078 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.058 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.089 |  0.098 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.086 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.083 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.097 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.064 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.088 |  0.098 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.085 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.083 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.091 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.071 |  0.065 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.096 |  0.097 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.082 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.087 |  0.095 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.094 |  0.076 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 | -0.001 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.4.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 |  0.001 | -0.059 |  0.080 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.089 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.078 |  0.092 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.080 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.088 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.059 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.089 |  0.097 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.079 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.094 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.091 |  0.083 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.063 |  0.074 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.083 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.080 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.086 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.083 |  0.094 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.064 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.087 |  0.096 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.085 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.093 |  0.095 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.079 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.061 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.084 |  0.083 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.089 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.080 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.083 |  0.091 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.066 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.081 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.082 |  0.093 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.085 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.089 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 |  0.000 | -0.025 |  0.025 |  0.015 | torch.Size([180]) || layers.5.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || norm.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.001 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 | -0.002 | -0.024 |  0.023 |  0.015 | torch.Size([64]) || conv_before_upsample.0.bias
 | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 |  0.001 | -0.042 |  0.041 |  0.025 | torch.Size([256]) || upsample.0.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.2.weight
 |  0.001 | -0.041 |  0.042 |  0.024 | torch.Size([256]) || upsample.2.bias
 | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.4.weight
 |  0.000 | -0.042 |  0.041 |  0.024 | torch.Size([256]) || upsample.4.bias
 |  0.000 | -0.042 |  0.042 |  0.024 | torch.Size([3, 64, 3, 3]) || conv_last.weight
 | -0.002 | -0.032 |  0.036 |  0.035 | torch.Size([3]) || conv_last.bias

22-11-05 11:55:13.961 : <epoch:  0, iter:     200, lr:2.000e-04> G_loss: 5.215e-02 
22-11-05 11:55:14.069 : <epoch:  0, iter:     200, lr:2.000e-04> G_loss: 4.676e-02 
22-11-05 11:59:08.089 : <epoch:  1, iter:     400, lr:2.000e-04> G_loss: 4.564e-02 
22-11-05 11:59:18.493 : <epoch:  1, iter:     400, lr:2.000e-04> G_loss: 4.288e-02 
22-11-05 12:03:37.774 :   task: swinir_sr_classical_patch48_x8
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 8
  n_channels: 3
  path:[
    root: superresolution
    pretrained_netG: None
    pretrained_netE: None
    task: superresolution/swinir_sr_classical_patch48_x8
    log: superresolution/swinir_sr_classical_patch48_x8
    options: superresolution/swinir_sr_classical_patch48_x8/options
    models: superresolution/swinir_sr_classical_patch48_x8/models
    images: superresolution/swinir_sr_classical_patch48_x8/images
    pretrained_optimizerG: None
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/WIN/train-HR
      dataroot_L: trainsets/WIN/train-LRx8
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 16
      dataloader_batch_size: 24
      phase: train
      scale: 8
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/WIN/test-HR-100
      dataroot_L: testsets/WIN/test-LRx8-100
      phase: test
      scale: 8
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 8
    in_chans: 3
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 8
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/swinir/train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  use_static_graph: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-11-05 12:03:37.863 : Number of train images: 7,142, iters: 298
22-11-05 12:04:08.070 : 
Networks name: SwinIR
Params number: 12047911
Net structure:
SwinIR(
  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.003)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.006)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.011)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.014)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.017)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.020)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.023)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.026)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.029)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.031)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.034)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.037)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.040)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.043)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.046)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.049)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.051)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.054)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.057)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.060)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.063)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.066)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.069)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.071)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.074)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.077)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.080)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.083)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.086)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.089)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.091)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.094)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.097)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): PixelShuffle(upscale_factor=2)
    (4): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-11-05 12:04:08.384 : 
 |  mean  |  min   |  max   |  std   || shape               
 | -0.000 | -0.192 |  0.192 |  0.111 | torch.Size([180, 3, 3, 3]) || conv_first.weight
 |  0.005 | -0.188 |  0.191 |  0.104 | torch.Size([180]) || conv_first.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || patch_embed.norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || patch_embed.norm.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.079 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.083 |  0.096 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.086 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.082 |  0.078 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.089 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.069 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.082 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.087 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.082 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.081 |  0.095 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.063 |  0.061 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.089 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.084 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.086 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.082 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.066 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.086 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.082 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.080 |  0.079 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.086 |  0.092 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 | -0.000 | -0.059 |  0.065 |  0.019 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.086 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.079 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 | -0.000 | -0.079 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.087 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.075 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.088 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.081 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.083 |  0.093 |  0.020 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.077 |  0.093 |  0.020 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 |  0.001 | -0.024 |  0.025 |  0.014 | torch.Size([180]) || layers.0.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.056 |  0.075 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.090 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.078 |  0.084 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.082 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.095 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.057 |  0.068 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.083 |  0.077 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.076 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.086 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.083 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.072 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.093 |  0.095 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.082 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.086 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.083 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 | -0.000 | -0.065 |  0.063 |  0.020 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.084 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.080 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.091 |  0.077 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 | -0.000 | -0.080 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.069 |  0.066 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.085 |  0.090 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.080 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.086 |  0.093 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.086 |  0.098 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.069 |  0.068 |  0.021 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.102 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.101 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.089 |  0.092 |  0.020 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.084 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 | -0.002 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.1.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 | -0.000 | -0.074 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.088 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.084 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.086 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.089 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.069 |  0.082 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.091 |  0.084 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.085 |  0.077 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.077 |  0.085 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.081 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.001 | -0.074 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.091 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.088 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.080 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.090 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.064 |  0.072 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.080 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.082 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.083 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.086 |  0.091 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.060 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.085 |  0.102 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.085 |  0.082 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.087 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.080 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.065 |  0.060 |  0.020 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.094 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.078 |  0.094 |  0.020 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.087 |  0.087 |  0.020 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.091 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 | -0.002 | -0.025 |  0.024 |  0.014 | torch.Size([180]) || layers.2.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.087 |  0.058 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.086 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.080 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.093 |  0.084 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.086 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 | -0.000 | -0.060 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.087 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.088 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.086 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 | -0.000 | -0.081 |  0.086 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 | -0.000 | -0.063 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.094 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.075 |  0.087 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.086 |  0.082 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.076 |  0.082 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.064 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.085 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.090 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.081 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.088 |  0.079 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 | -0.001 | -0.072 |  0.069 |  0.020 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.098 |  0.088 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.079 |  0.079 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.094 |  0.078 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.095 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 |  0.000 | -0.064 |  0.055 |  0.019 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.086 |  0.094 |  0.020 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.091 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.082 |  0.088 |  0.020 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.077 |  0.080 |  0.020 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 | -0.001 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || layers.3.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 |  0.000 | -0.076 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.091 |  0.091 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.079 |  0.093 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.090 |  0.080 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.086 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 |  0.001 | -0.065 |  0.064 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.086 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.080 |  0.083 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 |  0.000 | -0.099 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.090 |  0.087 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 |  0.000 | -0.070 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.084 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.079 |  0.080 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.091 |  0.078 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.083 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 |  0.000 | -0.060 |  0.066 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.089 |  0.082 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.082 |  0.091 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.082 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.086 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.057 |  0.062 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.091 |  0.087 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.078 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.086 |  0.083 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.083 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 |  0.001 | -0.072 |  0.059 |  0.020 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.090 |  0.098 |  0.020 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.083 |  0.085 |  0.020 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 | -0.000 | -0.092 |  0.081 |  0.020 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 |  0.000 | -0.083 |  0.084 |  0.020 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 |  0.001 | -0.024 |  0.025 |  0.014 | torch.Size([180]) || layers.4.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.062 |  0.073 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.085 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.085 |  0.078 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.090 |  0.094 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 | -0.000 | -0.082 |  0.085 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 |  0.000 | -0.056 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.102 |  0.086 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.075 |  0.086 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 | -0.000 | -0.085 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.081 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.071 |  0.071 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.088 |  0.085 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.076 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.083 |  0.092 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.080 |  0.089 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 |  0.001 | -0.067 |  0.067 |  0.020 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.081 |  0.092 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.077 |  0.081 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 |  0.000 | -0.089 |  0.086 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.099 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 |  0.000 | -0.060 |  0.075 |  0.019 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.090 |  0.089 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.080 |  0.076 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 |  0.000 | -0.078 |  0.089 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.102 |  0.088 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 | -0.001 | -0.073 |  0.068 |  0.019 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.098 |  0.081 |  0.020 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.076 |  0.089 |  0.020 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 |  0.000 | -0.077 |  0.093 |  0.020 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.095 |  0.081 |  0.020 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 |  0.002 | -0.024 |  0.025 |  0.014 | torch.Size([180]) || layers.5.conv.bias
 |  1.000 |  1.000 |  1.000 |  0.000 | torch.Size([180]) || norm.weight
 |  0.000 |  0.000 |  0.000 |  0.000 | torch.Size([180]) || norm.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 |  0.001 | -0.025 |  0.025 |  0.014 | torch.Size([180]) || conv_after_body.bias
 | -0.000 | -0.025 |  0.025 |  0.014 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 | -0.002 | -0.024 |  0.025 |  0.014 | torch.Size([64]) || conv_before_upsample.0.bias
 | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256]) || upsample.0.bias
 | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.2.weight
 | -0.001 | -0.041 |  0.042 |  0.024 | torch.Size([256]) || upsample.2.bias
 | -0.000 | -0.042 |  0.042 |  0.024 | torch.Size([256, 64, 3, 3]) || upsample.4.weight
 | -0.000 | -0.042 |  0.041 |  0.023 | torch.Size([256]) || upsample.4.bias
 | -0.000 | -0.042 |  0.042 |  0.025 | torch.Size([3, 64, 3, 3]) || conv_last.weight
 |  0.008 | -0.019 |  0.028 |  0.024 | torch.Size([3]) || conv_last.bias

22-11-05 12:06:53.039 : <epoch:  0, iter:     200, lr:2.000e-04> G_loss: 4.521e-02 
22-11-05 12:09:24.762 : <epoch:  1, iter:     400, lr:2.000e-04> G_loss: 4.860e-02 
22-11-05 12:11:55.558 : <epoch:  2, iter:     600, lr:2.000e-04> G_loss: 5.046e-02 
22-11-05 12:14:23.987 : <epoch:  2, iter:     800, lr:2.000e-04> G_loss: 4.086e-02 
22-11-05 12:16:54.657 : <epoch:  3, iter:   1,000, lr:2.000e-04> G_loss: 4.360e-02 
22-11-05 12:19:29.243 : <epoch:  4, iter:   1,200, lr:2.000e-04> G_loss: 3.575e-02 
22-11-05 12:21:58.758 : <epoch:  4, iter:   1,400, lr:2.000e-04> G_loss: 3.724e-02 
22-11-05 12:24:26.661 : <epoch:  5, iter:   1,600, lr:2.000e-04> G_loss: 3.807e-02 
22-11-05 12:26:57.465 : <epoch:  6, iter:   1,800, lr:2.000e-04> G_loss: 3.280e-02 
22-11-05 12:29:28.286 : <epoch:  6, iter:   2,000, lr:2.000e-04> G_loss: 4.729e-02 
22-11-05 12:31:55.156 : <epoch:  7, iter:   2,200, lr:2.000e-04> G_loss: 5.011e-02 
22-11-05 12:34:24.797 : <epoch:  8, iter:   2,400, lr:2.000e-04> G_loss: 3.804e-02 
22-11-05 12:36:56.655 : <epoch:  8, iter:   2,600, lr:2.000e-04> G_loss: 3.437e-02 
22-11-05 12:39:30.558 : <epoch:  9, iter:   2,800, lr:2.000e-04> G_loss: 4.378e-02 
22-11-05 12:42:04.254 : <epoch: 10, iter:   3,000, lr:2.000e-04> G_loss: 3.963e-02 
22-11-05 12:44:36.535 : <epoch: 10, iter:   3,200, lr:2.000e-04> G_loss: 3.638e-02 
22-11-05 12:47:11.130 : <epoch: 11, iter:   3,400, lr:2.000e-04> G_loss: 3.565e-02 
22-11-05 12:49:41.440 : <epoch: 12, iter:   3,600, lr:2.000e-04> G_loss: 3.448e-02 
22-11-05 12:52:06.803 : <epoch: 12, iter:   3,800, lr:2.000e-04> G_loss: 3.846e-02 
22-11-05 12:54:29.763 : <epoch: 13, iter:   4,000, lr:2.000e-04> G_loss: 3.409e-02 
22-11-05 12:57:00.709 : <epoch: 14, iter:   4,200, lr:2.000e-04> G_loss: 3.616e-02 
22-11-05 12:59:27.842 : <epoch: 14, iter:   4,400, lr:2.000e-04> G_loss: 3.756e-02 
22-11-05 13:01:54.384 : <epoch: 15, iter:   4,600, lr:2.000e-04> G_loss: 4.203e-02 
22-11-05 13:04:26.615 : <epoch: 16, iter:   4,800, lr:2.000e-04> G_loss: 4.558e-02 
22-11-05 13:06:55.660 : <epoch: 16, iter:   5,000, lr:2.000e-04> G_loss: 4.353e-02 
22-11-05 13:06:55.662 : Saving the model.
22-11-05 13:07:05.182 : ---1-->   0001.jpg | 22.84dB
22-11-05 13:07:13.924 : ---2-->   0002.jpg | 24.76dB
22-11-05 13:07:17.079 : ---3-->   0003.jpg | 28.86dB
22-11-05 13:07:20.568 : ---4-->   0004.jpg | 27.56dB
22-11-05 13:07:27.584 : ---5-->   0005.jpg | 27.47dB
22-11-05 13:07:31.655 : ---6-->   0006.jpg | 30.61dB
22-11-05 13:07:35.181 : ---7-->   0007.jpg | 30.24dB
22-11-05 13:07:43.247 : ---8-->   0008.jpg | 23.23dB
22-11-05 13:07:54.798 : ---9-->   0009.jpg | 22.93dB
22-11-05 13:07:59.568 : --10-->   0010.jpg | 26.86dB
22-11-05 13:08:03.031 : --11-->   0011.jpg | 24.58dB
22-11-05 13:08:09.727 : --12-->   0012.jpg | 23.90dB
22-11-05 13:08:18.326 : --13-->   0013.jpg | 23.52dB
22-11-05 13:08:21.118 : --14-->   0014.jpg | 22.13dB
22-11-05 13:08:25.364 : --15-->   0015.jpg | 20.10dB
22-11-05 13:08:32.278 : --16-->   0016.jpg | 27.92dB
22-11-05 13:08:39.537 : --17-->   0017.jpg | 27.15dB
22-11-05 13:08:42.624 : --18-->   0018.jpg | 26.47dB
22-11-05 13:08:51.470 : --19-->   0019.jpg | 30.54dB
22-11-05 13:08:58.550 : --20-->   0020.jpg | 23.10dB
22-11-05 13:09:04.884 : --21-->   0021.jpg | 25.49dB
22-11-05 13:09:10.743 : --22-->   0022.jpg | 34.87dB
22-11-05 13:09:18.455 : --23-->   0023.jpg | 24.37dB
22-11-05 13:09:24.218 : --24-->   0024.jpg | 31.37dB
22-11-05 13:09:26.438 : --25-->   0025.jpg | 24.91dB
22-11-05 13:09:30.110 : --26-->   0026.jpg | 24.54dB
22-11-05 13:09:33.932 : --27-->   0027.jpg | 28.14dB
22-11-05 13:09:37.127 : --28-->   0028.jpg | 28.43dB
22-11-05 13:09:40.768 : --29-->   0029.jpg | 23.91dB
22-11-05 13:09:44.861 : --30-->   0030.jpg | 28.05dB
22-11-05 13:09:47.458 : --31-->   0031.jpg | 27.43dB
22-11-05 13:09:51.776 : --32-->   0032.jpg | 24.57dB
22-11-05 13:09:53.026 : --33-->   0033.jpg | 22.40dB
22-11-05 13:09:58.352 : --34-->   0034.jpg | 22.77dB
22-11-05 13:10:03.638 : --35-->   0035.jpg | 26.03dB
22-11-05 13:10:12.655 : --36-->   0036.jpg | 22.89dB
22-11-05 13:10:16.687 : --37-->   0037.jpg | 22.06dB
22-11-05 13:10:17.666 : --38-->   0038.jpg | 24.90dB
22-11-05 13:10:19.747 : --39-->   0039.jpg | 23.23dB
22-11-05 13:10:23.773 : --40-->   0040.jpg | 27.43dB
22-11-05 13:10:27.518 : --41-->   0041.jpg | 20.45dB
22-11-05 13:10:35.208 : --42-->   0042.jpg | 26.05dB
22-11-05 13:10:36.830 : --43-->   0043.jpg | 26.12dB
22-11-05 13:10:42.452 : --44-->   0044.jpg | 24.37dB
22-11-05 13:10:51.544 : --45-->   0045.jpg | 20.96dB
22-11-05 13:10:58.544 : --46-->   0046.jpg | 22.77dB
22-11-05 13:11:03.615 : --47-->   0047.jpg | 23.59dB
22-11-05 13:11:05.423 : --48-->   0048.jpg | 26.39dB
22-11-05 13:11:10.548 : --49-->   0049.jpg | 26.21dB
22-11-05 13:11:16.369 : --50-->   0050.jpg | 19.89dB
22-11-05 13:11:21.129 : --51-->   0051.jpg | 23.29dB
22-11-05 13:11:27.669 : --52-->   0052.jpg | 22.76dB
22-11-05 13:11:34.062 : --53-->   0053.jpg | 23.91dB
22-11-05 13:11:39.039 : --54-->   0054.jpg | 21.62dB
22-11-05 13:11:45.952 : --55-->   0055.jpg | 33.20dB
22-11-05 13:11:53.777 : --56-->   0056.jpg | 23.53dB
22-11-05 13:11:56.902 : --57-->   0057.jpg | 29.02dB
22-11-05 13:12:02.685 : --58-->   0058.jpg | 25.69dB
22-11-05 13:12:06.957 : --59-->   0059.jpg | 26.54dB
22-11-05 13:12:12.938 : --60-->   0060.jpg | 23.85dB
22-11-05 13:12:15.419 : --61-->   0061.jpg | 24.94dB
22-11-05 13:12:18.789 : --62-->   0062.jpg | 23.82dB
22-11-05 13:12:20.741 : --63-->   0063.jpg | 22.73dB
22-11-05 13:12:28.763 : --64-->   0064.jpg | 27.77dB
22-11-05 13:12:33.420 : --65-->   0065.jpg | 33.30dB
22-11-05 13:12:37.060 : --66-->   0066.jpg | 29.64dB
22-11-05 13:12:47.544 : --67-->   0067.jpg | 25.66dB
22-11-05 13:12:49.340 : --68-->   0068.jpg | 24.63dB
22-11-05 13:12:57.979 : --69-->   0069.jpg | 19.22dB
22-11-05 13:13:02.555 : --70-->   0070.jpg | 20.46dB
22-11-05 13:13:07.893 : --71-->   0071.jpg | 22.40dB
22-11-05 13:13:13.238 : --72-->   0072.jpg | 26.55dB
22-11-05 13:13:22.760 : --73-->   0073.jpg | 26.70dB
22-11-05 13:13:25.512 : --74-->   0074.jpg | 22.88dB
22-11-05 13:13:32.965 : --75-->   0075.jpg | 23.65dB
22-11-05 13:13:35.655 : --76-->   0076.jpg | 24.43dB
22-11-05 13:13:40.615 : --77-->   0077.jpg | 21.93dB
22-11-05 13:13:47.189 : --78-->   0078.jpg | 25.96dB
22-11-05 13:13:50.749 : --79-->   0079.jpg | 20.23dB
22-11-05 13:13:54.980 : --80-->   0080.jpg | 28.16dB
22-11-05 13:14:00.933 : --81-->   0081.jpg | 28.39dB
22-11-05 13:14:06.457 : --82-->   0082.jpg | 27.53dB
22-11-05 13:14:12.978 : --83-->   0083.jpg | 21.59dB
22-11-05 13:14:20.187 : --84-->   0084.jpg | 27.50dB
22-11-05 13:14:23.936 : --85-->   0085.jpg | 20.55dB
22-11-05 13:14:26.719 : --86-->   0086.jpg | 23.80dB
22-11-05 13:14:30.624 : --87-->   0087.jpg | 20.49dB
22-11-05 13:14:39.378 : --88-->   0088.jpg | 25.55dB
22-11-05 13:14:43.402 : --89-->   0089.jpg | 23.45dB
22-11-05 13:14:47.670 : --90-->   0090.jpg | 21.67dB
22-11-05 13:14:52.931 : --91-->   0091.jpg | 28.01dB
22-11-05 13:15:01.938 : --92-->   0092.jpg | 20.70dB
22-11-05 13:15:03.015 : --93-->   0093.jpg | 26.38dB
22-11-05 13:15:05.059 : --94-->   0094.jpg | 29.33dB
22-11-05 13:15:10.092 : --95-->   0095.jpg | 25.07dB
22-11-05 13:15:14.921 : --96-->   0096.jpg | 20.38dB
22-11-05 13:15:24.991 : --97-->   0097.jpg | 32.24dB
22-11-05 13:15:30.995 : --98-->   0098.jpg | 24.17dB
22-11-05 13:15:38.923 : --99-->   0099.jpg | 24.30dB
22-11-05 13:15:44.870 : -100-->   0100.jpg | 22.67dB
22-11-05 13:15:45.054 : <epoch: 16, iter:   5,000, Average PSNR : 25.12dB

22-11-05 13:18:22.906 : <epoch: 17, iter:   5,200, lr:2.000e-04> G_loss: 4.690e-02 
22-11-05 13:20:54.932 : <epoch: 18, iter:   5,400, lr:2.000e-04> G_loss: 4.412e-02 
22-11-05 13:23:12.912 : <epoch: 18, iter:   5,600, lr:2.000e-04> G_loss: 4.362e-02 
22-11-05 13:25:52.538 : <epoch: 19, iter:   5,800, lr:2.000e-04> G_loss: 3.482e-02 
22-11-05 13:28:24.276 : <epoch: 20, iter:   6,000, lr:2.000e-04> G_loss: 3.569e-02 
22-11-05 13:30:41.157 : <epoch: 20, iter:   6,200, lr:2.000e-04> G_loss: 3.342e-02 
22-11-05 13:33:16.686 : <epoch: 21, iter:   6,400, lr:2.000e-04> G_loss: 4.265e-02 
22-11-05 13:35:59.686 : <epoch: 22, iter:   6,600, lr:2.000e-04> G_loss: 3.586e-02 
22-11-05 13:38:29.420 : <epoch: 22, iter:   6,800, lr:2.000e-04> G_loss: 3.521e-02 
22-11-05 13:40:58.871 : <epoch: 23, iter:   7,000, lr:2.000e-04> G_loss: 3.739e-02 
22-11-05 13:43:27.539 : <epoch: 24, iter:   7,200, lr:2.000e-04> G_loss: 2.955e-02 
22-11-05 13:45:56.053 : <epoch: 24, iter:   7,400, lr:2.000e-04> G_loss: 3.800e-02 
22-11-05 13:48:33.906 : <epoch: 25, iter:   7,600, lr:2.000e-04> G_loss: 3.551e-02 
22-11-05 13:51:10.028 : <epoch: 26, iter:   7,800, lr:2.000e-04> G_loss: 3.942e-02 
22-11-05 13:53:37.002 : <epoch: 26, iter:   8,000, lr:2.000e-04> G_loss: 3.796e-02 
22-11-05 13:56:13.011 : <epoch: 27, iter:   8,200, lr:2.000e-04> G_loss: 3.373e-02 
22-11-05 13:58:46.291 : <epoch: 28, iter:   8,400, lr:2.000e-04> G_loss: 3.631e-02 
22-11-05 14:01:03.392 : <epoch: 28, iter:   8,600, lr:2.000e-04> G_loss: 4.467e-02 
22-11-05 14:03:40.870 : <epoch: 29, iter:   8,800, lr:2.000e-04> G_loss: 4.063e-02 
22-11-05 14:06:09.270 : <epoch: 30, iter:   9,000, lr:2.000e-04> G_loss: 3.146e-02 
22-11-05 14:08:23.465 : <epoch: 30, iter:   9,200, lr:2.000e-04> G_loss: 3.526e-02 
22-11-05 14:11:04.229 : <epoch: 31, iter:   9,400, lr:2.000e-04> G_loss: 3.627e-02 
22-11-05 14:13:37.260 : <epoch: 32, iter:   9,600, lr:2.000e-04> G_loss: 4.169e-02 
22-11-05 14:15:58.421 : <epoch: 32, iter:   9,800, lr:2.000e-04> G_loss: 2.992e-02 
22-11-05 14:18:34.796 : <epoch: 33, iter:  10,000, lr:2.000e-04> G_loss: 3.497e-02 
22-11-05 14:18:34.812 : Saving the model.
22-11-05 14:18:45.891 : ---1-->   0001.jpg | 23.51dB
22-11-05 14:18:51.330 : ---2-->   0002.jpg | 24.92dB
22-11-05 14:19:00.128 : ---3-->   0003.jpg | 28.97dB
22-11-05 14:19:03.071 : ---4-->   0004.jpg | 27.75dB
22-11-05 14:19:06.734 : ---5-->   0005.jpg | 27.94dB
22-11-05 14:19:12.618 : ---6-->   0006.jpg | 31.19dB
22-11-05 14:19:15.064 : ---7-->   0007.jpg | 31.30dB
22-11-05 14:19:20.350 : ---8-->   0008.jpg | 23.40dB
22-11-05 14:19:31.522 : ---9-->   0009.jpg | 23.10dB
22-11-05 14:19:34.815 : --10-->   0010.jpg | 27.15dB
22-11-05 14:19:38.317 : --11-->   0011.jpg | 25.19dB
22-11-05 14:19:44.489 : --12-->   0012.jpg | 23.96dB
22-11-05 14:19:48.157 : --13-->   0013.jpg | 23.66dB
22-11-05 14:19:56.408 : --14-->   0014.jpg | 22.59dB
22-11-05 14:20:03.214 : --15-->   0015.jpg | 20.58dB
22-11-05 14:20:08.984 : --16-->   0016.jpg | 28.74dB
22-11-05 14:20:13.517 : --17-->   0017.jpg | 27.93dB
22-11-05 14:20:20.391 : --18-->   0018.jpg | 26.50dB
22-11-05 14:20:25.687 : --19-->   0019.jpg | 30.90dB
22-11-05 14:20:28.174 : --20-->   0020.jpg | 23.42dB
22-11-05 14:20:32.195 : --21-->   0021.jpg | 25.83dB
22-11-05 14:20:35.597 : --22-->   0022.jpg | 37.38dB
22-11-05 14:20:38.145 : --23-->   0023.jpg | 24.72dB
22-11-05 14:20:40.852 : --24-->   0024.jpg | 32.08dB
22-11-05 14:20:47.762 : --25-->   0025.jpg | 25.37dB
22-11-05 14:20:56.198 : --26-->   0026.jpg | 25.10dB
22-11-05 14:20:59.382 : --27-->   0027.jpg | 28.14dB
22-11-05 14:21:07.170 : --28-->   0028.jpg | 29.03dB
22-11-05 14:21:12.965 : --29-->   0029.jpg | 23.99dB
22-11-05 14:21:16.708 : --30-->   0030.jpg | 28.43dB
22-11-05 14:21:23.364 : --31-->   0031.jpg | 27.53dB
22-11-05 14:21:31.856 : --32-->   0032.jpg | 25.06dB
22-11-05 14:21:35.573 : --33-->   0033.jpg | 22.63dB
22-11-05 14:21:41.829 : --34-->   0034.jpg | 22.79dB
22-11-05 14:21:50.032 : --35-->   0035.jpg | 26.34dB
22-11-05 14:21:58.109 : --36-->   0036.jpg | 22.99dB
22-11-05 14:22:04.194 : --37-->   0037.jpg | 22.38dB
22-11-05 14:22:09.522 : --38-->   0038.jpg | 25.35dB
22-11-05 14:22:13.421 : --39-->   0039.jpg | 23.42dB
22-11-05 14:22:20.725 : --40-->   0040.jpg | 27.67dB
22-11-05 14:22:24.020 : --41-->   0041.jpg | 20.83dB
22-11-05 14:22:32.756 : --42-->   0042.jpg | 26.30dB
22-11-05 14:22:38.827 : --43-->   0043.jpg | 26.75dB
22-11-05 14:22:40.439 : --44-->   0044.jpg | 24.41dB
22-11-05 14:22:41.475 : --45-->   0045.jpg | 21.07dB
22-11-05 14:22:48.212 : --46-->   0046.jpg | 22.75dB
22-11-05 14:22:54.333 : --47-->   0047.jpg | 23.91dB
22-11-05 14:22:57.764 : --48-->   0048.jpg | 27.04dB
22-11-05 14:22:58.984 : --49-->   0049.jpg | 27.14dB
22-11-05 14:23:03.689 : --50-->   0050.jpg | 20.29dB
22-11-05 14:23:10.652 : --51-->   0051.jpg | 24.21dB
22-11-05 14:23:13.354 : --52-->   0052.jpg | 23.01dB
22-11-05 14:23:16.936 : --53-->   0053.jpg | 23.89dB
22-11-05 14:23:23.593 : --54-->   0054.jpg | 21.75dB
22-11-05 14:23:26.250 : --55-->   0055.jpg | 33.60dB
22-11-05 14:23:32.944 : --56-->   0056.jpg | 24.10dB
22-11-05 14:23:37.882 : --57-->   0057.jpg | 29.04dB
22-11-05 14:23:44.701 : --58-->   0058.jpg | 25.89dB
22-11-05 14:23:50.218 : --59-->   0059.jpg | 26.59dB
22-11-05 14:23:58.978 : --60-->   0060.jpg | 24.03dB
22-11-05 14:24:05.314 : --61-->   0061.jpg | 25.41dB
22-11-05 14:24:07.421 : --62-->   0062.jpg | 23.95dB
22-11-05 14:24:13.271 : --63-->   0063.jpg | 22.81dB
22-11-05 14:24:19.916 : --64-->   0064.jpg | 28.00dB
22-11-05 14:24:27.711 : --65-->   0065.jpg | 33.82dB
22-11-05 14:24:30.574 : --66-->   0066.jpg | 29.65dB
22-11-05 14:24:36.340 : --67-->   0067.jpg | 25.79dB
22-11-05 14:24:41.688 : --68-->   0068.jpg | 24.85dB
22-11-05 14:24:48.552 : --69-->   0069.jpg | 19.28dB
22-11-05 14:24:52.011 : --70-->   0070.jpg | 20.70dB
22-11-05 14:24:54.727 : --71-->   0071.jpg | 22.48dB
22-11-05 14:25:00.767 : --72-->   0072.jpg | 27.07dB
22-11-05 14:25:03.489 : --73-->   0073.jpg | 26.88dB
22-11-05 14:25:05.669 : --74-->   0074.jpg | 23.39dB
22-11-05 14:25:07.172 : --75-->   0075.jpg | 23.92dB
22-11-05 14:25:15.972 : --76-->   0076.jpg | 24.63dB
22-11-05 14:25:20.987 : --77-->   0077.jpg | 22.12dB
22-11-05 14:25:24.700 : --78-->   0078.jpg | 26.55dB
22-11-05 14:25:32.197 : --79-->   0079.jpg | 20.30dB
22-11-05 14:25:38.907 : --80-->   0080.jpg | 28.75dB
22-11-05 14:25:44.058 : --81-->   0081.jpg | 28.42dB
22-11-05 14:25:45.648 : --82-->   0082.jpg | 27.57dB
22-11-05 14:25:47.946 : --83-->   0083.jpg | 21.95dB
22-11-05 14:25:57.543 : --84-->   0084.jpg | 28.00dB
22-11-05 14:26:01.182 : --85-->   0085.jpg | 20.67dB
22-11-05 14:26:04.666 : --86-->   0086.jpg | 23.81dB
22-11-05 14:26:09.768 : --87-->   0087.jpg | 20.58dB
22-11-05 14:26:16.112 : --88-->   0088.jpg | 26.12dB
22-11-05 14:26:21.798 : --89-->   0089.jpg | 23.53dB
22-11-05 14:26:26.417 : --90-->   0090.jpg | 21.70dB
22-11-05 14:26:32.511 : --91-->   0091.jpg | 28.86dB
22-11-05 14:26:41.166 : --92-->   0092.jpg | 21.25dB
22-11-05 14:26:46.561 : --93-->   0093.jpg | 26.78dB
22-11-05 14:26:53.723 : --94-->   0094.jpg | 29.70dB
22-11-05 14:27:02.331 : --95-->   0095.jpg | 25.03dB
22-11-05 14:27:10.156 : --96-->   0096.jpg | 20.45dB
22-11-05 14:27:15.741 : --97-->   0097.jpg | 32.01dB
22-11-05 14:27:19.182 : --98-->   0098.jpg | 24.48dB
22-11-05 14:27:20.580 : --99-->   0099.jpg | 24.40dB
22-11-05 14:27:23.099 : -100-->   0100.jpg | 22.85dB
22-11-05 14:27:23.450 : <epoch: 33, iter:  10,000, Average PSNR : 25.44dB

22-11-05 14:29:48.145 : <epoch: 34, iter:  10,200, lr:2.000e-04> G_loss: 2.756e-02 
22-11-05 14:32:27.495 : <epoch: 35, iter:  10,400, lr:2.000e-04> G_loss: 4.178e-02 
22-11-05 14:34:59.287 : <epoch: 35, iter:  10,600, lr:2.000e-04> G_loss: 4.406e-02 
22-11-05 14:37:32.108 : <epoch: 36, iter:  10,800, lr:2.000e-04> G_loss: 4.160e-02 
22-11-05 14:39:57.774 : <epoch: 37, iter:  11,000, lr:2.000e-04> G_loss: 3.598e-02 
22-11-05 14:42:25.697 : <epoch: 37, iter:  11,200, lr:2.000e-04> G_loss: 3.609e-02 
22-11-05 14:44:58.590 : <epoch: 38, iter:  11,400, lr:2.000e-04> G_loss: 4.413e-02 
22-11-05 14:47:20.716 : <epoch: 39, iter:  11,600, lr:2.000e-04> G_loss: 3.942e-02 
22-11-05 14:49:44.248 : <epoch: 39, iter:  11,800, lr:2.000e-04> G_loss: 3.540e-02 
22-11-05 14:52:17.328 : <epoch: 40, iter:  12,000, lr:2.000e-04> G_loss: 3.460e-02 
22-11-05 14:54:55.043 : <epoch: 41, iter:  12,200, lr:2.000e-04> G_loss: 3.414e-02 
22-11-05 14:57:22.886 : <epoch: 41, iter:  12,400, lr:2.000e-04> G_loss: 3.842e-02 
22-11-05 14:59:47.294 : <epoch: 42, iter:  12,600, lr:2.000e-04> G_loss: 3.221e-02 
22-11-05 15:02:19.046 : <epoch: 43, iter:  12,800, lr:2.000e-04> G_loss: 4.567e-02 
22-11-05 15:04:44.452 : <epoch: 43, iter:  13,000, lr:2.000e-04> G_loss: 3.088e-02 
22-11-05 15:07:23.233 : <epoch: 44, iter:  13,200, lr:2.000e-04> G_loss: 3.751e-02 
22-11-05 15:09:53.280 : <epoch: 45, iter:  13,400, lr:2.000e-04> G_loss: 3.642e-02 
22-11-05 15:12:19.062 : <epoch: 45, iter:  13,600, lr:2.000e-04> G_loss: 3.845e-02 
22-11-05 15:14:56.605 : <epoch: 46, iter:  13,800, lr:2.000e-04> G_loss: 3.104e-02 
22-11-05 15:17:32.526 : <epoch: 47, iter:  14,000, lr:2.000e-04> G_loss: 3.716e-02 
22-11-05 15:20:05.253 : <epoch: 47, iter:  14,200, lr:2.000e-04> G_loss: 3.236e-02 
22-11-05 15:22:37.863 : <epoch: 48, iter:  14,400, lr:2.000e-04> G_loss: 3.562e-02 
22-11-05 15:25:11.187 : <epoch: 49, iter:  14,600, lr:2.000e-04> G_loss: 4.189e-02 
22-11-05 15:27:38.535 : <epoch: 49, iter:  14,800, lr:2.000e-04> G_loss: 3.900e-02 
22-11-05 15:30:13.104 : <epoch: 50, iter:  15,000, lr:2.000e-04> G_loss: 3.539e-02 
22-11-05 15:30:13.106 : Saving the model.
22-11-05 15:30:18.629 : ---1-->   0001.jpg | 23.57dB
22-11-05 15:30:25.847 : ---2-->   0002.jpg | 25.26dB
22-11-05 15:30:29.087 : ---3-->   0003.jpg | 29.38dB
22-11-05 15:30:30.530 : ---4-->   0004.jpg | 27.88dB
22-11-05 15:30:31.405 : ---5-->   0005.jpg | 28.32dB
22-11-05 15:30:35.664 : ---6-->   0006.jpg | 31.88dB
22-11-05 15:30:42.718 : ---7-->   0007.jpg | 32.34dB
22-11-05 15:30:46.981 : ---8-->   0008.jpg | 23.75dB
22-11-05 15:30:55.092 : ---9-->   0009.jpg | 23.27dB
22-11-05 15:31:02.707 : --10-->   0010.jpg | 27.40dB
22-11-05 15:31:07.898 : --11-->   0011.jpg | 25.20dB
22-11-05 15:31:11.040 : --12-->   0012.jpg | 23.93dB
22-11-05 15:31:21.249 : --13-->   0013.jpg | 23.78dB
22-11-05 15:31:27.273 : --14-->   0014.jpg | 22.90dB
22-11-05 15:31:33.376 : --15-->   0015.jpg | 20.85dB
22-11-05 15:31:40.052 : --16-->   0016.jpg | 29.15dB
22-11-05 15:31:45.780 : --17-->   0017.jpg | 28.23dB
22-11-05 15:31:50.685 : --18-->   0018.jpg | 26.56dB
22-11-05 15:31:57.851 : --19-->   0019.jpg | 31.42dB
22-11-05 15:32:01.000 : --20-->   0020.jpg | 23.55dB
22-11-05 15:32:12.843 : --21-->   0021.jpg | 25.88dB
22-11-05 15:32:21.579 : --22-->   0022.jpg | 36.91dB
22-11-05 15:32:23.455 : --23-->   0023.jpg | 24.94dB
22-11-05 15:32:30.110 : --24-->   0024.jpg | 32.85dB
22-11-05 15:32:33.227 : --25-->   0025.jpg | 25.55dB
22-11-05 15:32:34.692 : --26-->   0026.jpg | 25.85dB
22-11-05 15:32:40.463 : --27-->   0027.jpg | 28.50dB
22-11-05 15:32:46.580 : --28-->   0028.jpg | 29.43dB
22-11-05 15:32:52.545 : --29-->   0029.jpg | 24.07dB
22-11-05 15:33:01.147 : --30-->   0030.jpg | 28.67dB
22-11-05 15:33:06.940 : --31-->   0031.jpg | 27.80dB
22-11-05 15:33:12.546 : --32-->   0032.jpg | 25.18dB
22-11-05 15:33:14.269 : --33-->   0033.jpg | 22.90dB
22-11-05 15:33:20.874 : --34-->   0034.jpg | 22.98dB
22-11-05 15:33:27.696 : --35-->   0035.jpg | 26.41dB
22-11-05 15:33:33.776 : --36-->   0036.jpg | 23.08dB
22-11-05 15:33:41.174 : --37-->   0037.jpg | 22.59dB
22-11-05 15:33:46.077 : --38-->   0038.jpg | 25.55dB
22-11-05 15:33:50.569 : --39-->   0039.jpg | 23.60dB
22-11-05 15:33:52.929 : --40-->   0040.jpg | 27.85dB
22-11-05 15:33:54.264 : --41-->   0041.jpg | 20.86dB
22-11-05 15:33:57.648 : --42-->   0042.jpg | 26.43dB
22-11-05 15:34:05.783 : --43-->   0043.jpg | 26.69dB
22-11-05 15:34:11.019 : --44-->   0044.jpg | 24.54dB
22-11-05 15:34:14.457 : --45-->   0045.jpg | 21.22dB
22-11-05 15:34:19.262 : --46-->   0046.jpg | 22.97dB
22-11-05 15:34:25.249 : --47-->   0047.jpg | 24.11dB
22-11-05 15:34:30.324 : --48-->   0048.jpg | 27.46dB
22-11-05 15:34:34.398 : --49-->   0049.jpg | 27.40dB
22-11-05 15:34:40.986 : --50-->   0050.jpg | 20.46dB
22-11-05 15:34:46.235 : --51-->   0051.jpg | 24.54dB
22-11-05 15:34:55.438 : --52-->   0052.jpg | 23.18dB
22-11-05 15:35:01.880 : --53-->   0053.jpg | 24.12dB
22-11-05 15:35:08.039 : --54-->   0054.jpg | 21.87dB
22-11-05 15:35:14.943 : --55-->   0055.jpg | 34.35dB
22-11-05 15:35:18.116 : --56-->   0056.jpg | 24.23dB
22-11-05 15:35:21.811 : --57-->   0057.jpg | 29.63dB
22-11-05 15:35:25.109 : --58-->   0058.jpg | 26.16dB
22-11-05 15:35:30.554 : --59-->   0059.jpg | 27.23dB
22-11-05 15:35:36.883 : --60-->   0060.jpg | 24.39dB
22-11-05 15:35:42.048 : --61-->   0061.jpg | 25.71dB
22-11-05 15:35:47.869 : --62-->   0062.jpg | 24.07dB
22-11-05 15:35:51.603 : --63-->   0063.jpg | 22.92dB
22-11-05 15:36:00.967 : --64-->   0064.jpg | 28.53dB
22-11-05 15:36:10.058 : --65-->   0065.jpg | 34.12dB
22-11-05 15:36:17.927 : --66-->   0066.jpg | 29.89dB
22-11-05 15:36:25.171 : --67-->   0067.jpg | 26.10dB
22-11-05 15:36:27.993 : --68-->   0068.jpg | 24.83dB
22-11-05 15:36:29.423 : --69-->   0069.jpg | 19.35dB
22-11-05 15:36:31.637 : --70-->   0070.jpg | 20.80dB
22-11-05 15:36:37.223 : --71-->   0071.jpg | 22.48dB
22-11-05 15:36:41.510 : --72-->   0072.jpg | 27.53dB
22-11-05 15:36:50.519 : --73-->   0073.jpg | 27.19dB
22-11-05 15:36:54.134 : --74-->   0074.jpg | 23.66dB
22-11-05 15:37:01.322 : --75-->   0075.jpg | 24.10dB
22-11-05 15:37:07.143 : --76-->   0076.jpg | 24.98dB
22-11-05 15:37:16.986 : --77-->   0077.jpg | 22.21dB
22-11-05 15:37:22.821 : --78-->   0078.jpg | 26.87dB
22-11-05 15:37:31.145 : --79-->   0079.jpg | 20.35dB
22-11-05 15:37:36.698 : --80-->   0080.jpg | 29.11dB
22-11-05 15:37:40.836 : --81-->   0081.jpg | 28.51dB
22-11-05 15:37:45.523 : --82-->   0082.jpg | 27.83dB
22-11-05 15:37:51.808 : --83-->   0083.jpg | 22.17dB
22-11-05 15:37:57.663 : --84-->   0084.jpg | 28.28dB
22-11-05 15:38:04.414 : --85-->   0085.jpg | 20.83dB
22-11-05 15:38:06.838 : --86-->   0086.jpg | 24.04dB
22-11-05 15:38:11.934 : --87-->   0087.jpg | 20.61dB
22-11-05 15:38:16.881 : --88-->   0088.jpg | 26.39dB
22-11-05 15:38:20.488 : --89-->   0089.jpg | 23.67dB
22-11-05 15:38:22.705 : --90-->   0090.jpg | 21.74dB
22-11-05 15:38:24.409 : --91-->   0091.jpg | 29.68dB
22-11-05 15:38:29.012 : --92-->   0092.jpg | 21.17dB
22-11-05 15:38:35.209 : --93-->   0093.jpg | 26.94dB
22-11-05 15:38:43.126 : --94-->   0094.jpg | 29.86dB
22-11-05 15:38:48.403 : --95-->   0095.jpg | 25.11dB
22-11-05 15:38:55.850 : --96-->   0096.jpg | 20.57dB
22-11-05 15:39:01.893 : --97-->   0097.jpg | 32.32dB
22-11-05 15:39:06.663 : --98-->   0098.jpg | 24.91dB
22-11-05 15:39:12.576 : --99-->   0099.jpg | 24.58dB
22-11-05 15:39:19.615 : -100-->   0100.jpg | 23.09dB
22-11-05 15:39:20.322 : <epoch: 50, iter:  15,000, Average PSNR : 25.68dB

22-11-05 15:41:47.325 : <epoch: 51, iter:  15,200, lr:2.000e-04> G_loss: 3.447e-02 
22-11-05 15:44:11.728 : <epoch: 51, iter:  15,400, lr:2.000e-04> G_loss: 2.872e-02 
22-11-05 15:46:36.255 : <epoch: 52, iter:  15,600, lr:2.000e-04> G_loss: 2.926e-02 
22-11-05 15:49:13.462 : <epoch: 53, iter:  15,800, lr:2.000e-04> G_loss: 3.713e-02 
22-11-05 15:51:47.770 : <epoch: 53, iter:  16,000, lr:2.000e-04> G_loss: 3.711e-02 
22-11-05 15:54:18.136 : <epoch: 54, iter:  16,200, lr:2.000e-04> G_loss: 3.037e-02 
22-11-05 15:56:47.768 : <epoch: 55, iter:  16,400, lr:2.000e-04> G_loss: 3.826e-02 
22-11-05 15:59:19.375 : <epoch: 55, iter:  16,600, lr:2.000e-04> G_loss: 4.329e-02 
22-11-05 16:01:44.706 : <epoch: 56, iter:  16,800, lr:2.000e-04> G_loss: 3.164e-02 
22-11-05 16:04:10.696 : <epoch: 57, iter:  17,000, lr:2.000e-04> G_loss: 3.073e-02 
22-11-05 16:06:46.312 : <epoch: 57, iter:  17,200, lr:2.000e-04> G_loss: 4.632e-02 
22-11-05 16:09:26.313 : <epoch: 58, iter:  17,400, lr:2.000e-04> G_loss: 3.070e-02 
22-11-05 16:11:53.997 : <epoch: 59, iter:  17,600, lr:2.000e-04> G_loss: 3.981e-02 
22-11-05 16:14:18.702 : <epoch: 59, iter:  17,800, lr:2.000e-04> G_loss: 3.911e-02 
22-11-05 16:16:52.477 : <epoch: 60, iter:  18,000, lr:2.000e-04> G_loss: 3.278e-02 
22-11-05 16:19:15.127 : <epoch: 61, iter:  18,200, lr:2.000e-04> G_loss: 3.376e-02 
22-11-05 16:21:30.377 : <epoch: 61, iter:  18,400, lr:2.000e-04> G_loss: 3.743e-02 
22-11-05 16:24:09.674 : <epoch: 62, iter:  18,600, lr:2.000e-04> G_loss: 3.642e-02 
22-11-05 16:26:47.336 : <epoch: 63, iter:  18,800, lr:2.000e-04> G_loss: 3.462e-02 
22-11-05 16:29:11.428 : <epoch: 63, iter:  19,000, lr:2.000e-04> G_loss: 3.683e-02 
22-11-05 16:31:48.691 : <epoch: 64, iter:  19,200, lr:2.000e-04> G_loss: 4.218e-02 
22-11-05 16:34:24.978 : <epoch: 65, iter:  19,400, lr:2.000e-04> G_loss: 4.247e-02 
22-11-05 16:36:50.447 : <epoch: 65, iter:  19,600, lr:2.000e-04> G_loss: 2.700e-02 
22-11-05 16:39:22.063 : <epoch: 66, iter:  19,800, lr:2.000e-04> G_loss: 4.567e-02 
22-11-05 16:41:57.211 : <epoch: 67, iter:  20,000, lr:2.000e-04> G_loss: 3.745e-02 
22-11-05 16:41:57.213 : Saving the model.
22-11-05 16:42:03.565 : ---1-->   0001.jpg | 23.87dB
22-11-05 16:42:10.281 : ---2-->   0002.jpg | 25.24dB
22-11-05 16:42:18.689 : ---3-->   0003.jpg | 29.64dB
22-11-05 16:42:23.122 : ---4-->   0004.jpg | 27.96dB
22-11-05 16:42:26.448 : ---5-->   0005.jpg | 28.53dB
22-11-05 16:42:29.810 : ---6-->   0006.jpg | 31.88dB
22-11-05 16:42:33.995 : ---7-->   0007.jpg | 32.95dB
22-11-05 16:42:43.425 : ---8-->   0008.jpg | 23.86dB
22-11-05 16:42:50.615 : ---9-->   0009.jpg | 23.23dB
22-11-05 16:42:55.057 : --10-->   0010.jpg | 27.59dB
22-11-05 16:42:57.258 : --11-->   0011.jpg | 25.48dB
22-11-05 16:43:06.221 : --12-->   0012.jpg | 24.01dB
22-11-05 16:43:08.821 : --13-->   0013.jpg | 23.90dB
22-11-05 16:43:16.208 : --14-->   0014.jpg | 23.01dB
22-11-05 16:43:25.305 : --15-->   0015.jpg | 21.01dB
22-11-05 16:43:33.890 : --16-->   0016.jpg | 29.28dB
22-11-05 16:43:36.966 : --17-->   0017.jpg | 28.49dB
22-11-05 16:43:38.655 : --18-->   0018.jpg | 26.49dB
22-11-05 16:43:44.426 : --19-->   0019.jpg | 32.04dB
22-11-05 16:43:51.016 : --20-->   0020.jpg | 23.61dB
22-11-05 16:43:57.792 : --21-->   0021.jpg | 26.10dB
22-11-05 16:44:05.552 : --22-->   0022.jpg | 38.06dB
22-11-05 16:44:10.109 : --23-->   0023.jpg | 25.16dB
22-11-05 16:44:14.562 : --24-->   0024.jpg | 33.26dB
22-11-05 16:44:21.009 : --25-->   0025.jpg | 25.67dB
22-11-05 16:44:22.190 : --26-->   0026.jpg | 25.91dB
22-11-05 16:44:27.050 : --27-->   0027.jpg | 28.68dB
22-11-05 16:44:35.378 : --28-->   0028.jpg | 29.56dB
22-11-05 16:44:41.359 : --29-->   0029.jpg | 24.11dB
22-11-05 16:44:46.251 : --30-->   0030.jpg | 28.83dB
22-11-05 16:44:50.964 : --31-->   0031.jpg | 27.91dB
22-11-05 16:44:54.321 : --32-->   0032.jpg | 25.46dB
22-11-05 16:45:01.170 : --33-->   0033.jpg | 23.01dB
22-11-05 16:45:07.224 : --34-->   0034.jpg | 23.02dB
22-11-05 16:45:10.805 : --35-->   0035.jpg | 26.38dB
22-11-05 16:45:20.572 : --36-->   0036.jpg | 23.07dB
22-11-05 16:45:26.533 : --37-->   0037.jpg | 22.58dB
22-11-05 16:45:30.984 : --38-->   0038.jpg | 25.83dB
22-11-05 16:45:37.893 : --39-->   0039.jpg | 23.75dB
22-11-05 16:45:44.706 : --40-->   0040.jpg | 27.92dB
22-11-05 16:45:50.172 : --41-->   0041.jpg | 20.99dB
22-11-05 16:45:53.679 : --42-->   0042.jpg | 26.56dB
22-11-05 16:45:57.389 : --43-->   0043.jpg | 26.91dB
22-11-05 16:46:02.755 : --44-->   0044.jpg | 24.56dB
22-11-05 16:46:12.523 : --45-->   0045.jpg | 21.30dB
22-11-05 16:46:18.634 : --46-->   0046.jpg | 23.13dB
22-11-05 16:46:26.812 : --47-->   0047.jpg | 24.09dB
22-11-05 16:46:34.468 : --48-->   0048.jpg | 27.74dB
22-11-05 16:46:38.864 : --49-->   0049.jpg | 27.52dB
22-11-05 16:46:46.374 : --50-->   0050.jpg | 20.63dB
22-11-05 16:46:54.004 : --51-->   0051.jpg | 24.64dB
22-11-05 16:46:57.450 : --52-->   0052.jpg | 23.21dB
22-11-05 16:47:00.008 : --53-->   0053.jpg | 24.24dB
22-11-05 16:47:05.842 : --54-->   0054.jpg | 21.87dB
22-11-05 16:47:11.258 : --55-->   0055.jpg | 34.91dB
22-11-05 16:47:16.478 : --56-->   0056.jpg | 24.25dB
22-11-05 16:47:23.022 : --57-->   0057.jpg | 30.01dB
22-11-05 16:47:26.806 : --58-->   0058.jpg | 26.22dB
22-11-05 16:47:32.871 : --59-->   0059.jpg | 27.34dB
22-11-05 16:47:38.743 : --60-->   0060.jpg | 24.48dB
22-11-05 16:47:44.701 : --61-->   0061.jpg | 25.75dB
22-11-05 16:47:47.903 : --62-->   0062.jpg | 24.11dB
22-11-05 16:47:56.469 : --63-->   0063.jpg | 23.00dB
22-11-05 16:48:00.672 : --64-->   0064.jpg | 28.43dB
22-11-05 16:48:05.028 : --65-->   0065.jpg | 34.37dB
22-11-05 16:48:08.114 : --66-->   0066.jpg | 29.94dB
22-11-05 16:48:15.236 : --67-->   0067.jpg | 26.18dB
22-11-05 16:48:24.824 : --68-->   0068.jpg | 24.94dB
22-11-05 16:48:29.319 : --69-->   0069.jpg | 19.39dB
22-11-05 16:48:37.958 : --70-->   0070.jpg | 20.92dB
22-11-05 16:48:41.860 : --71-->   0071.jpg | 22.49dB
22-11-05 16:48:43.930 : --72-->   0072.jpg | 27.70dB
22-11-05 16:48:49.021 : --73-->   0073.jpg | 27.13dB
22-11-05 16:48:56.607 : --74-->   0074.jpg | 23.87dB
22-11-05 16:49:04.139 : --75-->   0075.jpg | 24.16dB
22-11-05 16:49:08.259 : --76-->   0076.jpg | 25.03dB
22-11-05 16:49:09.290 : --77-->   0077.jpg | 22.17dB
22-11-05 16:49:14.177 : --78-->   0078.jpg | 27.04dB
22-11-05 16:49:17.908 : --79-->   0079.jpg | 20.38dB
22-11-05 16:49:19.400 : --80-->   0080.jpg | 29.33dB
22-11-05 16:49:22.177 : --81-->   0081.jpg | 28.53dB
22-11-05 16:49:27.227 : --82-->   0082.jpg | 27.92dB
22-11-05 16:49:30.791 : --83-->   0083.jpg | 22.22dB
22-11-05 16:49:36.255 : --84-->   0084.jpg | 28.32dB
22-11-05 16:49:42.371 : --85-->   0085.jpg | 20.85dB
22-11-05 16:49:46.092 : --86-->   0086.jpg | 24.06dB
22-11-05 16:49:50.565 : --87-->   0087.jpg | 20.73dB
22-11-05 16:49:57.580 : --88-->   0088.jpg | 26.62dB
22-11-05 16:50:06.609 : --89-->   0089.jpg | 23.74dB
22-11-05 16:50:08.409 : --90-->   0090.jpg | 21.74dB
22-11-05 16:50:10.396 : --91-->   0091.jpg | 29.68dB
22-11-05 16:50:15.215 : --92-->   0092.jpg | 21.67dB
22-11-05 16:50:19.172 : --93-->   0093.jpg | 27.14dB
22-11-05 16:50:25.891 : --94-->   0094.jpg | 30.09dB
22-11-05 16:50:30.697 : --95-->   0095.jpg | 25.12dB
22-11-05 16:50:37.235 : --96-->   0096.jpg | 20.62dB
22-11-05 16:50:44.599 : --97-->   0097.jpg | 32.42dB
22-11-05 16:50:48.030 : --98-->   0098.jpg | 24.97dB
22-11-05 16:50:52.167 : --99-->   0099.jpg | 24.62dB
22-11-05 16:50:56.463 : -100-->   0100.jpg | 23.15dB
22-11-05 16:50:57.161 : <epoch: 67, iter:  20,000, Average PSNR : 25.81dB

22-11-05 16:53:41.924 : <epoch: 68, iter:  20,200, lr:2.000e-04> G_loss: 2.871e-02 
22-11-05 16:56:05.900 : <epoch: 68, iter:  20,400, lr:2.000e-04> G_loss: 3.345e-02 
22-11-05 16:58:36.168 : <epoch: 69, iter:  20,600, lr:2.000e-04> G_loss: 3.370e-02 
22-11-05 17:01:04.326 : <epoch: 70, iter:  20,800, lr:2.000e-04> G_loss: 3.124e-02 
22-11-05 17:03:33.681 : <epoch: 70, iter:  21,000, lr:2.000e-04> G_loss: 3.585e-02 
22-11-05 17:06:00.202 : <epoch: 71, iter:  21,200, lr:2.000e-04> G_loss: 3.746e-02 
22-11-05 17:08:24.489 : <epoch: 72, iter:  21,400, lr:2.000e-04> G_loss: 3.113e-02 
22-11-05 17:11:02.109 : <epoch: 72, iter:  21,600, lr:2.000e-04> G_loss: 3.878e-02 
22-11-05 17:13:35.923 : <epoch: 73, iter:  21,800, lr:2.000e-04> G_loss: 3.271e-02 
22-11-05 17:16:18.158 : <epoch: 74, iter:  22,000, lr:2.000e-04> G_loss: 3.088e-02 
22-11-05 17:18:54.948 : <epoch: 74, iter:  22,200, lr:2.000e-04> G_loss: 3.906e-02 
22-11-05 17:21:31.382 : <epoch: 75, iter:  22,400, lr:2.000e-04> G_loss: 3.465e-02 
22-11-05 17:23:57.553 : <epoch: 76, iter:  22,600, lr:2.000e-04> G_loss: 3.876e-02 
22-11-05 17:26:28.289 : <epoch: 76, iter:  22,800, lr:2.000e-04> G_loss: 3.146e-02 
22-11-05 17:29:08.261 : <epoch: 77, iter:  23,000, lr:2.000e-04> G_loss: 3.615e-02 
22-11-05 17:31:48.017 : <epoch: 78, iter:  23,200, lr:2.000e-04> G_loss: 4.136e-02 
22-11-05 17:34:17.595 : <epoch: 78, iter:  23,400, lr:2.000e-04> G_loss: 2.885e-02 
22-11-05 17:36:56.516 : <epoch: 79, iter:  23,600, lr:2.000e-04> G_loss: 3.941e-02 
22-11-05 17:39:09.973 : <epoch: 80, iter:  23,800, lr:2.000e-04> G_loss: 4.089e-02 
22-11-05 17:41:37.250 : <epoch: 80, iter:  24,000, lr:2.000e-04> G_loss: 3.114e-02 
22-11-05 17:44:05.625 : <epoch: 81, iter:  24,200, lr:2.000e-04> G_loss: 5.154e-02 
22-11-05 17:46:37.566 : <epoch: 82, iter:  24,400, lr:2.000e-04> G_loss: 2.965e-02 
22-11-05 17:49:08.687 : <epoch: 82, iter:  24,600, lr:2.000e-04> G_loss: 3.965e-02 
22-11-05 17:51:36.804 : <epoch: 83, iter:  24,800, lr:2.000e-04> G_loss: 3.263e-02 
22-11-05 17:54:06.630 : <epoch: 84, iter:  25,000, lr:2.000e-04> G_loss: 3.498e-02 
22-11-05 17:54:06.632 : Saving the model.
22-11-05 17:54:14.293 : ---1-->   0001.jpg | 23.88dB
22-11-05 17:54:22.523 : ---2-->   0002.jpg | 25.27dB
22-11-05 17:54:28.842 : ---3-->   0003.jpg | 29.56dB
22-11-05 17:54:31.317 : ---4-->   0004.jpg | 27.97dB
22-11-05 17:54:36.019 : ---5-->   0005.jpg | 28.51dB
22-11-05 17:54:39.247 : ---6-->   0006.jpg | 31.80dB
22-11-05 17:54:42.240 : ---7-->   0007.jpg | 33.10dB
22-11-05 17:54:48.014 : ---8-->   0008.jpg | 23.87dB
22-11-05 17:54:53.240 : ---9-->   0009.jpg | 23.21dB
22-11-05 17:54:57.717 : --10-->   0010.jpg | 27.48dB
22-11-05 17:55:06.559 : --11-->   0011.jpg | 25.38dB
22-11-05 17:55:13.393 : --12-->   0012.jpg | 24.00dB
22-11-05 17:55:15.296 : --13-->   0013.jpg | 23.83dB
22-11-05 17:55:17.277 : --14-->   0014.jpg | 23.00dB
22-11-05 17:55:22.439 : --15-->   0015.jpg | 21.02dB
22-11-05 17:55:25.535 : --16-->   0016.jpg | 29.31dB
22-11-05 17:55:31.618 : --17-->   0017.jpg | 28.43dB
22-11-05 17:55:34.889 : --18-->   0018.jpg | 26.22dB
22-11-05 17:55:41.198 : --19-->   0019.jpg | 31.85dB
22-11-05 17:55:45.502 : --20-->   0020.jpg | 23.61dB
22-11-05 17:55:49.852 : --21-->   0021.jpg | 25.86dB
22-11-05 17:55:55.600 : --22-->   0022.jpg | 37.56dB
22-11-05 17:56:01.648 : --23-->   0023.jpg | 25.10dB
22-11-05 17:56:06.095 : --24-->   0024.jpg | 33.01dB
22-11-05 17:56:13.270 : --25-->   0025.jpg | 25.66dB
22-11-05 17:56:18.313 : --26-->   0026.jpg | 26.16dB
22-11-05 17:56:24.816 : --27-->   0027.jpg | 28.65dB
22-11-05 17:56:33.767 : --28-->   0028.jpg | 29.45dB
22-11-05 17:56:39.986 : --29-->   0029.jpg | 24.09dB
22-11-05 17:56:46.573 : --30-->   0030.jpg | 28.85dB
22-11-05 17:56:53.178 : --31-->   0031.jpg | 27.89dB
22-11-05 17:56:57.660 : --32-->   0032.jpg | 25.40dB
22-11-05 17:57:03.742 : --33-->   0033.jpg | 23.02dB
22-11-05 17:57:05.766 : --34-->   0034.jpg | 23.01dB
22-11-05 17:57:12.406 : --35-->   0035.jpg | 26.09dB
22-11-05 17:57:19.728 : --36-->   0036.jpg | 23.10dB
22-11-05 17:57:24.400 : --37-->   0037.jpg | 22.56dB
22-11-05 17:57:28.186 : --38-->   0038.jpg | 25.79dB
22-11-05 17:57:37.299 : --39-->   0039.jpg | 23.70dB
22-11-05 17:57:45.778 : --40-->   0040.jpg | 27.77dB
22-11-05 17:57:50.783 : --41-->   0041.jpg | 21.11dB
22-11-05 17:57:55.634 : --42-->   0042.jpg | 26.47dB
22-11-05 17:58:02.876 : --43-->   0043.jpg | 26.84dB
22-11-05 17:58:08.489 : --44-->   0044.jpg | 24.57dB
22-11-05 17:58:14.625 : --45-->   0045.jpg | 21.23dB
22-11-05 17:58:24.465 : --46-->   0046.jpg | 22.96dB
22-11-05 17:58:30.354 : --47-->   0047.jpg | 24.12dB
22-11-05 17:58:34.659 : --48-->   0048.jpg | 27.67dB
22-11-05 17:58:38.667 : --49-->   0049.jpg | 27.59dB
22-11-05 17:58:43.524 : --50-->   0050.jpg | 20.51dB
22-11-05 17:58:49.521 : --51-->   0051.jpg | 24.78dB
22-11-05 17:58:56.203 : --52-->   0052.jpg | 23.28dB
22-11-05 17:59:00.133 : --53-->   0053.jpg | 24.18dB
22-11-05 17:59:06.626 : --54-->   0054.jpg | 21.93dB
22-11-05 17:59:10.127 : --55-->   0055.jpg | 34.70dB
22-11-05 17:59:13.970 : --56-->   0056.jpg | 24.53dB
22-11-05 17:59:20.397 : --57-->   0057.jpg | 29.96dB
22-11-05 17:59:24.586 : --58-->   0058.jpg | 26.36dB
22-11-05 17:59:32.230 : --59-->   0059.jpg | 27.31dB
22-11-05 17:59:35.234 : --60-->   0060.jpg | 24.47dB
22-11-05 17:59:36.570 : --61-->   0061.jpg | 25.77dB
22-11-05 17:59:41.246 : --62-->   0062.jpg | 24.12dB
22-11-05 17:59:42.610 : --63-->   0063.jpg | 22.94dB
22-11-05 17:59:45.247 : --64-->   0064.jpg | 28.54dB
22-11-05 17:59:47.947 : --65-->   0065.jpg | 33.99dB
22-11-05 17:59:51.418 : --66-->   0066.jpg | 29.86dB
22-11-05 17:59:56.119 : --67-->   0067.jpg | 26.33dB
22-11-05 18:00:02.996 : --68-->   0068.jpg | 24.96dB
22-11-05 18:00:08.425 : --69-->   0069.jpg | 19.32dB
22-11-05 18:00:09.590 : --70-->   0070.jpg | 20.88dB
22-11-05 18:00:15.120 : --71-->   0071.jpg | 22.46dB
22-11-05 18:00:21.833 : --72-->   0072.jpg | 27.59dB
22-11-05 18:00:24.807 : --73-->   0073.jpg | 27.20dB
22-11-05 18:00:27.279 : --74-->   0074.jpg | 23.95dB
22-11-05 18:00:33.475 : --75-->   0075.jpg | 24.09dB
22-11-05 18:00:37.844 : --76-->   0076.jpg | 25.10dB
22-11-05 18:00:44.837 : --77-->   0077.jpg | 22.23dB
22-11-05 18:00:46.113 : --78-->   0078.jpg | 27.14dB
22-11-05 18:00:48.503 : --79-->   0079.jpg | 20.37dB
22-11-05 18:00:54.146 : --80-->   0080.jpg | 29.24dB
22-11-05 18:01:01.168 : --81-->   0081.jpg | 28.53dB
22-11-05 18:01:03.789 : --82-->   0082.jpg | 27.94dB
22-11-05 18:01:09.542 : --83-->   0083.jpg | 22.44dB
22-11-05 18:01:17.218 : --84-->   0084.jpg | 28.42dB
22-11-05 18:01:22.859 : --85-->   0085.jpg | 20.86dB
22-11-05 18:01:31.397 : --86-->   0086.jpg | 24.07dB
22-11-05 18:01:40.348 : --87-->   0087.jpg | 20.70dB
22-11-05 18:01:46.069 : --88-->   0088.jpg | 26.57dB
22-11-05 18:01:51.664 : --89-->   0089.jpg | 23.66dB
22-11-05 18:01:57.217 : --90-->   0090.jpg | 21.73dB
22-11-05 18:02:00.843 : --91-->   0091.jpg | 29.87dB
22-11-05 18:02:04.607 : --92-->   0092.jpg | 21.51dB
22-11-05 18:02:06.626 : --93-->   0093.jpg | 27.19dB
22-11-05 18:02:13.628 : --94-->   0094.jpg | 30.05dB
22-11-05 18:02:20.785 : --95-->   0095.jpg | 25.09dB
22-11-05 18:02:27.465 : --96-->   0096.jpg | 20.62dB
22-11-05 18:02:34.822 : --97-->   0097.jpg | 32.37dB
22-11-05 18:02:42.191 : --98-->   0098.jpg | 24.95dB
22-11-05 18:02:46.148 : --99-->   0099.jpg | 24.52dB
22-11-05 18:02:50.635 : -100-->   0100.jpg | 23.15dB
22-11-05 18:02:50.751 : <epoch: 84, iter:  25,000, Average PSNR : 25.79dB

22-11-05 18:05:15.984 : <epoch: 84, iter:  25,200, lr:2.000e-04> G_loss: 3.220e-02 
22-11-05 18:07:41.964 : <epoch: 85, iter:  25,400, lr:2.000e-04> G_loss: 3.223e-02 
22-11-05 18:10:10.786 : <epoch: 86, iter:  25,600, lr:2.000e-04> G_loss: 3.178e-02 
22-11-05 18:12:34.467 : <epoch: 86, iter:  25,800, lr:2.000e-04> G_loss: 3.640e-02 
22-11-05 18:15:01.997 : <epoch: 87, iter:  26,000, lr:2.000e-04> G_loss: 3.258e-02 
22-11-05 18:17:22.884 : <epoch: 88, iter:  26,200, lr:2.000e-04> G_loss: 3.115e-02 
22-11-05 18:19:47.166 : <epoch: 88, iter:  26,400, lr:2.000e-04> G_loss: 3.100e-02 
22-11-05 18:22:20.885 : <epoch: 89, iter:  26,600, lr:2.000e-04> G_loss: 3.606e-02 
22-11-05 18:24:47.550 : <epoch: 90, iter:  26,800, lr:2.000e-04> G_loss: 3.601e-02 
22-11-05 18:27:13.151 : <epoch: 90, iter:  27,000, lr:2.000e-04> G_loss: 3.813e-02 
22-11-05 18:29:46.733 : <epoch: 91, iter:  27,200, lr:2.000e-04> G_loss: 3.155e-02 
22-11-05 18:32:20.430 : <epoch: 92, iter:  27,400, lr:2.000e-04> G_loss: 3.048e-02 
22-11-05 18:34:42.972 : <epoch: 92, iter:  27,600, lr:2.000e-04> G_loss: 2.928e-02 
22-11-05 18:37:11.628 : <epoch: 93, iter:  27,800, lr:2.000e-04> G_loss: 3.201e-02 
22-11-05 18:39:38.274 : <epoch: 94, iter:  28,000, lr:2.000e-04> G_loss: 3.450e-02 
22-11-05 18:41:53.425 : <epoch: 94, iter:  28,200, lr:2.000e-04> G_loss: 3.451e-02 
22-11-05 18:44:25.686 : <epoch: 95, iter:  28,400, lr:2.000e-04> G_loss: 3.922e-02 
22-11-05 18:46:54.126 : <epoch: 96, iter:  28,600, lr:2.000e-04> G_loss: 3.389e-02 
22-11-05 18:49:24.685 : <epoch: 96, iter:  28,800, lr:2.000e-04> G_loss: 3.821e-02 
22-11-05 18:52:03.402 : <epoch: 97, iter:  29,000, lr:2.000e-04> G_loss: 2.811e-02 
22-11-05 18:54:38.162 : <epoch: 98, iter:  29,200, lr:2.000e-04> G_loss: 4.247e-02 
22-11-05 18:57:04.782 : <epoch: 98, iter:  29,400, lr:2.000e-04> G_loss: 3.453e-02 
22-11-05 18:59:38.253 : <epoch: 99, iter:  29,600, lr:2.000e-04> G_loss: 3.608e-02 
22-11-05 19:02:06.183 : <epoch:100, iter:  29,800, lr:2.000e-04> G_loss: 3.272e-02 
22-11-05 19:04:24.347 : <epoch:101, iter:  30,000, lr:2.000e-04> G_loss: 3.183e-02 
22-11-05 19:04:24.409 : Saving the model.
22-11-05 19:04:34.185 : ---1-->   0001.jpg | 24.17dB
22-11-05 19:04:37.777 : ---2-->   0002.jpg | 25.62dB
22-11-05 19:04:45.003 : ---3-->   0003.jpg | 29.80dB
22-11-05 19:04:50.743 : ---4-->   0004.jpg | 28.07dB
22-11-05 19:04:52.683 : ---5-->   0005.jpg | 28.63dB
22-11-05 19:05:00.039 : ---6-->   0006.jpg | 32.26dB
22-11-05 19:05:04.136 : ---7-->   0007.jpg | 33.74dB
22-11-05 19:05:13.792 : ---8-->   0008.jpg | 23.91dB
22-11-05 19:05:22.747 : ---9-->   0009.jpg | 23.36dB
22-11-05 19:05:26.680 : --10-->   0010.jpg | 27.76dB
22-11-05 19:05:31.811 : --11-->   0011.jpg | 25.49dB
22-11-05 19:05:37.369 : --12-->   0012.jpg | 24.14dB
22-11-05 19:05:40.603 : --13-->   0013.jpg | 23.93dB
22-11-05 19:05:44.639 : --14-->   0014.jpg | 23.16dB
22-11-05 19:05:49.613 : --15-->   0015.jpg | 21.35dB
22-11-05 19:05:50.995 : --16-->   0016.jpg | 29.63dB
22-11-05 19:05:52.781 : --17-->   0017.jpg | 28.81dB
22-11-05 19:05:55.187 : --18-->   0018.jpg | 26.42dB
22-11-05 19:06:00.601 : --19-->   0019.jpg | 32.15dB
22-11-05 19:06:07.268 : --20-->   0020.jpg | 23.72dB
22-11-05 19:06:12.567 : --21-->   0021.jpg | 26.22dB
22-11-05 19:06:20.345 : --22-->   0022.jpg | 38.33dB
22-11-05 19:06:22.591 : --23-->   0023.jpg | 25.17dB
22-11-05 19:06:29.863 : --24-->   0024.jpg | 33.70dB
22-11-05 19:06:34.622 : --25-->   0025.jpg | 25.71dB
22-11-05 19:06:37.611 : --26-->   0026.jpg | 26.42dB
22-11-05 19:06:41.944 : --27-->   0027.jpg | 28.91dB
22-11-05 19:06:49.664 : --28-->   0028.jpg | 30.05dB
22-11-05 19:06:52.610 : --29-->   0029.jpg | 24.14dB
22-11-05 19:06:56.936 : --30-->   0030.jpg | 29.00dB
22-11-05 19:06:59.462 : --31-->   0031.jpg | 28.07dB
22-11-05 19:07:02.812 : --32-->   0032.jpg | 25.71dB
22-11-05 19:07:08.797 : --33-->   0033.jpg | 23.15dB
22-11-05 19:07:17.090 : --34-->   0034.jpg | 23.22dB
22-11-05 19:07:22.620 : --35-->   0035.jpg | 26.48dB
22-11-05 19:07:29.497 : --36-->   0036.jpg | 23.20dB
22-11-05 19:07:33.303 : --37-->   0037.jpg | 22.73dB
22-11-05 19:07:37.068 : --38-->   0038.jpg | 26.10dB
22-11-05 19:07:42.314 : --39-->   0039.jpg | 23.74dB
22-11-05 19:07:45.520 : --40-->   0040.jpg | 27.99dB
22-11-05 19:07:53.946 : --41-->   0041.jpg | 21.00dB
22-11-05 19:07:58.664 : --42-->   0042.jpg | 26.70dB
22-11-05 19:08:02.567 : --43-->   0043.jpg | 27.00dB
22-11-05 19:08:05.591 : --44-->   0044.jpg | 24.64dB
22-11-05 19:08:06.939 : --45-->   0045.jpg | 21.41dB
22-11-05 19:08:11.923 : --46-->   0046.jpg | 23.18dB
22-11-05 19:08:18.012 : --47-->   0047.jpg | 24.18dB
22-11-05 19:08:23.052 : --48-->   0048.jpg | 27.79dB
22-11-05 19:08:30.539 : --49-->   0049.jpg | 27.84dB
22-11-05 19:08:34.534 : --50-->   0050.jpg | 20.80dB
22-11-05 19:08:38.960 : --51-->   0051.jpg | 25.14dB
22-11-05 19:08:44.501 : --52-->   0052.jpg | 23.30dB
22-11-05 19:08:49.040 : --53-->   0053.jpg | 24.24dB
22-11-05 19:08:53.771 : --54-->   0054.jpg | 21.95dB
22-11-05 19:08:59.947 : --55-->   0055.jpg | 35.20dB
22-11-05 19:09:06.297 : --56-->   0056.jpg | 24.43dB
22-11-05 19:09:10.636 : --57-->   0057.jpg | 30.28dB
22-11-05 19:09:15.895 : --58-->   0058.jpg | 26.42dB
22-11-05 19:09:23.850 : --59-->   0059.jpg | 27.61dB
22-11-05 19:09:30.864 : --60-->   0060.jpg | 24.62dB
22-11-05 19:09:36.092 : --61-->   0061.jpg | 26.06dB
22-11-05 19:09:40.111 : --62-->   0062.jpg | 24.22dB
22-11-05 19:09:47.758 : --63-->   0063.jpg | 23.04dB
22-11-05 19:09:55.179 : --64-->   0064.jpg | 28.54dB
22-11-05 19:10:02.689 : --65-->   0065.jpg | 34.76dB
22-11-05 19:10:08.398 : --66-->   0066.jpg | 29.94dB
22-11-05 19:10:13.363 : --67-->   0067.jpg | 26.16dB
22-11-05 19:10:17.906 : --68-->   0068.jpg | 25.10dB
22-11-05 19:10:22.275 : --69-->   0069.jpg | 19.44dB
22-11-05 19:10:27.683 : --70-->   0070.jpg | 21.02dB
22-11-05 19:10:33.525 : --71-->   0071.jpg | 22.53dB
22-11-05 19:10:36.968 : --72-->   0072.jpg | 27.80dB
22-11-05 19:10:40.545 : --73-->   0073.jpg | 27.30dB
22-11-05 19:10:47.039 : --74-->   0074.jpg | 24.04dB
22-11-05 19:10:54.540 : --75-->   0075.jpg | 24.34dB
22-11-05 19:10:58.093 : --76-->   0076.jpg | 25.16dB
22-11-05 19:11:00.557 : --77-->   0077.jpg | 22.31dB
22-11-05 19:11:01.732 : --78-->   0078.jpg | 27.36dB
22-11-05 19:11:06.843 : --79-->   0079.jpg | 20.43dB
22-11-05 19:11:12.189 : --80-->   0080.jpg | 29.41dB
22-11-05 19:11:16.796 : --81-->   0081.jpg | 28.53dB
22-11-05 19:11:18.741 : --82-->   0082.jpg | 28.13dB
22-11-05 19:11:21.052 : --83-->   0083.jpg | 22.66dB
22-11-05 19:11:23.455 : --84-->   0084.jpg | 28.58dB
22-11-05 19:11:30.667 : --85-->   0085.jpg | 20.84dB
22-11-05 19:11:38.794 : --86-->   0086.jpg | 24.34dB
22-11-05 19:11:44.425 : --87-->   0087.jpg | 20.80dB
22-11-05 19:11:50.319 : --88-->   0088.jpg | 26.82dB
22-11-05 19:11:54.218 : --89-->   0089.jpg | 23.79dB
22-11-05 19:11:59.745 : --90-->   0090.jpg | 21.78dB
22-11-05 19:12:04.281 : --91-->   0091.jpg | 30.16dB
22-11-05 19:12:09.836 : --92-->   0092.jpg | 21.95dB
22-11-05 19:12:12.532 : --93-->   0093.jpg | 27.29dB
22-11-05 19:12:19.110 : --94-->   0094.jpg | 30.18dB
22-11-05 19:12:23.556 : --95-->   0095.jpg | 25.13dB
22-11-05 19:12:28.301 : --96-->   0096.jpg | 20.69dB
22-11-05 19:12:34.854 : --97-->   0097.jpg | 32.38dB
22-11-05 19:12:39.466 : --98-->   0098.jpg | 25.27dB
22-11-05 19:12:41.797 : --99-->   0099.jpg | 24.71dB
22-11-05 19:12:49.783 : -100-->   0100.jpg | 23.23dB
22-11-05 19:12:50.212 : <epoch:101, iter:  30,000, Average PSNR : 25.98dB

22-11-05 19:15:13.335 : <epoch:101, iter:  30,200, lr:2.000e-04> G_loss: 3.555e-02 
22-11-05 19:17:47.577 : <epoch:102, iter:  30,400, lr:2.000e-04> G_loss: 3.453e-02 
22-11-05 19:20:26.459 : <epoch:103, iter:  30,600, lr:2.000e-04> G_loss: 3.104e-02 
22-11-05 19:22:53.300 : <epoch:103, iter:  30,800, lr:2.000e-04> G_loss: 3.377e-02 
22-11-05 19:25:18.561 : <epoch:104, iter:  31,000, lr:2.000e-04> G_loss: 4.028e-02 
22-11-05 19:27:46.606 : <epoch:105, iter:  31,200, lr:2.000e-04> G_loss: 3.409e-02 
22-11-05 19:30:08.505 : <epoch:105, iter:  31,400, lr:2.000e-04> G_loss: 3.088e-02 
22-11-05 19:32:38.221 : <epoch:106, iter:  31,600, lr:2.000e-04> G_loss: 3.256e-02 
22-11-05 19:35:22.167 : <epoch:107, iter:  31,800, lr:2.000e-04> G_loss: 3.584e-02 
22-11-05 19:37:48.521 : <epoch:107, iter:  32,000, lr:2.000e-04> G_loss: 3.659e-02 
22-11-05 19:40:21.636 : <epoch:108, iter:  32,200, lr:2.000e-04> G_loss: 3.046e-02 
22-11-05 19:43:01.337 : <epoch:109, iter:  32,400, lr:2.000e-04> G_loss: 3.310e-02 
22-11-05 19:45:23.980 : <epoch:109, iter:  32,600, lr:2.000e-04> G_loss: 3.313e-02 
22-11-05 19:47:57.722 : <epoch:110, iter:  32,800, lr:2.000e-04> G_loss: 3.154e-02 
22-11-05 19:50:26.819 : <epoch:111, iter:  33,000, lr:2.000e-04> G_loss: 2.693e-02 
22-11-05 19:52:57.767 : <epoch:111, iter:  33,200, lr:2.000e-04> G_loss: 2.965e-02 
22-11-05 19:55:18.572 : <epoch:112, iter:  33,400, lr:2.000e-04> G_loss: 3.385e-02 
22-11-05 19:57:54.691 : <epoch:113, iter:  33,600, lr:2.000e-04> G_loss: 4.133e-02 
22-11-05 20:00:29.179 : <epoch:113, iter:  33,800, lr:2.000e-04> G_loss: 3.342e-02 
22-11-05 20:03:00.441 : <epoch:114, iter:  34,000, lr:2.000e-04> G_loss: 3.119e-02 
22-11-05 20:05:31.320 : <epoch:115, iter:  34,200, lr:2.000e-04> G_loss: 2.654e-02 
22-11-05 20:07:53.829 : <epoch:115, iter:  34,400, lr:2.000e-04> G_loss: 4.343e-02 
22-11-05 20:10:27.878 : <epoch:116, iter:  34,600, lr:2.000e-04> G_loss: 3.179e-02 
22-11-05 20:12:58.928 : <epoch:117, iter:  34,800, lr:2.000e-04> G_loss: 3.542e-02 
22-11-05 20:15:27.194 : <epoch:117, iter:  35,000, lr:2.000e-04> G_loss: 3.647e-02 
22-11-05 20:15:27.195 : Saving the model.
22-11-05 20:15:34.663 : ---1-->   0001.jpg | 24.04dB
22-11-05 20:15:45.182 : ---2-->   0002.jpg | 25.49dB
22-11-05 20:15:51.048 : ---3-->   0003.jpg | 29.44dB
22-11-05 20:15:55.320 : ---4-->   0004.jpg | 28.34dB
22-11-05 20:16:01.731 : ---5-->   0005.jpg | 28.59dB
22-11-05 20:16:07.953 : ---6-->   0006.jpg | 31.78dB
22-11-05 20:16:12.599 : ---7-->   0007.jpg | 33.63dB
22-11-05 20:16:19.056 : ---8-->   0008.jpg | 23.93dB
22-11-05 20:16:24.805 : ---9-->   0009.jpg | 23.24dB
22-11-05 20:16:28.103 : --10-->   0010.jpg | 27.75dB
22-11-05 20:16:34.595 : --11-->   0011.jpg | 25.74dB
22-11-05 20:16:38.530 : --12-->   0012.jpg | 23.97dB
22-11-05 20:16:41.845 : --13-->   0013.jpg | 23.88dB
22-11-05 20:16:48.411 : --14-->   0014.jpg | 23.12dB
22-11-05 20:16:55.228 : --15-->   0015.jpg | 21.09dB
22-11-05 20:16:57.188 : --16-->   0016.jpg | 29.19dB
22-11-05 20:16:59.158 : --17-->   0017.jpg | 28.63dB
22-11-05 20:17:04.729 : --18-->   0018.jpg | 26.54dB
22-11-05 20:17:12.740 : --19-->   0019.jpg | 31.95dB
22-11-05 20:17:20.767 : --20-->   0020.jpg | 23.52dB
22-11-05 20:17:27.893 : --21-->   0021.jpg | 26.33dB
22-11-05 20:17:30.797 : --22-->   0022.jpg | 38.31dB
22-11-05 20:17:34.586 : --23-->   0023.jpg | 25.21dB
22-11-05 20:17:38.839 : --24-->   0024.jpg | 33.28dB
22-11-05 20:17:44.317 : --25-->   0025.jpg | 25.70dB
22-11-05 20:17:46.645 : --26-->   0026.jpg | 26.10dB
22-11-05 20:17:55.876 : --27-->   0027.jpg | 28.74dB
22-11-05 20:17:59.005 : --28-->   0028.jpg | 29.87dB
22-11-05 20:18:01.761 : --29-->   0029.jpg | 24.14dB
22-11-05 20:18:04.016 : --30-->   0030.jpg | 28.70dB
22-11-05 20:18:05.855 : --31-->   0031.jpg | 27.92dB
22-11-05 20:18:10.483 : --32-->   0032.jpg | 25.43dB
22-11-05 20:18:14.688 : --33-->   0033.jpg | 23.00dB
22-11-05 20:18:21.623 : --34-->   0034.jpg | 23.10dB
22-11-05 20:18:23.929 : --35-->   0035.jpg | 26.55dB
22-11-05 20:18:28.346 : --36-->   0036.jpg | 23.16dB
22-11-05 20:18:36.388 : --37-->   0037.jpg | 22.78dB
22-11-05 20:18:39.856 : --38-->   0038.jpg | 26.02dB
22-11-05 20:18:43.525 : --39-->   0039.jpg | 23.59dB
22-11-05 20:18:46.496 : --40-->   0040.jpg | 28.01dB
22-11-05 20:18:52.754 : --41-->   0041.jpg | 20.92dB
22-11-05 20:18:58.210 : --42-->   0042.jpg | 26.46dB
22-11-05 20:19:02.689 : --43-->   0043.jpg | 26.90dB
22-11-05 20:19:04.509 : --44-->   0044.jpg | 24.52dB
22-11-05 20:19:07.746 : --45-->   0045.jpg | 21.41dB
22-11-05 20:19:12.080 : --46-->   0046.jpg | 23.14dB
22-11-05 20:19:19.708 : --47-->   0047.jpg | 24.10dB
22-11-05 20:19:25.451 : --48-->   0048.jpg | 27.55dB
22-11-05 20:19:30.790 : --49-->   0049.jpg | 27.73dB
22-11-05 20:19:39.956 : --50-->   0050.jpg | 20.80dB
22-11-05 20:19:49.114 : --51-->   0051.jpg | 25.11dB
22-11-05 20:19:56.436 : --52-->   0052.jpg | 23.19dB
22-11-05 20:19:59.664 : --53-->   0053.jpg | 24.27dB
22-11-05 20:20:05.970 : --54-->   0054.jpg | 21.97dB
22-11-05 20:20:11.329 : --55-->   0055.jpg | 35.28dB
22-11-05 20:20:13.993 : --56-->   0056.jpg | 24.10dB
22-11-05 20:20:18.198 : --57-->   0057.jpg | 29.81dB
22-11-05 20:20:21.405 : --58-->   0058.jpg | 26.35dB
22-11-05 20:20:24.980 : --59-->   0059.jpg | 27.36dB
22-11-05 20:20:26.376 : --60-->   0060.jpg | 24.48dB
22-11-05 20:20:28.865 : --61-->   0061.jpg | 25.72dB
22-11-05 20:20:29.873 : --62-->   0062.jpg | 24.05dB
22-11-05 20:20:31.770 : --63-->   0063.jpg | 23.11dB
22-11-05 20:20:34.200 : --64-->   0064.jpg | 28.26dB
22-11-05 20:20:37.004 : --65-->   0065.jpg | 34.28dB
22-11-05 20:20:39.877 : --66-->   0066.jpg | 29.80dB
22-11-05 20:20:42.075 : --67-->   0067.jpg | 26.34dB
22-11-05 20:20:46.296 : --68-->   0068.jpg | 25.03dB
22-11-05 20:20:52.578 : --69-->   0069.jpg | 19.39dB
22-11-05 20:20:59.624 : --70-->   0070.jpg | 20.83dB
22-11-05 20:21:06.648 : --71-->   0071.jpg | 22.50dB
22-11-05 20:21:11.593 : --72-->   0072.jpg | 27.80dB
22-11-05 20:21:17.845 : --73-->   0073.jpg | 27.31dB
22-11-05 20:21:23.745 : --74-->   0074.jpg | 23.96dB
22-11-05 20:21:28.584 : --75-->   0075.jpg | 24.33dB
22-11-05 20:21:30.752 : --76-->   0076.jpg | 24.98dB
22-11-05 20:21:35.403 : --77-->   0077.jpg | 22.01dB
22-11-05 20:21:37.118 : --78-->   0078.jpg | 27.13dB
22-11-05 20:21:42.700 : --79-->   0079.jpg | 20.38dB
22-11-05 20:21:49.603 : --80-->   0080.jpg | 29.24dB
22-11-05 20:22:00.095 : --81-->   0081.jpg | 28.51dB
22-11-05 20:22:03.164 : --82-->   0082.jpg | 28.05dB
22-11-05 20:22:07.107 : --83-->   0083.jpg | 22.54dB
22-11-05 20:22:10.920 : --84-->   0084.jpg | 28.14dB
22-11-05 20:22:16.203 : --85-->   0085.jpg | 20.86dB
22-11-05 20:22:18.866 : --86-->   0086.jpg | 24.17dB
22-11-05 20:22:23.566 : --87-->   0087.jpg | 20.69dB
22-11-05 20:22:25.708 : --88-->   0088.jpg | 26.69dB
22-11-05 20:22:30.799 : --89-->   0089.jpg | 23.56dB
22-11-05 20:22:34.721 : --90-->   0090.jpg | 21.74dB
22-11-05 20:22:40.760 : --91-->   0091.jpg | 29.97dB
22-11-05 20:22:48.648 : --92-->   0092.jpg | 22.17dB
22-11-05 20:22:51.324 : --93-->   0093.jpg | 27.19dB
22-11-05 20:22:53.379 : --94-->   0094.jpg | 30.18dB
22-11-05 20:22:55.511 : --95-->   0095.jpg | 25.11dB
22-11-05 20:23:05.915 : --96-->   0096.jpg | 20.67dB
22-11-05 20:23:12.649 : --97-->   0097.jpg | 32.42dB
22-11-05 20:23:19.630 : --98-->   0098.jpg | 25.16dB
22-11-05 20:23:25.013 : --99-->   0099.jpg | 24.59dB
22-11-05 20:23:27.852 : -100-->   0100.jpg | 23.28dB
22-11-05 20:23:28.981 : <epoch:117, iter:  35,000, Average PSNR : 25.87dB

22-11-05 20:26:00.361 : <epoch:118, iter:  35,200, lr:2.000e-04> G_loss: 4.037e-02 
22-11-05 20:28:37.125 : <epoch:119, iter:  35,400, lr:2.000e-04> G_loss: 3.433e-02 
22-11-05 20:31:15.386 : <epoch:119, iter:  35,600, lr:2.000e-04> G_loss: 3.928e-02 
22-11-05 20:33:42.648 : <epoch:120, iter:  35,800, lr:2.000e-04> G_loss: 2.908e-02 
22-11-05 20:36:10.449 : <epoch:121, iter:  36,000, lr:2.000e-04> G_loss: 3.360e-02 
22-11-05 20:38:41.967 : <epoch:121, iter:  36,200, lr:2.000e-04> G_loss: 3.210e-02 
22-11-05 20:41:21.283 : <epoch:122, iter:  36,400, lr:2.000e-04> G_loss: 3.547e-02 
22-11-05 20:43:50.796 : <epoch:123, iter:  36,600, lr:2.000e-04> G_loss: 3.970e-02 
22-11-05 20:46:16.978 : <epoch:123, iter:  36,800, lr:2.000e-04> G_loss: 3.445e-02 
22-11-05 20:48:53.225 : <epoch:124, iter:  37,000, lr:2.000e-04> G_loss: 3.608e-02 
22-11-05 20:51:23.465 : <epoch:125, iter:  37,200, lr:2.000e-04> G_loss: 2.625e-02 
22-11-05 20:53:50.827 : <epoch:125, iter:  37,400, lr:2.000e-04> G_loss: 2.668e-02 
22-11-05 20:56:26.251 : <epoch:126, iter:  37,600, lr:2.000e-04> G_loss: 3.250e-02 
22-11-05 20:59:01.241 : <epoch:127, iter:  37,800, lr:2.000e-04> G_loss: 3.386e-02 
22-11-05 21:01:13.020 : <epoch:127, iter:  38,000, lr:2.000e-04> G_loss: 3.381e-02 
22-11-05 21:03:51.293 : <epoch:128, iter:  38,200, lr:2.000e-04> G_loss: 3.308e-02 
22-11-05 21:06:16.108 : <epoch:129, iter:  38,400, lr:2.000e-04> G_loss: 2.822e-02 
22-11-05 21:08:45.748 : <epoch:129, iter:  38,600, lr:2.000e-04> G_loss: 2.842e-02 
22-11-05 21:11:17.315 : <epoch:130, iter:  38,800, lr:2.000e-04> G_loss: 3.531e-02 
22-11-05 21:13:53.819 : <epoch:131, iter:  39,000, lr:2.000e-04> G_loss: 2.912e-02 
22-11-05 21:16:15.954 : <epoch:131, iter:  39,200, lr:2.000e-04> G_loss: 3.614e-02 
22-11-05 21:18:51.010 : <epoch:132, iter:  39,400, lr:2.000e-04> G_loss: 4.059e-02 
22-11-05 21:21:21.217 : <epoch:133, iter:  39,600, lr:2.000e-04> G_loss: 3.206e-02 
22-11-05 21:23:56.643 : <epoch:134, iter:  39,800, lr:2.000e-04> G_loss: 3.754e-02 
22-11-05 21:26:26.462 : <epoch:134, iter:  40,000, lr:2.000e-04> G_loss: 2.504e-02 
22-11-05 21:26:26.463 : Saving the model.
22-11-05 21:26:32.438 : ---1-->   0001.jpg | 24.21dB
22-11-05 21:26:39.890 : ---2-->   0002.jpg | 25.63dB
22-11-05 21:26:48.242 : ---3-->   0003.jpg | 29.74dB
22-11-05 21:26:52.995 : ---4-->   0004.jpg | 28.22dB
22-11-05 21:26:56.875 : ---5-->   0005.jpg | 28.70dB
22-11-05 21:27:01.588 : ---6-->   0006.jpg | 32.49dB
22-11-05 21:27:08.154 : ---7-->   0007.jpg | 34.23dB
22-11-05 21:27:17.116 : ---8-->   0008.jpg | 24.01dB
22-11-05 21:27:21.181 : ---9-->   0009.jpg | 23.30dB
22-11-05 21:27:25.092 : --10-->   0010.jpg | 27.85dB
22-11-05 21:27:28.467 : --11-->   0011.jpg | 25.71dB
22-11-05 21:27:37.009 : --12-->   0012.jpg | 24.14dB
22-11-05 21:27:42.103 : --13-->   0013.jpg | 23.96dB
22-11-05 21:27:50.063 : --14-->   0014.jpg | 23.12dB
22-11-05 21:28:00.426 : --15-->   0015.jpg | 21.55dB
22-11-05 21:28:06.065 : --16-->   0016.jpg | 29.81dB
22-11-05 21:28:08.713 : --17-->   0017.jpg | 28.86dB
22-11-05 21:28:17.632 : --18-->   0018.jpg | 26.76dB
22-11-05 21:28:21.458 : --19-->   0019.jpg | 31.97dB
22-11-05 21:28:29.606 : --20-->   0020.jpg | 23.65dB
22-11-05 21:28:35.849 : --21-->   0021.jpg | 26.26dB
22-11-05 21:28:40.144 : --22-->   0022.jpg | 37.75dB
22-11-05 21:28:46.712 : --23-->   0023.jpg | 25.12dB
22-11-05 21:28:55.853 : --24-->   0024.jpg | 33.75dB
22-11-05 21:28:59.466 : --25-->   0025.jpg | 25.78dB
22-11-05 21:29:04.361 : --26-->   0026.jpg | 26.43dB
22-11-05 21:29:14.268 : --27-->   0027.jpg | 29.02dB
22-11-05 21:29:18.897 : --28-->   0028.jpg | 30.20dB
22-11-05 21:29:26.473 : --29-->   0029.jpg | 24.14dB
22-11-05 21:29:33.240 : --30-->   0030.jpg | 29.08dB
22-11-05 21:29:39.132 : --31-->   0031.jpg | 28.12dB
22-11-05 21:29:44.351 : --32-->   0032.jpg | 25.84dB
22-11-05 21:29:52.164 : --33-->   0033.jpg | 23.29dB
22-11-05 21:29:57.161 : --34-->   0034.jpg | 23.19dB
22-11-05 21:29:58.468 : --35-->   0035.jpg | 26.51dB
22-11-05 21:30:04.437 : --36-->   0036.jpg | 23.18dB
22-11-05 21:30:10.434 : --37-->   0037.jpg | 22.82dB
22-11-05 21:30:14.822 : --38-->   0038.jpg | 26.23dB
22-11-05 21:30:17.476 : --39-->   0039.jpg | 23.81dB
22-11-05 21:30:23.498 : --40-->   0040.jpg | 28.11dB
22-11-05 21:30:27.163 : --41-->   0041.jpg | 21.06dB
22-11-05 21:30:34.382 : --42-->   0042.jpg | 26.55dB
22-11-05 21:30:40.989 : --43-->   0043.jpg | 27.22dB
22-11-05 21:30:48.184 : --44-->   0044.jpg | 24.66dB
22-11-05 21:30:54.639 : --45-->   0045.jpg | 21.46dB
22-11-05 21:31:04.472 : --46-->   0046.jpg | 23.26dB
22-11-05 21:31:10.175 : --47-->   0047.jpg | 24.19dB
22-11-05 21:31:14.033 : --48-->   0048.jpg | 28.07dB
22-11-05 21:31:16.901 : --49-->   0049.jpg | 27.95dB
22-11-05 21:31:21.840 : --50-->   0050.jpg | 20.90dB
22-11-05 21:31:28.687 : --51-->   0051.jpg | 25.14dB
22-11-05 21:31:35.646 : --52-->   0052.jpg | 23.41dB
22-11-05 21:31:42.418 : --53-->   0053.jpg | 24.32dB
22-11-05 21:31:49.510 : --54-->   0054.jpg | 21.99dB
22-11-05 21:32:02.510 : --55-->   0055.jpg | 35.60dB
22-11-05 21:32:06.457 : --56-->   0056.jpg | 24.37dB
22-11-05 21:32:11.573 : --57-->   0057.jpg | 30.24dB
22-11-05 21:32:21.025 : --58-->   0058.jpg | 26.51dB
22-11-05 21:32:27.820 : --59-->   0059.jpg | 27.73dB
22-11-05 21:32:34.448 : --60-->   0060.jpg | 24.62dB
22-11-05 21:32:40.305 : --61-->   0061.jpg | 26.07dB
22-11-05 21:32:47.140 : --62-->   0062.jpg | 24.24dB
22-11-05 21:32:50.058 : --63-->   0063.jpg | 23.08dB
22-11-05 21:32:53.906 : --64-->   0064.jpg | 28.73dB
22-11-05 21:32:57.123 : --65-->   0065.jpg | 34.80dB
22-11-05 21:33:00.141 : --66-->   0066.jpg | 29.99dB
22-11-05 21:33:09.997 : --67-->   0067.jpg | 26.30dB
22-11-05 21:33:14.453 : --68-->   0068.jpg | 25.08dB
22-11-05 21:33:21.449 : --69-->   0069.jpg | 19.45dB
22-11-05 21:33:30.609 : --70-->   0070.jpg | 21.05dB
22-11-05 21:33:34.298 : --71-->   0071.jpg | 22.55dB
22-11-05 21:33:39.169 : --72-->   0072.jpg | 27.81dB
22-11-05 21:33:41.834 : --73-->   0073.jpg | 27.35dB
22-11-05 21:33:45.561 : --74-->   0074.jpg | 24.21dB
22-11-05 21:33:47.451 : --75-->   0075.jpg | 24.40dB
22-11-05 21:33:49.918 : --76-->   0076.jpg | 25.04dB
22-11-05 21:33:52.974 : --77-->   0077.jpg | 22.35dB
22-11-05 21:33:56.930 : --78-->   0078.jpg | 27.46dB
22-11-05 21:34:06.831 : --79-->   0079.jpg | 20.45dB
22-11-05 21:34:13.129 : --80-->   0080.jpg | 29.51dB
22-11-05 21:34:17.979 : --81-->   0081.jpg | 28.58dB
22-11-05 21:34:22.547 : --82-->   0082.jpg | 28.17dB
22-11-05 21:34:27.371 : --83-->   0083.jpg | 22.63dB
22-11-05 21:34:35.447 : --84-->   0084.jpg | 28.66dB
22-11-05 21:34:44.754 : --85-->   0085.jpg | 20.94dB
22-11-05 21:34:51.396 : --86-->   0086.jpg | 24.16dB
22-11-05 21:34:56.874 : --87-->   0087.jpg | 20.79dB
22-11-05 21:35:04.002 : --88-->   0088.jpg | 26.75dB
22-11-05 21:35:07.477 : --89-->   0089.jpg | 23.77dB
22-11-05 21:35:11.366 : --90-->   0090.jpg | 21.76dB
22-11-05 21:35:16.438 : --91-->   0091.jpg | 30.20dB
22-11-05 21:35:22.886 : --92-->   0092.jpg | 22.19dB
22-11-05 21:35:30.715 : --93-->   0093.jpg | 27.18dB
22-11-05 21:35:39.167 : --94-->   0094.jpg | 30.19dB
22-11-05 21:35:46.654 : --95-->   0095.jpg | 25.16dB
22-11-05 21:35:51.351 : --96-->   0096.jpg | 20.68dB
22-11-05 21:35:58.601 : --97-->   0097.jpg | 32.49dB
22-11-05 21:36:00.986 : --98-->   0098.jpg | 25.19dB
22-11-05 21:36:05.304 : --99-->   0099.jpg | 24.77dB
22-11-05 21:36:12.557 : -100-->   0100.jpg | 23.33dB
22-11-05 21:36:12.727 : <epoch:134, iter:  40,000, Average PSNR : 26.03dB

22-11-05 21:38:45.309 : <epoch:135, iter:  40,200, lr:2.000e-04> G_loss: 3.311e-02 
22-11-05 21:41:13.291 : <epoch:136, iter:  40,400, lr:2.000e-04> G_loss: 2.999e-02 
22-11-05 21:43:27.337 : <epoch:136, iter:  40,600, lr:2.000e-04> G_loss: 2.743e-02 
22-11-05 21:45:53.056 : <epoch:137, iter:  40,800, lr:2.000e-04> G_loss: 3.540e-02 
22-11-05 21:48:20.130 : <epoch:138, iter:  41,000, lr:2.000e-04> G_loss: 3.596e-02 
22-11-05 21:50:44.257 : <epoch:138, iter:  41,200, lr:2.000e-04> G_loss: 3.341e-02 
22-11-05 21:53:05.947 : <epoch:139, iter:  41,400, lr:2.000e-04> G_loss: 3.722e-02 
22-11-05 21:55:29.599 : <epoch:140, iter:  41,600, lr:2.000e-04> G_loss: 3.216e-02 
22-11-05 21:57:56.340 : <epoch:140, iter:  41,800, lr:2.000e-04> G_loss: 3.655e-02 
22-11-05 22:00:28.757 : <epoch:141, iter:  42,000, lr:2.000e-04> G_loss: 3.219e-02 
22-11-05 22:03:02.502 : <epoch:142, iter:  42,200, lr:2.000e-04> G_loss: 3.289e-02 
22-11-05 22:05:33.550 : <epoch:142, iter:  42,400, lr:2.000e-04> G_loss: 3.251e-02 
22-11-05 22:08:05.873 : <epoch:143, iter:  42,600, lr:2.000e-04> G_loss: 3.442e-02 
22-11-05 22:10:35.788 : <epoch:144, iter:  42,800, lr:2.000e-04> G_loss: 5.218e-02 
22-11-05 22:13:00.626 : <epoch:144, iter:  43,000, lr:2.000e-04> G_loss: 2.983e-02 
22-11-05 22:15:20.932 : <epoch:145, iter:  43,200, lr:2.000e-04> G_loss: 4.280e-02 
22-11-05 22:17:49.701 : <epoch:146, iter:  43,400, lr:2.000e-04> G_loss: 3.290e-02 
22-11-05 22:20:16.755 : <epoch:146, iter:  43,600, lr:2.000e-04> G_loss: 2.725e-02 
22-11-05 22:22:49.815 : <epoch:147, iter:  43,800, lr:2.000e-04> G_loss: 3.375e-02 
22-11-05 22:25:25.849 : <epoch:148, iter:  44,000, lr:2.000e-04> G_loss: 3.190e-02 
22-11-05 22:27:53.186 : <epoch:148, iter:  44,200, lr:2.000e-04> G_loss: 3.276e-02 
22-11-05 22:30:28.515 : <epoch:149, iter:  44,400, lr:2.000e-04> G_loss: 4.344e-02 
22-11-05 22:32:49.774 : <epoch:150, iter:  44,600, lr:2.000e-04> G_loss: 2.557e-02 
22-11-05 22:35:19.484 : <epoch:150, iter:  44,800, lr:2.000e-04> G_loss: 3.356e-02 
22-11-05 22:37:51.868 : <epoch:151, iter:  45,000, lr:2.000e-04> G_loss: 3.557e-02 
22-11-05 22:37:51.870 : Saving the model.
22-11-05 22:38:03.324 : ---1-->   0001.jpg | 24.38dB
22-11-05 22:38:06.194 : ---2-->   0002.jpg | 25.62dB
22-11-05 22:38:14.783 : ---3-->   0003.jpg | 29.62dB
22-11-05 22:38:21.020 : ---4-->   0004.jpg | 27.98dB
22-11-05 22:38:27.341 : ---5-->   0005.jpg | 28.76dB
22-11-05 22:38:30.648 : ---6-->   0006.jpg | 32.36dB
22-11-05 22:38:33.142 : ---7-->   0007.jpg | 34.36dB
22-11-05 22:38:40.075 : ---8-->   0008.jpg | 24.07dB
22-11-05 22:38:47.618 : ---9-->   0009.jpg | 23.37dB
22-11-05 22:38:52.470 : --10-->   0010.jpg | 28.00dB
22-11-05 22:39:02.607 : --11-->   0011.jpg | 25.88dB
22-11-05 22:39:06.224 : --12-->   0012.jpg | 24.14dB
22-11-05 22:39:10.645 : --13-->   0013.jpg | 24.05dB
22-11-05 22:39:14.925 : --14-->   0014.jpg | 23.27dB
22-11-05 22:39:23.561 : --15-->   0015.jpg | 21.45dB
22-11-05 22:39:28.590 : --16-->   0016.jpg | 29.69dB
22-11-05 22:39:33.976 : --17-->   0017.jpg | 28.80dB
22-11-05 22:39:36.595 : --18-->   0018.jpg | 26.81dB
22-11-05 22:39:43.911 : --19-->   0019.jpg | 32.25dB
22-11-05 22:39:48.447 : --20-->   0020.jpg | 23.75dB
22-11-05 22:39:57.054 : --21-->   0021.jpg | 26.37dB
22-11-05 22:40:02.444 : --22-->   0022.jpg | 38.36dB
22-11-05 22:40:08.390 : --23-->   0023.jpg | 25.30dB
22-11-05 22:40:13.032 : --24-->   0024.jpg | 33.81dB
22-11-05 22:40:18.309 : --25-->   0025.jpg | 25.82dB
22-11-05 22:40:22.345 : --26-->   0026.jpg | 26.57dB
22-11-05 22:40:28.935 : --27-->   0027.jpg | 28.94dB
22-11-05 22:40:35.763 : --28-->   0028.jpg | 30.26dB
22-11-05 22:40:42.223 : --29-->   0029.jpg | 24.15dB
22-11-05 22:40:45.654 : --30-->   0030.jpg | 29.03dB
22-11-05 22:40:48.223 : --31-->   0031.jpg | 28.07dB
22-11-05 22:40:53.646 : --32-->   0032.jpg | 25.82dB
22-11-05 22:40:55.102 : --33-->   0033.jpg | 23.36dB
22-11-05 22:41:02.748 : --34-->   0034.jpg | 23.20dB
22-11-05 22:41:08.416 : --35-->   0035.jpg | 26.54dB
22-11-05 22:41:11.933 : --36-->   0036.jpg | 23.21dB
22-11-05 22:41:15.718 : --37-->   0037.jpg | 22.87dB
22-11-05 22:41:21.926 : --38-->   0038.jpg | 26.32dB
22-11-05 22:41:26.330 : --39-->   0039.jpg | 23.96dB
22-11-05 22:41:32.104 : --40-->   0040.jpg | 28.09dB
22-11-05 22:41:42.031 : --41-->   0041.jpg | 21.06dB
22-11-05 22:41:50.282 : --42-->   0042.jpg | 26.64dB
22-11-05 22:41:58.492 : --43-->   0043.jpg | 27.08dB
22-11-05 22:42:00.859 : --44-->   0044.jpg | 24.64dB
22-11-05 22:42:07.111 : --45-->   0045.jpg | 21.55dB
22-11-05 22:42:13.267 : --46-->   0046.jpg | 23.27dB
22-11-05 22:42:16.955 : --47-->   0047.jpg | 24.31dB
22-11-05 22:42:23.886 : --48-->   0048.jpg | 28.10dB
22-11-05 22:42:26.092 : --49-->   0049.jpg | 28.07dB
22-11-05 22:42:28.836 : --50-->   0050.jpg | 20.93dB
22-11-05 22:42:35.216 : --51-->   0051.jpg | 25.37dB
22-11-05 22:42:40.695 : --52-->   0052.jpg | 23.40dB
22-11-05 22:42:42.635 : --53-->   0053.jpg | 24.45dB
22-11-05 22:42:49.693 : --54-->   0054.jpg | 22.01dB
22-11-05 22:42:51.554 : --55-->   0055.jpg | 36.08dB
22-11-05 22:42:57.686 : --56-->   0056.jpg | 24.62dB
22-11-05 22:43:02.561 : --57-->   0057.jpg | 30.15dB
22-11-05 22:43:08.828 : --58-->   0058.jpg | 26.58dB
22-11-05 22:43:11.205 : --59-->   0059.jpg | 27.70dB
22-11-05 22:43:20.058 : --60-->   0060.jpg | 24.71dB
22-11-05 22:43:26.582 : --61-->   0061.jpg | 26.12dB
22-11-05 22:43:34.670 : --62-->   0062.jpg | 24.25dB
22-11-05 22:43:39.118 : --63-->   0063.jpg | 23.14dB
22-11-05 22:43:47.665 : --64-->   0064.jpg | 28.67dB
22-11-05 22:43:54.700 : --65-->   0065.jpg | 34.77dB
22-11-05 22:44:01.209 : --66-->   0066.jpg | 30.14dB
22-11-05 22:44:06.747 : --67-->   0067.jpg | 26.23dB
22-11-05 22:44:15.162 : --68-->   0068.jpg | 25.13dB
22-11-05 22:44:19.378 : --69-->   0069.jpg | 19.46dB
22-11-05 22:44:22.560 : --70-->   0070.jpg | 21.05dB
22-11-05 22:44:31.684 : --71-->   0071.jpg | 22.52dB
22-11-05 22:44:34.766 : --72-->   0072.jpg | 27.92dB
22-11-05 22:44:39.539 : --73-->   0073.jpg | 27.17dB
22-11-05 22:44:43.227 : --74-->   0074.jpg | 24.10dB
22-11-05 22:44:53.616 : --75-->   0075.jpg | 24.44dB
22-11-05 22:44:57.950 : --76-->   0076.jpg | 25.15dB
22-11-05 22:45:04.155 : --77-->   0077.jpg | 22.38dB
22-11-05 22:45:05.304 : --78-->   0078.jpg | 27.45dB
22-11-05 22:45:07.752 : --79-->   0079.jpg | 20.45dB
22-11-05 22:45:11.563 : --80-->   0080.jpg | 29.63dB
22-11-05 22:45:15.309 : --81-->   0081.jpg | 28.58dB
22-11-05 22:45:19.327 : --82-->   0082.jpg | 28.19dB
22-11-05 22:45:22.046 : --83-->   0083.jpg | 22.72dB
22-11-05 22:45:26.605 : --84-->   0084.jpg | 28.50dB
22-11-05 22:45:32.264 : --85-->   0085.jpg | 20.85dB
22-11-05 22:45:37.925 : --86-->   0086.jpg | 24.30dB
22-11-05 22:45:44.126 : --87-->   0087.jpg | 20.81dB
22-11-05 22:45:47.376 : --88-->   0088.jpg | 26.88dB
22-11-05 22:45:51.641 : --89-->   0089.jpg | 23.76dB
22-11-05 22:45:55.970 : --90-->   0090.jpg | 21.75dB
22-11-05 22:45:59.536 : --91-->   0091.jpg | 30.38dB
22-11-05 22:46:03.190 : --92-->   0092.jpg | 22.13dB
22-11-05 22:46:07.882 : --93-->   0093.jpg | 27.37dB
22-11-05 22:46:15.043 : --94-->   0094.jpg | 30.27dB
22-11-05 22:46:20.822 : --95-->   0095.jpg | 25.11dB
22-11-05 22:46:26.250 : --96-->   0096.jpg | 20.71dB
22-11-05 22:46:33.151 : --97-->   0097.jpg | 32.46dB
22-11-05 22:46:42.921 : --98-->   0098.jpg | 25.34dB
22-11-05 22:46:45.559 : --99-->   0099.jpg | 24.70dB
22-11-05 22:46:47.439 : -100-->   0100.jpg | 23.29dB
22-11-05 22:46:47.637 : <epoch:151, iter:  45,000, Average PSNR : 26.08dB

22-11-05 22:49:21.151 : <epoch:152, iter:  45,200, lr:2.000e-04> G_loss: 3.290e-02 
22-11-05 22:51:58.563 : <epoch:152, iter:  45,400, lr:2.000e-04> G_loss: 4.049e-02 
22-11-05 22:54:33.192 : <epoch:153, iter:  45,600, lr:2.000e-04> G_loss: 3.476e-02 
22-11-05 22:57:07.868 : <epoch:154, iter:  45,800, lr:2.000e-04> G_loss: 3.791e-02 
22-11-05 22:59:39.631 : <epoch:154, iter:  46,000, lr:2.000e-04> G_loss: 3.865e-02 
22-11-05 23:02:05.613 : <epoch:155, iter:  46,200, lr:2.000e-04> G_loss: 3.470e-02 
22-11-05 23:04:32.724 : <epoch:156, iter:  46,400, lr:2.000e-04> G_loss: 3.282e-02 
22-11-05 23:06:54.483 : <epoch:156, iter:  46,600, lr:2.000e-04> G_loss: 3.443e-02 
22-11-05 23:09:25.876 : <epoch:157, iter:  46,800, lr:2.000e-04> G_loss: 3.134e-02 
22-11-05 23:12:06.344 : <epoch:158, iter:  47,000, lr:2.000e-04> G_loss: 2.978e-02 
22-11-05 23:14:33.176 : <epoch:158, iter:  47,200, lr:2.000e-04> G_loss: 3.669e-02 
22-11-05 23:17:09.417 : <epoch:159, iter:  47,400, lr:2.000e-04> G_loss: 2.894e-02 
22-11-05 23:19:37.434 : <epoch:160, iter:  47,600, lr:2.000e-04> G_loss: 3.163e-02 
22-11-05 23:22:02.133 : <epoch:160, iter:  47,800, lr:2.000e-04> G_loss: 3.487e-02 
22-11-05 23:24:35.350 : <epoch:161, iter:  48,000, lr:2.000e-04> G_loss: 3.961e-02 
22-11-05 23:27:04.735 : <epoch:162, iter:  48,200, lr:2.000e-04> G_loss: 3.170e-02 
22-11-05 23:29:32.740 : <epoch:162, iter:  48,400, lr:2.000e-04> G_loss: 3.600e-02 
22-11-05 23:32:20.382 : <epoch:163, iter:  48,600, lr:2.000e-04> G_loss: 3.988e-02 
22-11-05 23:34:47.834 : <epoch:164, iter:  48,800, lr:2.000e-04> G_loss: 3.401e-02 
22-11-05 23:37:12.103 : <epoch:164, iter:  49,000, lr:2.000e-04> G_loss: 3.960e-02 
22-11-05 23:39:51.837 : <epoch:165, iter:  49,200, lr:2.000e-04> G_loss: 3.278e-02 
22-11-05 23:42:24.739 : <epoch:166, iter:  49,400, lr:2.000e-04> G_loss: 3.655e-02 
22-11-05 23:44:54.210 : <epoch:167, iter:  49,600, lr:2.000e-04> G_loss: 3.676e-02 
22-11-05 23:47:21.361 : <epoch:167, iter:  49,800, lr:2.000e-04> G_loss: 3.212e-02 
22-11-05 23:49:51.991 : <epoch:168, iter:  50,000, lr:2.000e-04> G_loss: 3.390e-02 
22-11-05 23:49:51.994 : Saving the model.
22-11-05 23:50:03.198 : ---1-->   0001.jpg | 24.43dB
22-11-05 23:50:08.252 : ---2-->   0002.jpg | 25.58dB
22-11-05 23:50:17.034 : ---3-->   0003.jpg | 29.71dB
22-11-05 23:50:22.418 : ---4-->   0004.jpg | 28.26dB
22-11-05 23:50:30.791 : ---5-->   0005.jpg | 28.99dB
22-11-05 23:50:36.924 : ---6-->   0006.jpg | 32.27dB
22-11-05 23:50:43.945 : ---7-->   0007.jpg | 34.14dB
22-11-05 23:50:48.369 : ---8-->   0008.jpg | 24.08dB
22-11-05 23:50:53.763 : ---9-->   0009.jpg | 23.41dB
22-11-05 23:50:59.605 : --10-->   0010.jpg | 27.91dB
22-11-05 23:51:03.829 : --11-->   0011.jpg | 25.60dB
22-11-05 23:51:10.008 : --12-->   0012.jpg | 24.05dB
22-11-05 23:51:16.452 : --13-->   0013.jpg | 23.93dB
22-11-05 23:51:21.934 : --14-->   0014.jpg | 23.20dB
22-11-05 23:51:26.949 : --15-->   0015.jpg | 21.46dB
22-11-05 23:51:32.941 : --16-->   0016.jpg | 29.71dB
22-11-05 23:51:36.046 : --17-->   0017.jpg | 29.09dB
22-11-05 23:51:43.775 : --18-->   0018.jpg | 27.04dB
22-11-05 23:51:49.516 : --19-->   0019.jpg | 31.92dB
22-11-05 23:51:51.680 : --20-->   0020.jpg | 23.74dB
22-11-05 23:51:57.168 : --21-->   0021.jpg | 26.38dB
22-11-05 23:52:00.501 : --22-->   0022.jpg | 38.07dB
22-11-05 23:52:03.923 : --23-->   0023.jpg | 25.19dB
22-11-05 23:52:14.738 : --24-->   0024.jpg | 33.33dB
22-11-05 23:52:17.491 : --25-->   0025.jpg | 25.82dB
22-11-05 23:52:21.335 : --26-->   0026.jpg | 26.65dB
22-11-05 23:52:29.303 : --27-->   0027.jpg | 28.91dB
22-11-05 23:52:32.614 : --28-->   0028.jpg | 29.82dB
22-11-05 23:52:35.962 : --29-->   0029.jpg | 24.15dB
22-11-05 23:52:39.022 : --30-->   0030.jpg | 29.13dB
22-11-05 23:52:43.367 : --31-->   0031.jpg | 28.07dB
22-11-05 23:52:47.018 : --32-->   0032.jpg | 25.87dB
22-11-05 23:52:51.800 : --33-->   0033.jpg | 23.31dB
22-11-05 23:52:58.392 : --34-->   0034.jpg | 23.14dB
22-11-05 23:53:07.161 : --35-->   0035.jpg | 26.58dB
22-11-05 23:53:11.776 : --36-->   0036.jpg | 23.22dB
22-11-05 23:53:14.600 : --37-->   0037.jpg | 22.91dB
22-11-05 23:53:23.723 : --38-->   0038.jpg | 26.22dB
22-11-05 23:53:32.222 : --39-->   0039.jpg | 23.99dB
22-11-05 23:53:38.045 : --40-->   0040.jpg | 28.14dB
22-11-05 23:53:42.030 : --41-->   0041.jpg | 21.09dB
22-11-05 23:53:50.543 : --42-->   0042.jpg | 26.57dB
22-11-05 23:53:59.491 : --43-->   0043.jpg | 27.08dB
22-11-05 23:54:04.226 : --44-->   0044.jpg | 24.60dB
22-11-05 23:54:08.273 : --45-->   0045.jpg | 21.39dB
22-11-05 23:54:16.653 : --46-->   0046.jpg | 23.10dB
22-11-05 23:54:23.133 : --47-->   0047.jpg | 24.31dB
22-11-05 23:54:32.306 : --48-->   0048.jpg | 28.18dB
22-11-05 23:54:35.125 : --49-->   0049.jpg | 28.03dB
22-11-05 23:54:38.906 : --50-->   0050.jpg | 20.90dB
22-11-05 23:54:45.088 : --51-->   0051.jpg | 25.31dB
22-11-05 23:54:52.837 : --52-->   0052.jpg | 23.38dB
22-11-05 23:54:56.863 : --53-->   0053.jpg | 24.42dB
22-11-05 23:55:05.946 : --54-->   0054.jpg | 21.99dB
22-11-05 23:55:15.163 : --55-->   0055.jpg | 35.09dB
22-11-05 23:55:17.361 : --56-->   0056.jpg | 24.62dB
22-11-05 23:55:22.488 : --57-->   0057.jpg | 30.45dB
22-11-05 23:55:28.916 : --58-->   0058.jpg | 26.37dB
22-11-05 23:55:34.929 : --59-->   0059.jpg | 27.65dB
22-11-05 23:55:41.398 : --60-->   0060.jpg | 24.77dB
22-11-05 23:55:44.367 : --61-->   0061.jpg | 26.15dB
22-11-05 23:55:50.285 : --62-->   0062.jpg | 24.29dB
22-11-05 23:55:55.350 : --63-->   0063.jpg | 23.10dB
22-11-05 23:56:01.176 : --64-->   0064.jpg | 28.89dB
22-11-05 23:56:10.414 : --65-->   0065.jpg | 34.87dB
22-11-05 23:56:13.155 : --66-->   0066.jpg | 30.11dB
22-11-05 23:56:16.935 : --67-->   0067.jpg | 26.29dB
22-11-05 23:56:22.884 : --68-->   0068.jpg | 25.07dB
22-11-05 23:56:29.891 : --69-->   0069.jpg | 19.44dB
22-11-05 23:56:33.101 : --70-->   0070.jpg | 21.01dB
22-11-05 23:56:36.007 : --71-->   0071.jpg | 22.53dB
22-11-05 23:56:41.801 : --72-->   0072.jpg | 27.80dB
22-11-05 23:56:48.532 : --73-->   0073.jpg | 27.27dB
22-11-05 23:56:49.605 : --74-->   0074.jpg | 24.66dB
22-11-05 23:56:54.478 : --75-->   0075.jpg | 24.40dB
22-11-05 23:57:00.954 : --76-->   0076.jpg | 25.25dB
22-11-05 23:57:12.321 : --77-->   0077.jpg | 22.36dB
22-11-05 23:57:14.581 : --78-->   0078.jpg | 27.61dB
22-11-05 23:57:21.690 : --79-->   0079.jpg | 20.43dB
22-11-05 23:57:25.110 : --80-->   0080.jpg | 29.61dB
22-11-05 23:57:31.984 : --81-->   0081.jpg | 28.58dB
22-11-05 23:57:35.854 : --82-->   0082.jpg | 28.24dB
22-11-05 23:57:39.875 : --83-->   0083.jpg | 22.76dB
22-11-05 23:57:45.440 : --84-->   0084.jpg | 28.83dB
22-11-05 23:57:51.027 : --85-->   0085.jpg | 20.85dB
22-11-05 23:57:59.677 : --86-->   0086.jpg | 24.24dB
22-11-05 23:58:02.017 : --87-->   0087.jpg | 20.75dB
22-11-05 23:58:07.618 : --88-->   0088.jpg | 26.80dB
22-11-05 23:58:11.974 : --89-->   0089.jpg | 23.76dB
22-11-05 23:58:18.617 : --90-->   0090.jpg | 21.78dB
22-11-05 23:58:23.654 : --91-->   0091.jpg | 30.47dB
22-11-05 23:58:32.179 : --92-->   0092.jpg | 22.09dB
22-11-05 23:58:37.540 : --93-->   0093.jpg | 27.54dB
22-11-05 23:58:46.052 : --94-->   0094.jpg | 30.31dB
22-11-05 23:58:51.286 : --95-->   0095.jpg | 25.16dB
22-11-05 23:58:58.157 : --96-->   0096.jpg | 20.71dB
22-11-05 23:59:03.476 : --97-->   0097.jpg | 32.54dB
22-11-05 23:59:08.738 : --98-->   0098.jpg | 25.29dB
22-11-05 23:59:10.740 : --99-->   0099.jpg | 24.78dB
22-11-05 23:59:12.222 : -100-->   0100.jpg | 23.36dB
22-11-05 23:59:12.381 : <epoch:168, iter:  50,000, Average PSNR : 26.07dB

22-11-06 00:01:45.352 : <epoch:169, iter:  50,200, lr:2.000e-04> G_loss: 3.093e-02 
22-11-06 00:04:05.553 : <epoch:169, iter:  50,400, lr:2.000e-04> G_loss: 3.261e-02 
22-11-06 00:06:25.517 : <epoch:170, iter:  50,600, lr:2.000e-04> G_loss: 3.473e-02 
22-11-06 00:08:59.718 : <epoch:171, iter:  50,800, lr:2.000e-04> G_loss: 3.787e-02 
22-11-06 00:11:20.913 : <epoch:171, iter:  51,000, lr:2.000e-04> G_loss: 3.133e-02 
22-11-06 00:13:54.124 : <epoch:172, iter:  51,200, lr:2.000e-04> G_loss: 2.580e-02 
22-11-06 00:16:18.101 : <epoch:173, iter:  51,400, lr:2.000e-04> G_loss: 3.622e-02 
22-11-06 00:18:44.666 : <epoch:173, iter:  51,600, lr:2.000e-04> G_loss: 3.858e-02 
22-11-06 00:21:08.011 : <epoch:174, iter:  51,800, lr:2.000e-04> G_loss: 3.654e-02 
22-11-06 00:23:34.614 : <epoch:175, iter:  52,000, lr:2.000e-04> G_loss: 3.111e-02 
22-11-06 00:26:11.468 : <epoch:175, iter:  52,200, lr:2.000e-04> G_loss: 3.285e-02 
22-11-06 00:28:43.843 : <epoch:176, iter:  52,400, lr:2.000e-04> G_loss: 3.359e-02 
22-11-06 00:31:18.131 : <epoch:177, iter:  52,600, lr:2.000e-04> G_loss: 3.360e-02 
22-11-06 00:33:38.040 : <epoch:177, iter:  52,800, lr:2.000e-04> G_loss: 3.252e-02 
22-11-06 00:36:02.046 : <epoch:178, iter:  53,000, lr:2.000e-04> G_loss: 2.662e-02 
22-11-06 00:38:39.901 : <epoch:179, iter:  53,200, lr:2.000e-04> G_loss: 3.321e-02 
22-11-06 00:41:10.107 : <epoch:179, iter:  53,400, lr:2.000e-04> G_loss: 3.405e-02 
22-11-06 00:43:49.912 : <epoch:180, iter:  53,600, lr:2.000e-04> G_loss: 3.214e-02 
22-11-06 00:46:23.566 : <epoch:181, iter:  53,800, lr:2.000e-04> G_loss: 4.034e-02 
22-11-06 00:48:49.460 : <epoch:181, iter:  54,000, lr:2.000e-04> G_loss: 3.308e-02 
22-11-06 00:51:19.008 : <epoch:182, iter:  54,200, lr:2.000e-04> G_loss: 3.273e-02 
22-11-06 00:53:58.535 : <epoch:183, iter:  54,400, lr:2.000e-04> G_loss: 3.011e-02 
22-11-06 00:56:20.651 : <epoch:183, iter:  54,600, lr:2.000e-04> G_loss: 3.195e-02 
22-11-06 00:58:50.930 : <epoch:184, iter:  54,800, lr:2.000e-04> G_loss: 3.525e-02 
22-11-06 01:01:20.636 : <epoch:185, iter:  55,000, lr:2.000e-04> G_loss: 3.156e-02 
22-11-06 01:01:20.639 : Saving the model.
22-11-06 01:01:25.880 : ---1-->   0001.jpg | 24.52dB
22-11-06 01:01:32.986 : ---2-->   0002.jpg | 25.58dB
22-11-06 01:01:44.135 : ---3-->   0003.jpg | 29.85dB
22-11-06 01:01:46.513 : ---4-->   0004.jpg | 28.17dB
22-11-06 01:01:49.004 : ---5-->   0005.jpg | 28.93dB
22-11-06 01:01:54.080 : ---6-->   0006.jpg | 33.19dB
22-11-06 01:01:57.925 : ---7-->   0007.jpg | 34.78dB
22-11-06 01:02:02.872 : ---8-->   0008.jpg | 24.08dB
22-11-06 01:02:08.476 : ---9-->   0009.jpg | 23.45dB
22-11-06 01:02:13.824 : --10-->   0010.jpg | 28.10dB
22-11-06 01:02:24.204 : --11-->   0011.jpg | 25.88dB
22-11-06 01:02:31.149 : --12-->   0012.jpg | 24.31dB
22-11-06 01:02:38.260 : --13-->   0013.jpg | 24.01dB
22-11-06 01:02:41.693 : --14-->   0014.jpg | 23.30dB
22-11-06 01:02:46.441 : --15-->   0015.jpg | 21.50dB
22-11-06 01:02:52.749 : --16-->   0016.jpg | 29.86dB
22-11-06 01:02:57.562 : --17-->   0017.jpg | 28.97dB
22-11-06 01:03:01.532 : --18-->   0018.jpg | 26.99dB
22-11-06 01:03:05.578 : --19-->   0019.jpg | 32.36dB
22-11-06 01:03:09.356 : --20-->   0020.jpg | 23.85dB
22-11-06 01:03:15.721 : --21-->   0021.jpg | 26.49dB
22-11-06 01:03:20.567 : --22-->   0022.jpg | 38.79dB
22-11-06 01:03:28.368 : --23-->   0023.jpg | 25.24dB
22-11-06 01:03:33.252 : --24-->   0024.jpg | 34.06dB
22-11-06 01:03:38.021 : --25-->   0025.jpg | 25.91dB
22-11-06 01:03:42.381 : --26-->   0026.jpg | 26.59dB
22-11-06 01:03:52.609 : --27-->   0027.jpg | 29.03dB
22-11-06 01:03:56.027 : --28-->   0028.jpg | 30.26dB
22-11-06 01:03:59.575 : --29-->   0029.jpg | 24.18dB
22-11-06 01:04:04.530 : --30-->   0030.jpg | 29.09dB
22-11-06 01:04:08.410 : --31-->   0031.jpg | 28.14dB
22-11-06 01:04:17.353 : --32-->   0032.jpg | 25.91dB
22-11-06 01:04:22.294 : --33-->   0033.jpg | 23.39dB
22-11-06 01:04:30.690 : --34-->   0034.jpg | 23.35dB
22-11-06 01:04:36.107 : --35-->   0035.jpg | 26.75dB
22-11-06 01:04:43.003 : --36-->   0036.jpg | 23.25dB
22-11-06 01:04:51.036 : --37-->   0037.jpg | 22.91dB
22-11-06 01:04:54.746 : --38-->   0038.jpg | 26.29dB
22-11-06 01:05:00.167 : --39-->   0039.jpg | 24.06dB
22-11-06 01:05:08.324 : --40-->   0040.jpg | 28.12dB
22-11-06 01:05:13.071 : --41-->   0041.jpg | 21.09dB
22-11-06 01:05:18.450 : --42-->   0042.jpg | 26.67dB
22-11-06 01:05:25.948 : --43-->   0043.jpg | 27.24dB
22-11-06 01:05:33.837 : --44-->   0044.jpg | 24.69dB
22-11-06 01:05:39.328 : --45-->   0045.jpg | 21.51dB
22-11-06 01:05:43.232 : --46-->   0046.jpg | 23.21dB
22-11-06 01:05:48.992 : --47-->   0047.jpg | 24.38dB
22-11-06 01:05:50.455 : --48-->   0048.jpg | 28.17dB
22-11-06 01:05:52.051 : --49-->   0049.jpg | 28.11dB
22-11-06 01:05:56.586 : --50-->   0050.jpg | 20.94dB
22-11-06 01:06:04.287 : --51-->   0051.jpg | 25.39dB
22-11-06 01:06:08.162 : --52-->   0052.jpg | 23.33dB
22-11-06 01:06:10.889 : --53-->   0053.jpg | 24.45dB
22-11-06 01:06:15.018 : --54-->   0054.jpg | 22.04dB
22-11-06 01:06:21.732 : --55-->   0055.jpg | 35.56dB
22-11-06 01:06:30.498 : --56-->   0056.jpg | 24.69dB
22-11-06 01:06:36.918 : --57-->   0057.jpg | 30.32dB
22-11-06 01:06:43.743 : --58-->   0058.jpg | 26.48dB
22-11-06 01:06:46.881 : --59-->   0059.jpg | 27.82dB
22-11-06 01:06:49.521 : --60-->   0060.jpg | 24.75dB
22-11-06 01:06:54.642 : --61-->   0061.jpg | 26.31dB
22-11-06 01:06:56.542 : --62-->   0062.jpg | 24.29dB
22-11-06 01:07:00.248 : --63-->   0063.jpg | 23.15dB
22-11-06 01:07:08.017 : --64-->   0064.jpg | 28.88dB
22-11-06 01:07:13.000 : --65-->   0065.jpg | 34.80dB
22-11-06 01:07:14.671 : --66-->   0066.jpg | 30.17dB
22-11-06 01:07:22.821 : --67-->   0067.jpg | 26.38dB
22-11-06 01:07:28.932 : --68-->   0068.jpg | 25.14dB
22-11-06 01:07:31.824 : --69-->   0069.jpg | 19.45dB
22-11-06 01:07:36.609 : --70-->   0070.jpg | 21.12dB
22-11-06 01:07:40.033 : --71-->   0071.jpg | 22.55dB
22-11-06 01:07:48.821 : --72-->   0072.jpg | 28.03dB
22-11-06 01:07:53.426 : --73-->   0073.jpg | 27.40dB
22-11-06 01:08:00.678 : --74-->   0074.jpg | 24.32dB
22-11-06 01:08:09.846 : --75-->   0075.jpg | 24.42dB
22-11-06 01:08:13.005 : --76-->   0076.jpg | 25.19dB
22-11-06 01:08:17.481 : --77-->   0077.jpg | 22.38dB
22-11-06 01:08:21.387 : --78-->   0078.jpg | 27.63dB
22-11-06 01:08:26.786 : --79-->   0079.jpg | 20.45dB
22-11-06 01:08:34.366 : --80-->   0080.jpg | 29.78dB
22-11-06 01:08:37.644 : --81-->   0081.jpg | 28.64dB
22-11-06 01:08:39.932 : --82-->   0082.jpg | 28.28dB
22-11-06 01:08:48.130 : --83-->   0083.jpg | 22.72dB
22-11-06 01:08:52.700 : --84-->   0084.jpg | 28.82dB
22-11-06 01:08:58.235 : --85-->   0085.jpg | 20.94dB
22-11-06 01:09:05.678 : --86-->   0086.jpg | 24.43dB
22-11-06 01:09:11.457 : --87-->   0087.jpg | 20.92dB
22-11-06 01:09:18.960 : --88-->   0088.jpg | 26.96dB
22-11-06 01:09:21.890 : --89-->   0089.jpg | 23.86dB
22-11-06 01:09:27.106 : --90-->   0090.jpg | 21.78dB
22-11-06 01:09:30.632 : --91-->   0091.jpg | 30.43dB
22-11-06 01:09:33.760 : --92-->   0092.jpg | 22.19dB
22-11-06 01:09:39.495 : --93-->   0093.jpg | 27.54dB
22-11-06 01:09:43.173 : --94-->   0094.jpg | 30.37dB
22-11-06 01:09:52.886 : --95-->   0095.jpg | 25.13dB
22-11-06 01:09:55.499 : --96-->   0096.jpg | 20.75dB
22-11-06 01:09:58.734 : --97-->   0097.jpg | 32.53dB
22-11-06 01:10:00.938 : --98-->   0098.jpg | 25.32dB
22-11-06 01:10:08.737 : --99-->   0099.jpg | 24.86dB
22-11-06 01:10:15.109 : -100-->   0100.jpg | 23.36dB
22-11-06 01:10:15.329 : <epoch:185, iter:  55,000, Average PSNR : 26.16dB

22-11-06 01:12:40.724 : <epoch:185, iter:  55,200, lr:2.000e-04> G_loss: 3.095e-02 
22-11-06 01:15:13.539 : <epoch:186, iter:  55,400, lr:2.000e-04> G_loss: 3.443e-02 
22-11-06 01:17:40.951 : <epoch:187, iter:  55,600, lr:2.000e-04> G_loss: 3.168e-02 
22-11-06 01:20:15.241 : <epoch:187, iter:  55,800, lr:2.000e-04> G_loss: 3.294e-02 
22-11-06 01:22:42.417 : <epoch:188, iter:  56,000, lr:2.000e-04> G_loss: 3.249e-02 
22-11-06 01:25:22.124 : <epoch:189, iter:  56,200, lr:2.000e-04> G_loss: 2.878e-02 
22-11-06 01:27:45.099 : <epoch:189, iter:  56,400, lr:2.000e-04> G_loss: 3.127e-02 
22-11-06 01:30:14.569 : <epoch:190, iter:  56,600, lr:2.000e-04> G_loss: 3.301e-02 
22-11-06 01:32:48.871 : <epoch:191, iter:  56,800, lr:2.000e-04> G_loss: 3.176e-02 
22-11-06 01:35:15.946 : <epoch:191, iter:  57,000, lr:2.000e-04> G_loss: 3.780e-02 
22-11-06 01:37:57.864 : <epoch:192, iter:  57,200, lr:2.000e-04> G_loss: 2.427e-02 
22-11-06 01:40:30.456 : <epoch:193, iter:  57,400, lr:2.000e-04> G_loss: 3.102e-02 
22-11-06 01:42:54.520 : <epoch:193, iter:  57,600, lr:2.000e-04> G_loss: 2.829e-02 
22-11-06 01:45:27.461 : <epoch:194, iter:  57,800, lr:2.000e-04> G_loss: 3.359e-02 
22-11-06 01:47:59.131 : <epoch:195, iter:  58,000, lr:2.000e-04> G_loss: 4.151e-02 
22-11-06 01:50:25.235 : <epoch:195, iter:  58,200, lr:2.000e-04> G_loss: 3.356e-02 
22-11-06 01:53:09.113 : <epoch:196, iter:  58,400, lr:2.000e-04> G_loss: 2.879e-02 
22-11-06 01:55:40.719 : <epoch:197, iter:  58,600, lr:2.000e-04> G_loss: 3.077e-02 
22-11-06 01:58:05.152 : <epoch:197, iter:  58,800, lr:2.000e-04> G_loss: 3.923e-02 
22-11-06 01:00:38.586 : <epoch:198, iter:  59,000, lr:2.000e-04> G_loss: 2.907e-02 
22-11-06 01:03:07.362 : <epoch:199, iter:  59,200, lr:2.000e-04> G_loss: 3.741e-02 
22-11-06 01:05:35.677 : <epoch:199, iter:  59,400, lr:2.000e-04> G_loss: 3.471e-02 
22-11-06 01:08:12.901 : <epoch:200, iter:  59,600, lr:2.000e-04> G_loss: 3.287e-02 
22-11-06 01:10:50.689 : <epoch:201, iter:  59,800, lr:2.000e-04> G_loss: 3.013e-02 
22-11-06 01:13:26.239 : <epoch:202, iter:  60,000, lr:2.000e-04> G_loss: 3.150e-02 
22-11-06 01:13:26.241 : Saving the model.
22-11-06 01:13:35.473 : ---1-->   0001.jpg | 24.54dB
22-11-06 01:13:41.618 : ---2-->   0002.jpg | 25.66dB
22-11-06 01:13:51.667 : ---3-->   0003.jpg | 29.81dB
22-11-06 01:13:57.123 : ---4-->   0004.jpg | 28.35dB
22-11-06 01:14:01.325 : ---5-->   0005.jpg | 29.04dB
22-11-06 01:14:05.235 : ---6-->   0006.jpg | 32.76dB
22-11-06 01:14:10.062 : ---7-->   0007.jpg | 34.67dB
22-11-06 01:14:18.853 : ---8-->   0008.jpg | 24.10dB
22-11-06 01:14:24.721 : ---9-->   0009.jpg | 23.39dB
22-11-06 01:14:30.326 : --10-->   0010.jpg | 28.02dB
22-11-06 01:14:35.155 : --11-->   0011.jpg | 25.89dB
22-11-06 01:14:41.735 : --12-->   0012.jpg | 24.24dB
22-11-06 01:14:47.028 : --13-->   0013.jpg | 24.05dB
22-11-06 01:14:50.173 : --14-->   0014.jpg | 23.35dB
22-11-06 01:14:51.417 : --15-->   0015.jpg | 21.58dB
22-11-06 01:14:54.246 : --16-->   0016.jpg | 29.83dB
22-11-06 01:14:56.065 : --17-->   0017.jpg | 29.07dB
22-11-06 01:14:59.817 : --18-->   0018.jpg | 26.96dB
22-11-06 01:15:06.811 : --19-->   0019.jpg | 32.46dB
22-11-06 01:15:13.182 : --20-->   0020.jpg | 23.91dB
22-11-06 01:15:20.195 : --21-->   0021.jpg | 26.46dB
22-11-06 01:15:26.345 : --22-->   0022.jpg | 38.68dB
22-11-06 01:15:27.815 : --23-->   0023.jpg | 25.31dB
22-11-06 01:15:33.011 : --24-->   0024.jpg | 34.07dB
22-11-06 01:15:38.585 : --25-->   0025.jpg | 25.94dB
22-11-06 01:15:41.103 : --26-->   0026.jpg | 26.68dB
22-11-06 01:15:50.125 : --27-->   0027.jpg | 28.97dB
22-11-06 01:15:57.106 : --28-->   0028.jpg | 30.19dB
22-11-06 01:16:03.038 : --29-->   0029.jpg | 24.18dB
22-11-06 01:16:10.244 : --30-->   0030.jpg | 29.13dB
22-11-06 01:16:14.059 : --31-->   0031.jpg | 28.20dB
22-11-06 01:16:20.654 : --32-->   0032.jpg | 25.97dB
22-11-06 01:16:22.346 : --33-->   0033.jpg | 23.42dB
22-11-06 01:16:25.233 : --34-->   0034.jpg | 23.23dB
22-11-06 01:16:29.824 : --35-->   0035.jpg | 26.69dB
22-11-06 01:16:34.111 : --36-->   0036.jpg | 23.23dB
22-11-06 01:16:40.888 : --37-->   0037.jpg | 22.95dB
22-11-06 01:16:42.179 : --38-->   0038.jpg | 26.35dB
22-11-06 01:16:45.690 : --39-->   0039.jpg | 23.97dB
22-11-06 01:16:48.137 : --40-->   0040.jpg | 28.15dB
22-11-06 01:16:50.812 : --41-->   0041.jpg | 21.16dB
22-11-06 01:16:57.896 : --42-->   0042.jpg | 26.67dB
22-11-06 01:17:03.888 : --43-->   0043.jpg | 27.16dB
22-11-06 01:17:12.152 : --44-->   0044.jpg | 24.65dB
22-11-06 01:17:17.982 : --45-->   0045.jpg | 21.45dB
22-11-06 01:17:26.035 : --46-->   0046.jpg | 23.17dB
22-11-06 01:17:33.433 : --47-->   0047.jpg | 24.31dB
22-11-06 01:17:39.851 : --48-->   0048.jpg | 28.01dB
22-11-06 01:17:44.300 : --49-->   0049.jpg | 28.16dB
22-11-06 01:17:51.165 : --50-->   0050.jpg | 21.02dB
22-11-06 01:17:58.817 : --51-->   0051.jpg | 25.55dB
22-11-06 01:18:05.921 : --52-->   0052.jpg | 23.44dB
22-11-06 01:18:08.894 : --53-->   0053.jpg | 24.49dB
22-11-06 01:18:12.635 : --54-->   0054.jpg | 22.03dB
22-11-06 01:18:15.651 : --55-->   0055.jpg | 36.17dB
22-11-06 01:18:17.738 : --56-->   0056.jpg | 24.50dB
22-11-06 01:18:21.038 : --57-->   0057.jpg | 30.38dB
22-11-06 01:18:27.194 : --58-->   0058.jpg | 26.48dB
22-11-06 01:18:34.185 : --59-->   0059.jpg | 27.88dB
22-11-06 01:18:37.350 : --60-->   0060.jpg | 24.85dB
22-11-06 01:18:42.197 : --61-->   0061.jpg | 26.08dB
22-11-06 01:18:48.704 : --62-->   0062.jpg | 24.29dB
22-11-06 01:18:52.215 : --63-->   0063.jpg | 23.11dB
22-11-06 01:19:03.297 : --64-->   0064.jpg | 28.86dB
22-11-06 01:19:08.152 : --65-->   0065.jpg | 34.92dB
22-11-06 01:19:14.183 : --66-->   0066.jpg | 30.14dB
22-11-06 01:19:22.697 : --67-->   0067.jpg | 26.61dB
22-11-06 01:19:27.631 : --68-->   0068.jpg | 25.15dB
22-11-06 01:19:29.143 : --69-->   0069.jpg | 19.41dB
22-11-06 01:19:30.390 : --70-->   0070.jpg | 21.08dB
22-11-06 01:19:30.920 : --71-->   0071.jpg | 22.54dB
22-11-06 01:19:33.504 : --72-->   0072.jpg | 27.93dB
22-11-06 01:19:40.203 : --73-->   0073.jpg | 27.20dB
22-11-06 01:19:44.785 : --74-->   0074.jpg | 24.39dB
22-11-06 01:19:49.107 : --75-->   0075.jpg | 24.45dB
22-11-06 01:19:52.068 : --76-->   0076.jpg | 25.30dB
22-11-06 01:19:56.919 : --77-->   0077.jpg | 22.36dB
22-11-06 01:20:03.122 : --78-->   0078.jpg | 27.59dB
22-11-06 01:20:12.500 : --79-->   0079.jpg | 20.44dB
22-11-06 01:20:17.375 : --80-->   0080.jpg | 29.74dB
22-11-06 01:20:23.883 : --81-->   0081.jpg | 28.64dB
22-11-06 01:20:25.190 : --82-->   0082.jpg | 28.45dB
22-11-06 01:20:27.294 : --83-->   0083.jpg | 22.90dB
22-11-06 01:20:33.968 : --84-->   0084.jpg | 28.96dB
22-11-06 01:20:37.378 : --85-->   0085.jpg | 21.02dB
22-11-06 01:20:38.590 : --86-->   0086.jpg | 24.32dB
22-11-06 01:20:40.522 : --87-->   0087.jpg | 20.76dB
22-11-06 01:20:47.382 : --88-->   0088.jpg | 26.97dB
22-11-06 01:20:50.331 : --89-->   0089.jpg | 23.89dB
22-11-06 01:20:53.699 : --90-->   0090.jpg | 21.77dB
22-11-06 01:21:00.459 : --91-->   0091.jpg | 30.44dB
22-11-06 01:21:03.462 : --92-->   0092.jpg | 22.37dB
22-11-06 01:21:09.050 : --93-->   0093.jpg | 27.60dB
22-11-06 01:21:16.444 : --94-->   0094.jpg | 30.40dB
22-11-06 01:21:22.238 : --95-->   0095.jpg | 25.20dB
22-11-06 01:21:30.822 : --96-->   0096.jpg | 20.78dB
22-11-06 01:21:40.990 : --97-->   0097.jpg | 32.57dB
22-11-06 01:21:47.184 : --98-->   0098.jpg | 25.32dB
22-11-06 01:21:52.461 : --99-->   0099.jpg | 24.83dB
22-11-06 01:21:58.819 : -100-->   0100.jpg | 23.46dB
22-11-06 01:21:59.189 : <epoch:202, iter:  60,000, Average PSNR : 26.17dB

22-11-06 01:24:32.484 : <epoch:202, iter:  60,200, lr:2.000e-04> G_loss: 4.062e-02 
22-11-06 01:27:13.635 : <epoch:203, iter:  60,400, lr:2.000e-04> G_loss: 3.089e-02 
22-11-06 01:29:42.835 : <epoch:204, iter:  60,600, lr:2.000e-04> G_loss: 3.602e-02 
22-11-06 01:32:20.500 : <epoch:204, iter:  60,800, lr:2.000e-04> G_loss: 3.002e-02 
22-11-06 01:34:47.866 : <epoch:205, iter:  61,000, lr:2.000e-04> G_loss: 3.831e-02 
22-11-06 01:37:13.719 : <epoch:206, iter:  61,200, lr:2.000e-04> G_loss: 2.992e-02 
22-11-06 01:39:41.805 : <epoch:206, iter:  61,400, lr:2.000e-04> G_loss: 3.468e-02 
22-11-06 01:42:17.109 : <epoch:207, iter:  61,600, lr:2.000e-04> G_loss: 3.113e-02 
22-11-06 01:44:37.222 : <epoch:208, iter:  61,800, lr:2.000e-04> G_loss: 3.801e-02 
22-11-06 01:47:06.718 : <epoch:208, iter:  62,000, lr:2.000e-04> G_loss: 4.136e-02 
22-11-06 01:49:31.831 : <epoch:209, iter:  62,200, lr:2.000e-04> G_loss: 2.966e-02 
22-11-06 01:52:08.747 : <epoch:210, iter:  62,400, lr:2.000e-04> G_loss: 3.055e-02 
22-11-06 01:54:38.505 : <epoch:210, iter:  62,600, lr:2.000e-04> G_loss: 3.219e-02 
22-11-06 01:57:12.769 : <epoch:211, iter:  62,800, lr:2.000e-04> G_loss: 3.516e-02 
22-11-06 01:59:49.923 : <epoch:212, iter:  63,000, lr:2.000e-04> G_loss: 3.127e-02 
22-11-06 02:02:17.526 : <epoch:212, iter:  63,200, lr:2.000e-04> G_loss: 3.234e-02 
22-11-06 02:04:48.521 : <epoch:213, iter:  63,400, lr:2.000e-04> G_loss: 3.154e-02 
22-11-06 02:07:15.841 : <epoch:214, iter:  63,600, lr:2.000e-04> G_loss: 3.137e-02 
22-11-06 02:09:55.167 : <epoch:214, iter:  63,800, lr:2.000e-04> G_loss: 3.037e-02 
22-11-06 02:12:22.231 : <epoch:215, iter:  64,000, lr:2.000e-04> G_loss: 3.824e-02 
22-11-06 02:14:47.230 : <epoch:216, iter:  64,200, lr:2.000e-04> G_loss: 2.494e-02 
22-11-06 02:17:18.118 : <epoch:216, iter:  64,400, lr:2.000e-04> G_loss: 3.912e-02 
22-11-06 02:19:52.112 : <epoch:217, iter:  64,600, lr:2.000e-04> G_loss: 3.006e-02 
22-11-06 02:22:27.981 : <epoch:218, iter:  64,800, lr:2.000e-04> G_loss: 3.521e-02 
22-11-06 02:24:58.037 : <epoch:218, iter:  65,000, lr:2.000e-04> G_loss: 3.115e-02 
22-11-06 02:24:58.038 : Saving the model.
22-11-06 02:25:07.637 : ---1-->   0001.jpg | 24.39dB
22-11-06 02:25:13.543 : ---2-->   0002.jpg | 25.59dB
22-11-06 02:25:19.810 : ---3-->   0003.jpg | 29.83dB
22-11-06 02:25:24.137 : ---4-->   0004.jpg | 28.15dB
22-11-06 02:25:28.717 : ---5-->   0005.jpg | 28.95dB
22-11-06 02:25:36.570 : ---6-->   0006.jpg | 32.08dB
22-11-06 02:25:43.671 : ---7-->   0007.jpg | 34.89dB
22-11-06 02:25:51.527 : ---8-->   0008.jpg | 24.13dB
22-11-06 02:26:03.320 : ---9-->   0009.jpg | 23.40dB
22-11-06 02:26:13.540 : --10-->   0010.jpg | 27.94dB
22-11-06 02:26:21.514 : --11-->   0011.jpg | 25.57dB
22-11-06 02:26:28.358 : --12-->   0012.jpg | 24.21dB
22-11-06 02:26:34.072 : --13-->   0013.jpg | 24.02dB
22-11-06 02:26:37.599 : --14-->   0014.jpg | 23.31dB
22-11-06 02:26:47.661 : --15-->   0015.jpg | 21.61dB
22-11-06 02:26:56.424 : --16-->   0016.jpg | 29.92dB
22-11-06 02:26:57.626 : --17-->   0017.jpg | 28.87dB
22-11-06 02:27:00.596 : --18-->   0018.jpg | 26.82dB
22-11-06 02:27:10.764 : --19-->   0019.jpg | 32.11dB
22-11-06 02:27:17.443 : --20-->   0020.jpg | 23.85dB
22-11-06 02:27:20.203 : --21-->   0021.jpg | 26.36dB
22-11-06 02:27:28.430 : --22-->   0022.jpg | 38.68dB
22-11-06 02:27:34.922 : --23-->   0023.jpg | 25.30dB
22-11-06 02:27:42.103 : --24-->   0024.jpg | 34.04dB
22-11-06 02:27:46.201 : --25-->   0025.jpg | 25.87dB
22-11-06 02:27:51.706 : --26-->   0026.jpg | 26.56dB
22-11-06 02:27:58.159 : --27-->   0027.jpg | 28.95dB
22-11-06 02:28:07.187 : --28-->   0028.jpg | 30.23dB
22-11-06 02:28:13.570 : --29-->   0029.jpg | 24.16dB
22-11-06 02:28:20.216 : --30-->   0030.jpg | 28.91dB
22-11-06 02:28:29.184 : --31-->   0031.jpg | 28.13dB
22-11-06 02:28:34.772 : --32-->   0032.jpg | 25.94dB
22-11-06 02:28:40.945 : --33-->   0033.jpg | 23.43dB
22-11-06 02:28:45.290 : --34-->   0034.jpg | 23.31dB
22-11-06 02:28:49.332 : --35-->   0035.jpg | 26.69dB
22-11-06 02:28:50.867 : --36-->   0036.jpg | 23.20dB
22-11-06 02:28:56.428 : --37-->   0037.jpg | 22.82dB
22-11-06 02:28:59.863 : --38-->   0038.jpg | 26.40dB
22-11-06 02:29:09.278 : --39-->   0039.jpg | 24.00dB
22-11-06 02:29:12.456 : --40-->   0040.jpg | 28.05dB
22-11-06 02:29:17.720 : --41-->   0041.jpg | 20.82dB
22-11-06 02:29:22.438 : --42-->   0042.jpg | 26.56dB
22-11-06 02:29:28.399 : --43-->   0043.jpg | 27.06dB
22-11-06 02:29:30.735 : --44-->   0044.jpg | 24.69dB
22-11-06 02:29:36.451 : --45-->   0045.jpg | 21.37dB
22-11-06 02:29:40.809 : --46-->   0046.jpg | 23.28dB
22-11-06 02:29:46.343 : --47-->   0047.jpg | 24.23dB
22-11-06 02:29:50.238 : --48-->   0048.jpg | 28.01dB
22-11-06 02:29:56.812 : --49-->   0049.jpg | 28.08dB
22-11-06 02:30:03.494 : --50-->   0050.jpg | 20.95dB
22-11-06 02:30:09.952 : --51-->   0051.jpg | 25.31dB
22-11-06 02:30:12.824 : --52-->   0052.jpg | 23.38dB
22-11-06 02:30:16.081 : --53-->   0053.jpg | 24.36dB
22-11-06 02:30:19.776 : --54-->   0054.jpg | 22.04dB
22-11-06 02:30:27.939 : --55-->   0055.jpg | 34.75dB
22-11-06 02:30:35.025 : --56-->   0056.jpg | 24.57dB
22-11-06 02:30:39.097 : --57-->   0057.jpg | 30.47dB
22-11-06 02:30:43.364 : --58-->   0058.jpg | 26.39dB
22-11-06 02:30:52.086 : --59-->   0059.jpg | 27.75dB
22-11-06 02:30:55.279 : --60-->   0060.jpg | 24.80dB
22-11-06 02:30:59.555 : --61-->   0061.jpg | 26.22dB
22-11-06 02:31:06.511 : --62-->   0062.jpg | 24.27dB
22-11-06 02:31:15.292 : --63-->   0063.jpg | 23.14dB
22-11-06 02:31:24.272 : --64-->   0064.jpg | 28.86dB
22-11-06 02:31:28.049 : --65-->   0065.jpg | 34.87dB
22-11-06 02:31:34.410 : --66-->   0066.jpg | 29.84dB
22-11-06 02:31:41.676 : --67-->   0067.jpg | 26.57dB
22-11-06 02:31:48.626 : --68-->   0068.jpg | 25.15dB
22-11-06 02:31:55.350 : --69-->   0069.jpg | 19.44dB
22-11-06 02:31:58.806 : --70-->   0070.jpg | 20.98dB
22-11-06 02:32:06.507 : --71-->   0071.jpg | 22.54dB
22-11-06 02:32:11.039 : --72-->   0072.jpg | 28.05dB
22-11-06 02:32:19.054 : --73-->   0073.jpg | 27.38dB
22-11-06 02:32:22.871 : --74-->   0074.jpg | 24.43dB
22-11-06 02:32:26.862 : --75-->   0075.jpg | 24.30dB
22-11-06 02:32:28.358 : --76-->   0076.jpg | 25.20dB
22-11-06 02:32:34.415 : --77-->   0077.jpg | 22.33dB
22-11-06 02:32:38.838 : --78-->   0078.jpg | 27.57dB
22-11-06 02:32:42.471 : --79-->   0079.jpg | 20.45dB
22-11-06 02:32:48.892 : --80-->   0080.jpg | 29.61dB
22-11-06 02:32:51.169 : --81-->   0081.jpg | 28.61dB
22-11-06 02:32:55.014 : --82-->   0082.jpg | 28.28dB
22-11-06 02:33:00.957 : --83-->   0083.jpg | 22.83dB
22-11-06 02:33:11.359 : --84-->   0084.jpg | 28.80dB
22-11-06 02:33:16.632 : --85-->   0085.jpg | 20.93dB
22-11-06 02:33:19.956 : --86-->   0086.jpg | 24.40dB
22-11-06 02:33:23.753 : --87-->   0087.jpg | 20.74dB
22-11-06 02:33:29.256 : --88-->   0088.jpg | 27.04dB
22-11-06 02:33:35.765 : --89-->   0089.jpg | 23.75dB
22-11-06 02:33:42.430 : --90-->   0090.jpg | 21.78dB
22-11-06 02:33:45.550 : --91-->   0091.jpg | 30.46dB
22-11-06 02:33:53.595 : --92-->   0092.jpg | 22.22dB
22-11-06 02:33:55.626 : --93-->   0093.jpg | 27.57dB
22-11-06 02:33:58.630 : --94-->   0094.jpg | 30.48dB
22-11-06 02:34:03.807 : --95-->   0095.jpg | 25.10dB
22-11-06 02:34:10.714 : --96-->   0096.jpg | 20.77dB
22-11-06 02:34:16.329 : --97-->   0097.jpg | 32.53dB
22-11-06 02:34:19.890 : --98-->   0098.jpg | 25.33dB
22-11-06 02:34:26.769 : --99-->   0099.jpg | 24.83dB
22-11-06 02:34:31.572 : -100-->   0100.jpg | 23.43dB
22-11-06 02:34:31.847 : <epoch:218, iter:  65,000, Average PSNR : 26.11dB

22-11-06 02:37:08.907 : <epoch:219, iter:  65,200, lr:2.000e-04> G_loss: 3.647e-02 
22-11-06 02:39:39.725 : <epoch:220, iter:  65,400, lr:2.000e-04> G_loss: 3.476e-02 
22-11-06 02:42:05.590 : <epoch:220, iter:  65,600, lr:2.000e-04> G_loss: 3.916e-02 
22-11-06 02:44:35.343 : <epoch:221, iter:  65,800, lr:2.000e-04> G_loss: 3.328e-02 
22-11-06 02:47:16.300 : <epoch:222, iter:  66,000, lr:2.000e-04> G_loss: 2.505e-02 
22-11-06 02:49:45.224 : <epoch:222, iter:  66,200, lr:2.000e-04> G_loss: 2.949e-02 
22-11-06 02:52:24.476 : <epoch:223, iter:  66,400, lr:2.000e-04> G_loss: 4.380e-02 
22-11-06 02:54:57.725 : <epoch:224, iter:  66,600, lr:2.000e-04> G_loss: 4.421e-02 
22-11-06 02:57:22.092 : <epoch:224, iter:  66,800, lr:2.000e-04> G_loss: 3.140e-02 
22-11-06 03:00:02.727 : <epoch:225, iter:  67,000, lr:2.000e-04> G_loss: 4.109e-02 
22-11-06 03:02:30.721 : <epoch:226, iter:  67,200, lr:2.000e-04> G_loss: 3.650e-02 
22-11-06 03:05:03.742 : <epoch:226, iter:  67,400, lr:2.000e-04> G_loss: 3.291e-02 
22-11-06 03:07:31.249 : <epoch:227, iter:  67,600, lr:2.000e-04> G_loss: 2.763e-02 
22-11-06 03:10:00.258 : <epoch:228, iter:  67,800, lr:2.000e-04> G_loss: 3.421e-02 
22-11-06 03:12:32.920 : <epoch:228, iter:  68,000, lr:2.000e-04> G_loss: 3.232e-02 
22-11-06 03:15:05.451 : <epoch:229, iter:  68,200, lr:2.000e-04> G_loss: 2.623e-02 
22-11-06 03:17:30.751 : <epoch:230, iter:  68,400, lr:2.000e-04> G_loss: 3.652e-02 
22-11-06 03:19:43.589 : <epoch:230, iter:  68,600, lr:2.000e-04> G_loss: 3.498e-02 
22-11-06 03:22:12.171 : <epoch:231, iter:  68,800, lr:2.000e-04> G_loss: 3.816e-02 
22-11-06 03:24:38.361 : <epoch:232, iter:  69,000, lr:2.000e-04> G_loss: 2.626e-02 
22-11-06 03:27:00.677 : <epoch:232, iter:  69,200, lr:2.000e-04> G_loss: 3.463e-02 
22-11-06 03:29:40.019 : <epoch:233, iter:  69,400, lr:2.000e-04> G_loss: 3.121e-02 
22-11-06 03:32:18.167 : <epoch:234, iter:  69,600, lr:2.000e-04> G_loss: 3.438e-02 
22-11-06 03:34:50.545 : <epoch:235, iter:  69,800, lr:2.000e-04> G_loss: 3.417e-02 
22-11-06 03:37:16.360 : <epoch:235, iter:  70,000, lr:2.000e-04> G_loss: 2.770e-02 
22-11-06 03:37:16.388 : Saving the model.
22-11-06 03:37:24.377 : ---1-->   0001.jpg | 24.62dB
22-11-06 03:37:32.172 : ---2-->   0002.jpg | 25.72dB
22-11-06 03:37:35.140 : ---3-->   0003.jpg | 29.76dB
22-11-06 03:37:40.658 : ---4-->   0004.jpg | 28.22dB
22-11-06 03:37:46.166 : ---5-->   0005.jpg | 28.72dB
22-11-06 03:37:53.545 : ---6-->   0006.jpg | 33.13dB
22-11-06 03:37:56.549 : ---7-->   0007.jpg | 34.09dB
22-11-06 03:37:58.848 : ---8-->   0008.jpg | 23.90dB
22-11-06 03:38:02.907 : ---9-->   0009.jpg | 23.36dB
22-11-06 03:38:11.319 : --10-->   0010.jpg | 27.97dB
22-11-06 03:38:17.923 : --11-->   0011.jpg | 26.10dB
22-11-06 03:38:24.248 : --12-->   0012.jpg | 24.02dB
22-11-06 03:38:27.103 : --13-->   0013.jpg | 24.06dB
22-11-06 03:38:32.777 : --14-->   0014.jpg | 23.39dB
22-11-06 03:38:36.600 : --15-->   0015.jpg | 21.17dB
22-11-06 03:38:40.540 : --16-->   0016.jpg | 29.79dB
22-11-06 03:38:41.838 : --17-->   0017.jpg | 29.11dB
22-11-06 03:38:52.735 : --18-->   0018.jpg | 26.88dB
22-11-06 03:38:57.871 : --19-->   0019.jpg | 32.40dB
22-11-06 03:39:02.422 : --20-->   0020.jpg | 23.82dB
22-11-06 03:39:06.461 : --21-->   0021.jpg | 26.51dB
22-11-06 03:39:08.402 : --22-->   0022.jpg | 38.53dB
22-11-06 03:39:12.181 : --23-->   0023.jpg | 25.20dB
22-11-06 03:39:17.923 : --24-->   0024.jpg | 34.15dB
22-11-06 03:39:21.857 : --25-->   0025.jpg | 25.86dB
22-11-06 03:39:28.117 : --26-->   0026.jpg | 26.69dB
22-11-06 03:39:37.677 : --27-->   0027.jpg | 28.84dB
22-11-06 03:39:43.129 : --28-->   0028.jpg | 30.02dB
22-11-06 03:39:48.703 : --29-->   0029.jpg | 24.18dB
22-11-06 03:39:55.509 : --30-->   0030.jpg | 29.15dB
22-11-06 03:40:02.458 : --31-->   0031.jpg | 28.02dB
22-11-06 03:40:06.595 : --32-->   0032.jpg | 25.98dB
22-11-06 03:40:13.028 : --33-->   0033.jpg | 23.31dB
22-11-06 03:40:19.854 : --34-->   0034.jpg | 23.36dB
22-11-06 03:40:25.397 : --35-->   0035.jpg | 26.30dB
22-11-06 03:40:31.543 : --36-->   0036.jpg | 23.21dB
22-11-06 03:40:36.005 : --37-->   0037.jpg | 22.70dB
22-11-06 03:40:42.470 : --38-->   0038.jpg | 26.38dB
22-11-06 03:40:46.599 : --39-->   0039.jpg | 23.99dB
22-11-06 03:40:52.678 : --40-->   0040.jpg | 28.07dB
22-11-06 03:40:58.274 : --41-->   0041.jpg | 21.08dB
22-11-06 03:41:00.586 : --42-->   0042.jpg | 26.64dB
22-11-06 03:41:04.063 : --43-->   0043.jpg | 27.26dB
22-11-06 03:41:11.787 : --44-->   0044.jpg | 24.66dB
22-11-06 03:41:14.237 : --45-->   0045.jpg | 21.49dB
22-11-06 03:41:19.199 : --46-->   0046.jpg | 23.19dB
22-11-06 03:41:23.576 : --47-->   0047.jpg | 24.24dB
22-11-06 03:41:27.811 : --48-->   0048.jpg | 28.10dB
22-11-06 03:41:34.268 : --49-->   0049.jpg | 28.05dB
22-11-06 03:41:37.837 : --50-->   0050.jpg | 20.85dB
22-11-06 03:41:42.634 : --51-->   0051.jpg | 25.77dB
22-11-06 03:41:47.233 : --52-->   0052.jpg | 23.44dB
22-11-06 03:41:51.203 : --53-->   0053.jpg | 24.34dB
22-11-06 03:41:57.201 : --54-->   0054.jpg | 22.02dB
22-11-06 03:42:04.266 : --55-->   0055.jpg | 35.94dB
22-11-06 03:42:08.966 : --56-->   0056.jpg | 24.62dB
22-11-06 03:42:14.105 : --57-->   0057.jpg | 30.29dB
22-11-06 03:42:17.175 : --58-->   0058.jpg | 26.40dB
22-11-06 03:42:21.295 : --59-->   0059.jpg | 27.86dB
22-11-06 03:42:28.235 : --60-->   0060.jpg | 24.70dB
22-11-06 03:42:29.614 : --61-->   0061.jpg | 26.33dB
22-11-06 03:42:33.833 : --62-->   0062.jpg | 24.23dB
22-11-06 03:42:38.801 : --63-->   0063.jpg | 23.09dB
22-11-06 03:42:42.729 : --64-->   0064.jpg | 28.79dB
22-11-06 03:42:47.125 : --65-->   0065.jpg | 34.74dB
22-11-06 03:42:50.208 : --66-->   0066.jpg | 30.13dB
22-11-06 03:42:57.692 : --67-->   0067.jpg | 26.41dB
22-11-06 03:43:00.777 : --68-->   0068.jpg | 25.20dB
22-11-06 03:43:02.968 : --69-->   0069.jpg | 19.41dB
22-11-06 03:43:09.222 : --70-->   0070.jpg | 21.10dB
22-11-06 03:43:16.004 : --71-->   0071.jpg | 22.50dB
22-11-06 03:43:20.535 : --72-->   0072.jpg | 27.97dB
22-11-06 03:43:23.660 : --73-->   0073.jpg | 27.47dB
22-11-06 03:43:26.639 : --74-->   0074.jpg | 24.08dB
22-11-06 03:43:32.585 : --75-->   0075.jpg | 24.39dB
22-11-06 03:43:36.778 : --76-->   0076.jpg | 25.02dB
22-11-06 03:43:41.132 : --77-->   0077.jpg | 22.40dB
22-11-06 03:43:41.987 : --78-->   0078.jpg | 27.61dB
22-11-06 03:43:47.890 : --79-->   0079.jpg | 20.47dB
22-11-06 03:43:54.533 : --80-->   0080.jpg | 29.90dB
22-11-06 03:44:02.571 : --81-->   0081.jpg | 28.65dB
22-11-06 03:44:05.997 : --82-->   0082.jpg | 28.14dB
22-11-06 03:44:08.745 : --83-->   0083.jpg | 22.71dB
22-11-06 03:44:14.544 : --84-->   0084.jpg | 28.89dB
22-11-06 03:44:21.118 : --85-->   0085.jpg | 20.82dB
22-11-06 03:44:28.288 : --86-->   0086.jpg | 24.30dB
22-11-06 03:44:30.451 : --87-->   0087.jpg | 20.90dB
22-11-06 03:44:35.343 : --88-->   0088.jpg | 26.87dB
22-11-06 03:44:40.102 : --89-->   0089.jpg | 23.87dB
22-11-06 03:44:41.801 : --90-->   0090.jpg | 21.77dB
22-11-06 03:44:47.561 : --91-->   0091.jpg | 30.64dB
22-11-06 03:44:49.151 : --92-->   0092.jpg | 22.23dB
22-11-06 03:44:53.130 : --93-->   0093.jpg | 27.33dB
22-11-06 03:45:00.907 : --94-->   0094.jpg | 30.57dB
22-11-06 03:45:05.949 : --95-->   0095.jpg | 25.12dB
22-11-06 03:45:15.592 : --96-->   0096.jpg | 20.78dB
22-11-06 03:45:24.241 : --97-->   0097.jpg | 32.59dB
22-11-06 03:45:31.236 : --98-->   0098.jpg | 25.40dB
22-11-06 03:45:35.363 : --99-->   0099.jpg | 24.82dB
22-11-06 03:45:43.077 : -100-->   0100.jpg | 23.35dB
22-11-06 03:45:43.725 : <epoch:235, iter:  70,000, Average PSNR : 26.13dB

22-11-06 03:48:13.106 : <epoch:236, iter:  70,200, lr:2.000e-04> G_loss: 2.529e-02 
22-11-06 03:50:48.529 : <epoch:237, iter:  70,400, lr:2.000e-04> G_loss: 3.148e-02 
22-11-06 03:53:16.050 : <epoch:237, iter:  70,600, lr:2.000e-04> G_loss: 3.484e-02 
22-11-06 03:55:47.387 : <epoch:238, iter:  70,800, lr:2.000e-04> G_loss: 3.284e-02 
22-11-06 03:58:22.080 : <epoch:239, iter:  71,000, lr:2.000e-04> G_loss: 3.184e-02 
22-11-06 04:00:35.687 : <epoch:239, iter:  71,200, lr:2.000e-04> G_loss: 3.355e-02 
22-11-06 04:03:08.197 : <epoch:240, iter:  71,400, lr:2.000e-04> G_loss: 3.016e-02 
22-11-06 04:05:42.466 : <epoch:241, iter:  71,600, lr:2.000e-04> G_loss: 3.872e-02 
22-11-06 04:08:09.481 : <epoch:241, iter:  71,800, lr:2.000e-04> G_loss: 2.741e-02 
22-11-06 04:10:40.742 : <epoch:242, iter:  72,000, lr:2.000e-04> G_loss: 3.480e-02 
22-11-06 04:13:15.409 : <epoch:243, iter:  72,200, lr:2.000e-04> G_loss: 2.954e-02 
22-11-06 04:15:39.018 : <epoch:243, iter:  72,400, lr:2.000e-04> G_loss: 2.688e-02 
22-11-06 04:18:03.745 : <epoch:244, iter:  72,600, lr:2.000e-04> G_loss: 2.785e-02 
22-11-06 04:20:31.120 : <epoch:245, iter:  72,800, lr:2.000e-04> G_loss: 3.449e-02 
22-11-06 04:22:58.986 : <epoch:245, iter:  73,000, lr:2.000e-04> G_loss: 3.136e-02 
22-11-06 04:25:29.684 : <epoch:246, iter:  73,200, lr:2.000e-04> G_loss: 3.426e-02 
22-11-06 04:28:01.339 : <epoch:247, iter:  73,400, lr:2.000e-04> G_loss: 3.903e-02 
22-11-06 04:30:19.404 : <epoch:247, iter:  73,600, lr:2.000e-04> G_loss: 3.120e-02 
22-11-06 04:32:49.161 : <epoch:248, iter:  73,800, lr:2.000e-04> G_loss: 3.272e-02 
22-11-06 04:35:13.359 : <epoch:249, iter:  74,000, lr:2.000e-04> G_loss: 3.020e-02 
22-11-06 04:37:41.851 : <epoch:249, iter:  74,200, lr:2.000e-04> G_loss: 3.509e-02 
22-11-06 04:40:10.604 : <epoch:250, iter:  74,400, lr:2.000e-04> G_loss: 3.041e-02 
22-11-06 04:42:42.208 : <epoch:251, iter:  74,600, lr:2.000e-04> G_loss: 3.218e-02 
22-11-06 04:45:07.351 : <epoch:251, iter:  74,800, lr:2.000e-04> G_loss: 2.864e-02 
22-11-06 04:47:40.620 : <epoch:252, iter:  75,000, lr:2.000e-04> G_loss: 3.502e-02 
22-11-06 04:47:40.628 : Saving the model.
22-11-06 04:47:50.417 : ---1-->   0001.jpg | 24.70dB
22-11-06 04:47:54.672 : ---2-->   0002.jpg | 25.66dB
22-11-06 04:48:00.387 : ---3-->   0003.jpg | 29.89dB
22-11-06 04:48:07.567 : ---4-->   0004.jpg | 28.22dB
22-11-06 04:48:12.565 : ---5-->   0005.jpg | 29.11dB
22-11-06 04:48:20.229 : ---6-->   0006.jpg | 32.86dB
22-11-06 04:48:27.787 : ---7-->   0007.jpg | 34.88dB
22-11-06 04:48:33.364 : ---8-->   0008.jpg | 24.21dB
22-11-06 04:48:39.451 : ---9-->   0009.jpg | 23.39dB
22-11-06 04:48:45.195 : --10-->   0010.jpg | 28.10dB
22-11-06 04:48:47.072 : --11-->   0011.jpg | 26.12dB
22-11-06 04:48:52.723 : --12-->   0012.jpg | 24.17dB
22-11-06 04:48:57.263 : --13-->   0013.jpg | 24.06dB
22-11-06 04:49:02.613 : --14-->   0014.jpg | 23.38dB
22-11-06 04:49:08.337 : --15-->   0015.jpg | 21.70dB
22-11-06 04:49:14.217 : --16-->   0016.jpg | 30.03dB
22-11-06 04:49:18.562 : --17-->   0017.jpg | 29.04dB
22-11-06 04:49:24.574 : --18-->   0018.jpg | 27.22dB
22-11-06 04:49:30.126 : --19-->   0019.jpg | 32.31dB
22-11-06 04:49:34.255 : --20-->   0020.jpg | 23.83dB
22-11-06 04:49:39.147 : --21-->   0021.jpg | 26.52dB
22-11-06 04:49:48.902 : --22-->   0022.jpg | 38.57dB
22-11-06 04:49:56.193 : --23-->   0023.jpg | 25.28dB
22-11-06 04:50:04.080 : --24-->   0024.jpg | 34.25dB
22-11-06 04:50:10.535 : --25-->   0025.jpg | 25.97dB
22-11-06 04:50:13.844 : --26-->   0026.jpg | 26.63dB
22-11-06 04:50:17.675 : --27-->   0027.jpg | 28.85dB
22-11-06 04:50:19.769 : --28-->   0028.jpg | 30.15dB
22-11-06 04:50:21.973 : --29-->   0029.jpg | 24.21dB
22-11-06 04:50:25.295 : --30-->   0030.jpg | 29.17dB
22-11-06 04:50:30.910 : --31-->   0031.jpg | 28.13dB
22-11-06 04:50:33.302 : --32-->   0032.jpg | 26.07dB
22-11-06 04:50:37.600 : --33-->   0033.jpg | 23.62dB
22-11-06 04:50:44.147 : --34-->   0034.jpg | 23.25dB
22-11-06 04:50:53.574 : --35-->   0035.jpg | 26.71dB
22-11-06 04:51:00.261 : --36-->   0036.jpg | 23.20dB
22-11-06 04:51:06.859 : --37-->   0037.jpg | 22.86dB
22-11-06 04:51:09.008 : --38-->   0038.jpg | 26.42dB
22-11-06 04:51:12.457 : --39-->   0039.jpg | 24.04dB
22-11-06 04:51:18.538 : --40-->   0040.jpg | 28.10dB
22-11-06 04:51:23.052 : --41-->   0041.jpg | 21.10dB
22-11-06 04:51:25.781 : --42-->   0042.jpg | 26.62dB
22-11-06 04:51:31.613 : --43-->   0043.jpg | 27.22dB
22-11-06 04:51:33.660 : --44-->   0044.jpg | 24.69dB
22-11-06 04:51:39.813 : --45-->   0045.jpg | 21.52dB
22-11-06 04:51:45.865 : --46-->   0046.jpg | 23.27dB
22-11-06 04:51:48.271 : --47-->   0047.jpg | 24.37dB
22-11-06 04:51:51.500 : --48-->   0048.jpg | 28.23dB
22-11-06 04:51:54.071 : --49-->   0049.jpg | 28.23dB
22-11-06 04:51:59.513 : --50-->   0050.jpg | 21.08dB
22-11-06 04:52:02.896 : --51-->   0051.jpg | 25.70dB
22-11-06 04:52:10.264 : --52-->   0052.jpg | 23.47dB
22-11-06 04:52:16.966 : --53-->   0053.jpg | 24.49dB
22-11-06 04:52:23.025 : --54-->   0054.jpg | 22.04dB
22-11-06 04:52:29.893 : --55-->   0055.jpg | 35.34dB
22-11-06 04:52:35.498 : --56-->   0056.jpg | 24.59dB
22-11-06 04:52:40.012 : --57-->   0057.jpg | 30.53dB
22-11-06 04:52:49.070 : --58-->   0058.jpg | 26.54dB
22-11-06 04:52:50.656 : --59-->   0059.jpg | 27.81dB
22-11-06 04:52:51.456 : --60-->   0060.jpg | 24.84dB
22-11-06 04:52:52.626 : --61-->   0061.jpg | 26.32dB
22-11-06 04:52:55.259 : --62-->   0062.jpg | 24.35dB
22-11-06 04:52:59.506 : --63-->   0063.jpg | 23.15dB
22-11-06 04:53:06.147 : --64-->   0064.jpg | 28.96dB
22-11-06 04:53:14.320 : --65-->   0065.jpg | 35.13dB
22-11-06 04:53:22.690 : --66-->   0066.jpg | 29.98dB
22-11-06 04:53:28.999 : --67-->   0067.jpg | 26.46dB
22-11-06 04:53:33.984 : --68-->   0068.jpg | 25.20dB
22-11-06 04:53:35.935 : --69-->   0069.jpg | 19.44dB
22-11-06 04:53:40.259 : --70-->   0070.jpg | 21.06dB
22-11-06 04:53:46.505 : --71-->   0071.jpg | 22.54dB
22-11-06 04:53:52.622 : --72-->   0072.jpg | 28.12dB
22-11-06 04:53:56.969 : --73-->   0073.jpg | 27.29dB
22-11-06 04:54:01.081 : --74-->   0074.jpg | 24.46dB
22-11-06 04:54:07.413 : --75-->   0075.jpg | 24.47dB
22-11-06 04:54:11.056 : --76-->   0076.jpg | 25.25dB
22-11-06 04:54:16.724 : --77-->   0077.jpg | 22.40dB
22-11-06 04:54:23.649 : --78-->   0078.jpg | 27.69dB
22-11-06 04:54:32.962 : --79-->   0079.jpg | 20.45dB
22-11-06 04:54:36.092 : --80-->   0080.jpg | 29.72dB
22-11-06 04:54:42.933 : --81-->   0081.jpg | 28.63dB
22-11-06 04:54:49.282 : --82-->   0082.jpg | 28.32dB
22-11-06 04:54:50.783 : --83-->   0083.jpg | 22.86dB
22-11-06 04:54:55.257 : --84-->   0084.jpg | 28.94dB
22-11-06 04:55:00.512 : --85-->   0085.jpg | 20.95dB
22-11-06 04:55:05.485 : --86-->   0086.jpg | 24.34dB
22-11-06 04:55:06.842 : --87-->   0087.jpg | 20.73dB
22-11-06 04:55:10.656 : --88-->   0088.jpg | 27.14dB
22-11-06 04:55:12.442 : --89-->   0089.jpg | 23.87dB
22-11-06 04:55:19.369 : --90-->   0090.jpg | 21.81dB
22-11-06 04:55:24.254 : --91-->   0091.jpg | 30.64dB
22-11-06 04:55:29.038 : --92-->   0092.jpg | 22.28dB
22-11-06 04:55:37.888 : --93-->   0093.jpg | 27.58dB
22-11-06 04:55:45.134 : --94-->   0094.jpg | 30.42dB
22-11-06 04:55:53.147 : --95-->   0095.jpg | 25.17dB
22-11-06 04:56:01.451 : --96-->   0096.jpg | 20.81dB
22-11-06 04:56:03.876 : --97-->   0097.jpg | 32.61dB
22-11-06 04:56:09.302 : --98-->   0098.jpg | 25.44dB
22-11-06 04:56:17.886 : --99-->   0099.jpg | 24.79dB
22-11-06 04:56:24.042 : -100-->   0100.jpg | 23.49dB
22-11-06 04:56:24.284 : <epoch:252, iter:  75,000, Average PSNR : 26.20dB

22-11-06 04:58:56.672 : <epoch:253, iter:  75,200, lr:2.000e-04> G_loss: 3.679e-02 
22-11-06 05:01:25.546 : <epoch:253, iter:  75,400, lr:2.000e-04> G_loss: 2.996e-02 
22-11-06 05:04:02.927 : <epoch:254, iter:  75,600, lr:2.000e-04> G_loss: 3.400e-02 
22-11-06 05:06:39.896 : <epoch:255, iter:  75,800, lr:2.000e-04> G_loss: 3.288e-02 
22-11-06 05:09:00.613 : <epoch:255, iter:  76,000, lr:2.000e-04> G_loss: 2.855e-02 
22-11-06 05:11:29.517 : <epoch:256, iter:  76,200, lr:2.000e-04> G_loss: 3.106e-02 
22-11-06 05:14:05.062 : <epoch:257, iter:  76,400, lr:2.000e-04> G_loss: 3.626e-02 
22-11-06 05:16:28.405 : <epoch:257, iter:  76,600, lr:2.000e-04> G_loss: 3.133e-02 
22-11-06 05:18:56.341 : <epoch:258, iter:  76,800, lr:2.000e-04> G_loss: 3.599e-02 
22-11-06 05:21:24.608 : <epoch:259, iter:  77,000, lr:2.000e-04> G_loss: 3.318e-02 
22-11-06 05:23:49.188 : <epoch:259, iter:  77,200, lr:2.000e-04> G_loss: 2.960e-02 
22-11-06 05:26:31.152 : <epoch:260, iter:  77,400, lr:2.000e-04> G_loss: 3.960e-02 
22-11-06 05:29:05.090 : <epoch:261, iter:  77,600, lr:2.000e-04> G_loss: 2.269e-02 
22-11-06 05:31:25.113 : <epoch:261, iter:  77,800, lr:2.000e-04> G_loss: 4.027e-02 
22-11-06 05:34:02.965 : <epoch:262, iter:  78,000, lr:2.000e-04> G_loss: 3.300e-02 
22-11-06 05:36:38.166 : <epoch:263, iter:  78,200, lr:2.000e-04> G_loss: 3.727e-02 
22-11-06 05:39:06.708 : <epoch:263, iter:  78,400, lr:2.000e-04> G_loss: 3.037e-02 
22-11-06 05:41:43.644 : <epoch:264, iter:  78,600, lr:2.000e-04> G_loss: 4.037e-02 
22-11-06 05:44:20.908 : <epoch:265, iter:  78,800, lr:2.000e-04> G_loss: 3.112e-02 
22-11-06 05:46:42.093 : <epoch:265, iter:  79,000, lr:2.000e-04> G_loss: 3.024e-02 
22-11-06 05:49:20.511 : <epoch:266, iter:  79,200, lr:2.000e-04> G_loss: 2.736e-02 
22-11-06 05:51:47.313 : <epoch:267, iter:  79,400, lr:2.000e-04> G_loss: 2.963e-02 
22-11-06 05:54:11.045 : <epoch:268, iter:  79,600, lr:2.000e-04> G_loss: 3.539e-02 
22-11-06 05:56:48.573 : <epoch:268, iter:  79,800, lr:2.000e-04> G_loss: 3.794e-02 
22-11-06 05:59:20.071 : <epoch:269, iter:  80,000, lr:2.000e-04> G_loss: 3.114e-02 
22-11-06 05:59:20.074 : Saving the model.
22-11-06 05:59:26.608 : ---1-->   0001.jpg | 24.61dB
22-11-06 05:59:32.801 : ---2-->   0002.jpg | 25.77dB
22-11-06 05:59:40.657 : ---3-->   0003.jpg | 29.97dB
22-11-06 05:59:45.153 : ---4-->   0004.jpg | 28.27dB
22-11-06 05:59:48.732 : ---5-->   0005.jpg | 28.98dB
22-11-06 05:59:55.296 : ---6-->   0006.jpg | 32.84dB
22-11-06 06:00:02.585 : ---7-->   0007.jpg | 34.89dB
22-11-06 06:00:05.067 : ---8-->   0008.jpg | 24.12dB
22-11-06 06:00:13.101 : ---9-->   0009.jpg | 23.40dB
22-11-06 06:00:22.813 : --10-->   0010.jpg | 28.12dB
22-11-06 06:00:28.853 : --11-->   0011.jpg | 26.15dB
22-11-06 06:00:37.490 : --12-->   0012.jpg | 24.11dB
22-11-06 06:00:40.068 : --13-->   0013.jpg | 24.04dB
22-11-06 06:00:46.684 : --14-->   0014.jpg | 23.25dB
22-11-06 06:00:54.120 : --15-->   0015.jpg | 21.67dB
22-11-06 06:00:58.863 : --16-->   0016.jpg | 29.97dB
22-11-06 06:01:05.842 : --17-->   0017.jpg | 29.18dB
22-11-06 06:01:11.338 : --18-->   0018.jpg | 26.95dB
22-11-06 06:01:15.086 : --19-->   0019.jpg | 32.37dB
22-11-06 06:01:20.501 : --20-->   0020.jpg | 23.68dB
22-11-06 06:01:25.255 : --21-->   0021.jpg | 26.42dB
22-11-06 06:01:29.217 : --22-->   0022.jpg | 38.32dB
22-11-06 06:01:35.258 : --23-->   0023.jpg | 25.35dB
22-11-06 06:01:39.457 : --24-->   0024.jpg | 33.92dB
22-11-06 06:01:44.637 : --25-->   0025.jpg | 25.85dB
22-11-06 06:01:52.229 : --26-->   0026.jpg | 26.89dB
22-11-06 06:01:57.013 : --27-->   0027.jpg | 28.92dB
22-11-06 06:02:01.126 : --28-->   0028.jpg | 29.69dB
22-11-06 06:02:06.750 : --29-->   0029.jpg | 24.21dB
22-11-06 06:02:14.894 : --30-->   0030.jpg | 29.18dB
22-11-06 06:02:20.460 : --31-->   0031.jpg | 28.18dB
22-11-06 06:02:25.288 : --32-->   0032.jpg | 25.98dB
22-11-06 06:02:31.484 : --33-->   0033.jpg | 23.62dB
22-11-06 06:02:38.841 : --34-->   0034.jpg | 23.29dB
22-11-06 06:02:47.412 : --35-->   0035.jpg | 26.74dB
22-11-06 06:02:55.537 : --36-->   0036.jpg | 23.27dB
22-11-06 06:03:00.970 : --37-->   0037.jpg | 22.93dB
22-11-06 06:03:08.777 : --38-->   0038.jpg | 26.48dB
22-11-06 06:03:17.243 : --39-->   0039.jpg | 24.19dB
22-11-06 06:03:26.171 : --40-->   0040.jpg | 28.09dB
22-11-06 06:03:33.296 : --41-->   0041.jpg | 21.14dB
22-11-06 06:03:41.477 : --42-->   0042.jpg | 26.73dB
22-11-06 06:03:48.205 : --43-->   0043.jpg | 27.19dB
22-11-06 06:03:52.128 : --44-->   0044.jpg | 24.66dB
22-11-06 06:03:58.566 : --45-->   0045.jpg | 21.47dB
22-11-06 06:04:04.467 : --46-->   0046.jpg | 23.33dB
22-11-06 06:04:08.577 : --47-->   0047.jpg | 24.35dB
22-11-06 06:04:13.597 : --48-->   0048.jpg | 28.12dB
22-11-06 06:04:22.424 : --49-->   0049.jpg | 28.24dB
22-11-06 06:04:27.338 : --50-->   0050.jpg | 20.99dB
22-11-06 06:04:32.384 : --51-->   0051.jpg | 25.59dB
22-11-06 06:04:38.930 : --52-->   0052.jpg | 23.44dB
22-11-06 06:04:40.668 : --53-->   0053.jpg | 24.49dB
22-11-06 06:04:44.581 : --54-->   0054.jpg | 21.99dB
22-11-06 06:04:47.219 : --55-->   0055.jpg | 35.71dB
22-11-06 06:04:53.107 : --56-->   0056.jpg | 24.78dB
22-11-06 06:05:01.717 : --57-->   0057.jpg | 30.32dB
22-11-06 06:05:09.174 : --58-->   0058.jpg | 26.59dB
22-11-06 06:05:10.612 : --59-->   0059.jpg | 27.87dB
22-11-06 06:05:13.337 : --60-->   0060.jpg | 24.87dB
22-11-06 06:05:20.909 : --61-->   0061.jpg | 26.37dB
22-11-06 06:05:28.045 : --62-->   0062.jpg | 24.34dB
22-11-06 06:05:34.480 : --63-->   0063.jpg | 23.10dB
22-11-06 06:05:40.182 : --64-->   0064.jpg | 28.93dB
22-11-06 06:05:42.121 : --65-->   0065.jpg | 34.92dB
22-11-06 06:05:45.057 : --66-->   0066.jpg | 30.03dB
22-11-06 06:05:52.121 : --67-->   0067.jpg | 26.72dB
22-11-06 06:05:53.725 : --68-->   0068.jpg | 25.21dB
22-11-06 06:05:55.291 : --69-->   0069.jpg | 19.44dB
22-11-06 06:05:57.756 : --70-->   0070.jpg | 21.07dB
22-11-06 06:06:02.938 : --71-->   0071.jpg | 22.54dB
22-11-06 06:06:08.370 : --72-->   0072.jpg | 28.00dB
22-11-06 06:06:16.417 : --73-->   0073.jpg | 27.42dB
22-11-06 06:06:19.895 : --74-->   0074.jpg | 24.29dB
22-11-06 06:06:22.524 : --75-->   0075.jpg | 24.40dB
22-11-06 06:06:26.154 : --76-->   0076.jpg | 25.34dB
22-11-06 06:06:34.375 : --77-->   0077.jpg | 22.48dB
22-11-06 06:06:40.980 : --78-->   0078.jpg | 27.65dB
22-11-06 06:06:44.359 : --79-->   0079.jpg | 20.45dB
22-11-06 06:06:47.843 : --80-->   0080.jpg | 29.76dB
22-11-06 06:06:49.618 : --81-->   0081.jpg | 28.64dB
22-11-06 06:06:55.647 : --82-->   0082.jpg | 28.28dB
22-11-06 06:07:02.347 : --83-->   0083.jpg | 22.97dB
22-11-06 06:07:11.567 : --84-->   0084.jpg | 28.90dB
22-11-06 06:07:12.780 : --85-->   0085.jpg | 20.94dB
22-11-06 06:07:13.960 : --86-->   0086.jpg | 24.31dB
22-11-06 06:07:20.114 : --87-->   0087.jpg | 20.72dB
22-11-06 06:07:28.582 : --88-->   0088.jpg | 27.01dB
22-11-06 06:07:32.482 : --89-->   0089.jpg | 23.80dB
22-11-06 06:07:38.621 : --90-->   0090.jpg | 21.76dB
22-11-06 06:07:43.217 : --91-->   0091.jpg | 30.65dB
22-11-06 06:07:50.686 : --92-->   0092.jpg | 21.99dB
22-11-06 06:07:52.425 : --93-->   0093.jpg | 27.65dB
22-11-06 06:07:57.914 : --94-->   0094.jpg | 30.46dB
22-11-06 06:08:00.852 : --95-->   0095.jpg | 25.16dB
22-11-06 06:08:05.777 : --96-->   0096.jpg | 20.82dB
22-11-06 06:08:10.432 : --97-->   0097.jpg | 32.57dB
22-11-06 06:08:13.070 : --98-->   0098.jpg | 25.52dB
22-11-06 06:08:17.066 : --99-->   0099.jpg | 24.85dB
22-11-06 06:08:22.770 : -100-->   0100.jpg | 23.40dB
22-11-06 06:08:22.866 : <epoch:269, iter:  80,000, Average PSNR : 26.19dB

22-11-06 06:10:51.234 : <epoch:270, iter:  80,200, lr:2.000e-04> G_loss: 3.077e-02 
22-11-06 06:13:24.352 : <epoch:270, iter:  80,400, lr:2.000e-04> G_loss: 2.882e-02 
22-11-06 06:16:00.121 : <epoch:271, iter:  80,600, lr:2.000e-04> G_loss: 2.524e-02 
22-11-06 06:18:30.251 : <epoch:272, iter:  80,800, lr:2.000e-04> G_loss: 3.351e-02 
22-11-06 06:20:56.548 : <epoch:272, iter:  81,000, lr:2.000e-04> G_loss: 4.554e-02 
22-11-06 06:23:29.531 : <epoch:273, iter:  81,200, lr:2.000e-04> G_loss: 4.010e-02 
22-11-06 06:25:58.701 : <epoch:274, iter:  81,400, lr:2.000e-04> G_loss: 3.419e-02 
22-11-06 06:28:22.778 : <epoch:274, iter:  81,600, lr:2.000e-04> G_loss: 3.166e-02 
22-11-06 06:30:56.970 : <epoch:275, iter:  81,800, lr:2.000e-04> G_loss: 2.599e-02 
22-11-06 06:33:31.473 : <epoch:276, iter:  82,000, lr:2.000e-04> G_loss: 3.321e-02 
22-11-06 06:35:52.824 : <epoch:276, iter:  82,200, lr:2.000e-04> G_loss: 3.446e-02 
22-11-06 06:38:26.580 : <epoch:277, iter:  82,400, lr:2.000e-04> G_loss: 3.325e-02 
22-11-06 06:40:58.360 : <epoch:278, iter:  82,600, lr:2.000e-04> G_loss: 3.119e-02 
22-11-06 06:43:26.439 : <epoch:278, iter:  82,800, lr:2.000e-04> G_loss: 2.656e-02 
22-11-06 06:45:45.256 : <epoch:279, iter:  83,000, lr:2.000e-04> G_loss: 2.959e-02 
22-11-06 06:48:21.455 : <epoch:280, iter:  83,200, lr:2.000e-04> G_loss: 4.040e-02 
22-11-06 06:50:49.045 : <epoch:280, iter:  83,400, lr:2.000e-04> G_loss: 2.737e-02 
22-11-06 06:53:21.314 : <epoch:281, iter:  83,600, lr:2.000e-04> G_loss: 3.607e-02 
22-11-06 06:55:56.891 : <epoch:282, iter:  83,800, lr:2.000e-04> G_loss: 3.225e-02 
22-11-06 06:58:21.491 : <epoch:282, iter:  84,000, lr:2.000e-04> G_loss: 3.339e-02 
22-11-06 07:00:56.576 : <epoch:283, iter:  84,200, lr:2.000e-04> G_loss: 2.951e-02 
22-11-06 07:03:27.130 : <epoch:284, iter:  84,400, lr:2.000e-04> G_loss: 3.224e-02 
22-11-06 07:05:54.926 : <epoch:284, iter:  84,600, lr:2.000e-04> G_loss: 3.368e-02 
22-11-06 07:08:27.628 : <epoch:285, iter:  84,800, lr:2.000e-04> G_loss: 3.320e-02 
22-11-06 07:11:01.565 : <epoch:286, iter:  85,000, lr:2.000e-04> G_loss: 3.018e-02 
22-11-06 07:11:01.568 : Saving the model.
22-11-06 07:11:08.951 : ---1-->   0001.jpg | 24.81dB
22-11-06 07:11:17.370 : ---2-->   0002.jpg | 25.76dB
22-11-06 07:11:25.837 : ---3-->   0003.jpg | 29.95dB
22-11-06 07:11:33.505 : ---4-->   0004.jpg | 28.24dB
22-11-06 07:11:40.333 : ---5-->   0005.jpg | 29.09dB
22-11-06 07:11:49.004 : ---6-->   0006.jpg | 33.33dB
22-11-06 07:11:59.227 : ---7-->   0007.jpg | 35.24dB
22-11-06 07:12:03.133 : ---8-->   0008.jpg | 24.18dB
22-11-06 07:12:11.693 : ---9-->   0009.jpg | 23.50dB
22-11-06 07:12:22.730 : --10-->   0010.jpg | 28.19dB
22-11-06 07:12:31.520 : --11-->   0011.jpg | 25.98dB
22-11-06 07:12:33.817 : --12-->   0012.jpg | 24.20dB
22-11-06 07:12:39.820 : --13-->   0013.jpg | 24.02dB
22-11-06 07:12:43.046 : --14-->   0014.jpg | 23.28dB
22-11-06 07:12:49.198 : --15-->   0015.jpg | 21.77dB
22-11-06 07:12:50.857 : --16-->   0016.jpg | 30.08dB
22-11-06 07:12:57.775 : --17-->   0017.jpg | 29.37dB
22-11-06 07:13:06.700 : --18-->   0018.jpg | 27.25dB
22-11-06 07:13:13.615 : --19-->   0019.jpg | 32.45dB
22-11-06 07:13:17.461 : --20-->   0020.jpg | 23.97dB
22-11-06 07:13:24.795 : --21-->   0021.jpg | 26.56dB
22-11-06 07:13:27.265 : --22-->   0022.jpg | 38.62dB
22-11-06 07:13:30.898 : --23-->   0023.jpg | 25.41dB
22-11-06 07:13:35.213 : --24-->   0024.jpg | 34.21dB
22-11-06 07:13:37.578 : --25-->   0025.jpg | 26.03dB
22-11-06 07:13:42.020 : --26-->   0026.jpg | 26.93dB
22-11-06 07:13:52.796 : --27-->   0027.jpg | 29.13dB
22-11-06 07:13:55.984 : --28-->   0028.jpg | 30.14dB
22-11-06 07:14:01.711 : --29-->   0029.jpg | 24.21dB
22-11-06 07:14:07.416 : --30-->   0030.jpg | 29.18dB
22-11-06 07:14:13.809 : --31-->   0031.jpg | 28.18dB
22-11-06 07:14:19.712 : --32-->   0032.jpg | 26.11dB
22-11-06 07:14:23.410 : --33-->   0033.jpg | 23.61dB
22-11-06 07:14:25.524 : --34-->   0034.jpg | 23.33dB
22-11-06 07:14:30.601 : --35-->   0035.jpg | 26.83dB
22-11-06 07:14:37.780 : --36-->   0036.jpg | 23.28dB
22-11-06 07:14:42.740 : --37-->   0037.jpg | 22.96dB
22-11-06 07:14:46.721 : --38-->   0038.jpg | 26.49dB
22-11-06 07:14:47.735 : --39-->   0039.jpg | 24.23dB
22-11-06 07:14:50.114 : --40-->   0040.jpg | 28.22dB
22-11-06 07:14:55.249 : --41-->   0041.jpg | 20.99dB
22-11-06 07:14:58.190 : --42-->   0042.jpg | 26.75dB
22-11-06 07:15:05.246 : --43-->   0043.jpg | 27.29dB
22-11-06 07:15:16.609 : --44-->   0044.jpg | 24.74dB
22-11-06 07:15:19.450 : --45-->   0045.jpg | 21.61dB
22-11-06 07:15:26.731 : --46-->   0046.jpg | 23.28dB
22-11-06 07:15:30.754 : --47-->   0047.jpg | 24.39dB
22-11-06 07:15:35.290 : --48-->   0048.jpg | 28.27dB
22-11-06 07:15:40.784 : --49-->   0049.jpg | 28.31dB
22-11-06 07:15:43.076 : --50-->   0050.jpg | 21.05dB
22-11-06 07:15:44.980 : --51-->   0051.jpg | 25.75dB
22-11-06 07:15:50.606 : --52-->   0052.jpg | 23.53dB
22-11-06 07:15:56.148 : --53-->   0053.jpg | 24.56dB
22-11-06 07:16:00.890 : --54-->   0054.jpg | 22.07dB
22-11-06 07:16:07.368 : --55-->   0055.jpg | 36.31dB
22-11-06 07:16:10.224 : --56-->   0056.jpg | 24.67dB
22-11-06 07:16:12.527 : --57-->   0057.jpg | 30.45dB
22-11-06 07:16:18.039 : --58-->   0058.jpg | 26.56dB
22-11-06 07:16:22.978 : --59-->   0059.jpg | 27.94dB
22-11-06 07:16:27.476 : --60-->   0060.jpg | 24.91dB
22-11-06 07:16:33.439 : --61-->   0061.jpg | 26.45dB
22-11-06 07:16:38.712 : --62-->   0062.jpg | 24.38dB
22-11-06 07:16:48.502 : --63-->   0063.jpg | 23.17dB
22-11-06 07:16:50.399 : --64-->   0064.jpg | 29.17dB
22-11-06 07:16:51.496 : --65-->   0065.jpg | 34.96dB
22-11-06 07:16:54.541 : --66-->   0066.jpg | 30.21dB
22-11-06 07:17:00.115 : --67-->   0067.jpg | 26.66dB
22-11-06 07:17:07.621 : --68-->   0068.jpg | 25.22dB
22-11-06 07:17:14.267 : --69-->   0069.jpg | 19.46dB
22-11-06 07:17:17.107 : --70-->   0070.jpg | 21.11dB
22-11-06 07:17:20.294 : --71-->   0071.jpg | 22.57dB
22-11-06 07:17:25.266 : --72-->   0072.jpg | 28.18dB
22-11-06 07:17:29.656 : --73-->   0073.jpg | 27.46dB
22-11-06 07:17:35.498 : --74-->   0074.jpg | 24.36dB
22-11-06 07:17:42.313 : --75-->   0075.jpg | 24.52dB
22-11-06 07:17:49.527 : --76-->   0076.jpg | 25.23dB
22-11-06 07:17:51.811 : --77-->   0077.jpg | 22.48dB
22-11-06 07:17:57.611 : --78-->   0078.jpg | 27.74dB
22-11-06 07:18:02.743 : --79-->   0079.jpg | 20.48dB
22-11-06 07:18:06.190 : --80-->   0080.jpg | 29.96dB
22-11-06 07:18:08.859 : --81-->   0081.jpg | 28.67dB
22-11-06 07:18:13.900 : --82-->   0082.jpg | 28.43dB
22-11-06 07:18:20.788 : --83-->   0083.jpg | 22.92dB
22-11-06 07:18:25.164 : --84-->   0084.jpg | 28.96dB
22-11-06 07:18:29.128 : --85-->   0085.jpg | 20.98dB
22-11-06 07:18:32.723 : --86-->   0086.jpg | 24.48dB
22-11-06 07:18:38.611 : --87-->   0087.jpg | 20.99dB
22-11-06 07:18:45.025 : --88-->   0088.jpg | 27.05dB
22-11-06 07:18:48.999 : --89-->   0089.jpg | 23.89dB
22-11-06 07:18:52.047 : --90-->   0090.jpg | 21.81dB
22-11-06 07:18:57.311 : --91-->   0091.jpg | 30.93dB
22-11-06 07:19:03.558 : --92-->   0092.jpg | 22.38dB
22-11-06 07:19:08.321 : --93-->   0093.jpg | 27.60dB
22-11-06 07:19:13.268 : --94-->   0094.jpg | 30.44dB
22-11-06 07:19:19.814 : --95-->   0095.jpg | 25.16dB
22-11-06 07:19:27.656 : --96-->   0096.jpg | 20.80dB
22-11-06 07:19:35.083 : --97-->   0097.jpg | 32.55dB
22-11-06 07:19:36.899 : --98-->   0098.jpg | 25.44dB
22-11-06 07:19:40.886 : --99-->   0099.jpg | 24.92dB
22-11-06 07:19:45.883 : -100-->   0100.jpg | 23.46dB
22-11-06 07:19:46.159 : <epoch:286, iter:  85,000, Average PSNR : 26.27dB

22-11-06 07:22:15.581 : <epoch:286, iter:  85,200, lr:2.000e-04> G_loss: 3.220e-02 
22-11-06 07:24:42.850 : <epoch:287, iter:  85,400, lr:2.000e-04> G_loss: 3.252e-02 
22-11-06 07:27:12.922 : <epoch:288, iter:  85,600, lr:2.000e-04> G_loss: 3.706e-02 
22-11-06 07:29:43.717 : <epoch:288, iter:  85,800, lr:2.000e-04> G_loss: 3.523e-02 
22-11-06 07:32:04.874 : <epoch:289, iter:  86,000, lr:2.000e-04> G_loss: 3.957e-02 
22-11-06 07:34:43.054 : <epoch:290, iter:  86,200, lr:2.000e-04> G_loss: 2.601e-02 
22-11-06 07:37:09.799 : <epoch:290, iter:  86,400, lr:2.000e-04> G_loss: 2.875e-02 
22-11-06 07:39:46.403 : <epoch:291, iter:  86,600, lr:2.000e-04> G_loss: 3.481e-02 
22-11-06 07:42:20.735 : <epoch:292, iter:  86,800, lr:2.000e-04> G_loss: 3.201e-02 
22-11-06 07:44:56.726 : <epoch:292, iter:  87,000, lr:2.000e-04> G_loss: 3.521e-02 
22-11-06 07:47:26.953 : <epoch:293, iter:  87,200, lr:2.000e-04> G_loss: 3.055e-02 
22-11-06 07:50:03.681 : <epoch:294, iter:  87,400, lr:2.000e-04> G_loss: 3.932e-02 
22-11-06 07:52:28.250 : <epoch:294, iter:  87,600, lr:2.000e-04> G_loss: 2.994e-02 
22-11-06 07:54:58.090 : <epoch:295, iter:  87,800, lr:2.000e-04> G_loss: 2.776e-02 
22-11-06 07:57:24.697 : <epoch:296, iter:  88,000, lr:2.000e-04> G_loss: 2.630e-02 
22-11-06 07:59:53.010 : <epoch:296, iter:  88,200, lr:2.000e-04> G_loss: 3.438e-02 
22-11-06 08:02:31.642 : <epoch:297, iter:  88,400, lr:2.000e-04> G_loss: 3.885e-02 
22-11-06 08:05:07.062 : <epoch:298, iter:  88,600, lr:2.000e-04> G_loss: 2.901e-02 
22-11-06 08:07:32.288 : <epoch:298, iter:  88,800, lr:2.000e-04> G_loss: 2.743e-02 
22-11-06 08:10:09.968 : <epoch:299, iter:  89,000, lr:2.000e-04> G_loss: 3.189e-02 
22-11-06 08:12:38.391 : <epoch:300, iter:  89,200, lr:2.000e-04> G_loss: 3.088e-02 
22-11-06 08:15:10.677 : <epoch:301, iter:  89,400, lr:2.000e-04> G_loss: 3.307e-02 
22-11-06 08:17:40.546 : <epoch:301, iter:  89,600, lr:2.000e-04> G_loss: 2.531e-02 
22-11-06 08:20:15.724 : <epoch:302, iter:  89,800, lr:2.000e-04> G_loss: 3.110e-02 
22-11-06 08:22:38.374 : <epoch:303, iter:  90,000, lr:2.000e-04> G_loss: 3.367e-02 
22-11-06 08:22:38.376 : Saving the model.
22-11-06 08:22:47.660 : ---1-->   0001.jpg | 24.90dB
22-11-06 08:22:57.749 : ---2-->   0002.jpg | 25.72dB
22-11-06 08:23:06.541 : ---3-->   0003.jpg | 30.00dB
22-11-06 08:23:10.548 : ---4-->   0004.jpg | 28.29dB
22-11-06 08:23:20.692 : ---5-->   0005.jpg | 29.15dB
22-11-06 08:23:25.323 : ---6-->   0006.jpg | 33.05dB
22-11-06 08:23:29.201 : ---7-->   0007.jpg | 35.52dB
22-11-06 08:23:37.267 : ---8-->   0008.jpg | 24.26dB
22-11-06 08:23:39.169 : ---9-->   0009.jpg | 23.47dB
22-11-06 08:23:43.006 : --10-->   0010.jpg | 28.21dB
22-11-06 08:23:48.157 : --11-->   0011.jpg | 25.99dB
22-11-06 08:23:58.536 : --12-->   0012.jpg | 24.07dB
22-11-06 08:24:00.571 : --13-->   0013.jpg | 24.07dB
22-11-06 08:24:04.430 : --14-->   0014.jpg | 23.30dB
22-11-06 08:24:15.313 : --15-->   0015.jpg | 21.69dB
22-11-06 08:24:19.048 : --16-->   0016.jpg | 30.09dB
22-11-06 08:24:24.267 : --17-->   0017.jpg | 29.25dB
22-11-06 08:24:29.882 : --18-->   0018.jpg | 26.99dB
22-11-06 08:24:37.120 : --19-->   0019.jpg | 32.53dB
22-11-06 08:24:42.114 : --20-->   0020.jpg | 23.94dB
22-11-06 08:24:45.578 : --21-->   0021.jpg | 26.55dB
22-11-06 08:24:48.685 : --22-->   0022.jpg | 38.84dB
22-11-06 08:24:55.582 : --23-->   0023.jpg | 25.50dB
22-11-06 08:25:00.075 : --24-->   0024.jpg | 33.92dB
22-11-06 08:25:03.740 : --25-->   0025.jpg | 25.97dB
22-11-06 08:25:07.020 : --26-->   0026.jpg | 26.98dB
22-11-06 08:25:08.679 : --27-->   0027.jpg | 29.13dB
22-11-06 08:25:10.714 : --28-->   0028.jpg | 30.34dB
22-11-06 08:25:14.214 : --29-->   0029.jpg | 24.19dB
22-11-06 08:25:19.808 : --30-->   0030.jpg | 29.18dB
22-11-06 08:25:25.322 : --31-->   0031.jpg | 28.23dB
22-11-06 08:25:29.750 : --32-->   0032.jpg | 26.11dB
22-11-06 08:25:35.514 : --33-->   0033.jpg | 23.61dB
22-11-06 08:25:39.775 : --34-->   0034.jpg | 23.36dB
22-11-06 08:25:43.963 : --35-->   0035.jpg | 26.78dB
22-11-06 08:25:47.875 : --36-->   0036.jpg | 23.24dB
22-11-06 08:25:51.383 : --37-->   0037.jpg | 23.02dB
22-11-06 08:25:55.970 : --38-->   0038.jpg | 26.56dB
22-11-06 08:26:02.344 : --39-->   0039.jpg | 24.06dB
22-11-06 08:26:09.441 : --40-->   0040.jpg | 28.19dB
22-11-06 08:26:13.936 : --41-->   0041.jpg | 21.16dB
22-11-06 08:26:22.017 : --42-->   0042.jpg | 26.79dB
22-11-06 08:26:29.672 : --43-->   0043.jpg | 27.36dB
22-11-06 08:26:35.767 : --44-->   0044.jpg | 24.71dB
22-11-06 08:26:42.655 : --45-->   0045.jpg | 21.61dB
22-11-06 08:26:48.304 : --46-->   0046.jpg | 23.37dB
22-11-06 08:26:53.761 : --47-->   0047.jpg | 24.35dB
22-11-06 08:26:56.776 : --48-->   0048.jpg | 28.29dB
22-11-06 08:27:05.658 : --49-->   0049.jpg | 28.40dB
22-11-06 08:27:11.185 : --50-->   0050.jpg | 21.05dB
22-11-06 08:27:16.038 : --51-->   0051.jpg | 25.76dB
22-11-06 08:27:21.645 : --52-->   0052.jpg | 23.37dB
22-11-06 08:27:30.795 : --53-->   0053.jpg | 24.51dB
22-11-06 08:27:32.702 : --54-->   0054.jpg | 22.09dB
22-11-06 08:27:37.387 : --55-->   0055.jpg | 36.27dB
22-11-06 08:27:38.049 : --56-->   0056.jpg | 24.68dB
22-11-06 08:27:39.568 : --57-->   0057.jpg | 30.40dB
22-11-06 08:27:45.477 : --58-->   0058.jpg | 26.85dB
22-11-06 08:27:48.096 : --59-->   0059.jpg | 27.91dB
22-11-06 08:27:50.884 : --60-->   0060.jpg | 24.93dB
22-11-06 08:27:54.554 : --61-->   0061.jpg | 26.31dB
22-11-06 08:27:56.873 : --62-->   0062.jpg | 24.39dB
22-11-06 08:28:02.127 : --63-->   0063.jpg | 23.21dB
22-11-06 08:28:08.451 : --64-->   0064.jpg | 29.08dB
22-11-06 08:28:10.828 : --65-->   0065.jpg | 35.12dB
22-11-06 08:28:13.213 : --66-->   0066.jpg | 29.99dB
22-11-06 08:28:21.759 : --67-->   0067.jpg | 26.75dB
22-11-06 08:28:26.477 : --68-->   0068.jpg | 25.18dB
22-11-06 08:28:30.646 : --69-->   0069.jpg | 19.45dB
22-11-06 08:28:37.728 : --70-->   0070.jpg | 21.16dB
22-11-06 08:28:42.290 : --71-->   0071.jpg | 22.57dB
22-11-06 08:28:44.779 : --72-->   0072.jpg | 28.23dB
22-11-06 08:28:50.525 : --73-->   0073.jpg | 27.36dB
22-11-06 08:28:58.342 : --74-->   0074.jpg | 24.45dB
22-11-06 08:29:02.002 : --75-->   0075.jpg | 24.58dB
22-11-06 08:29:05.181 : --76-->   0076.jpg | 25.25dB
22-11-06 08:29:13.225 : --77-->   0077.jpg | 22.53dB
22-11-06 08:29:14.746 : --78-->   0078.jpg | 27.83dB
22-11-06 08:29:17.522 : --79-->   0079.jpg | 20.47dB
22-11-06 08:29:21.648 : --80-->   0080.jpg | 29.93dB
22-11-06 08:29:24.547 : --81-->   0081.jpg | 28.63dB
22-11-06 08:29:28.297 : --82-->   0082.jpg | 28.40dB
22-11-06 08:29:33.311 : --83-->   0083.jpg | 23.03dB
22-11-06 08:29:43.367 : --84-->   0084.jpg | 28.97dB
22-11-06 08:29:47.504 : --85-->   0085.jpg | 21.07dB
22-11-06 08:29:50.312 : --86-->   0086.jpg | 24.36dB
22-11-06 08:29:54.353 : --87-->   0087.jpg | 20.83dB
22-11-06 08:29:59.732 : --88-->   0088.jpg | 27.07dB
22-11-06 08:30:04.255 : --89-->   0089.jpg | 23.84dB
22-11-06 08:30:09.248 : --90-->   0090.jpg | 21.79dB
22-11-06 08:30:17.382 : --91-->   0091.jpg | 30.96dB
22-11-06 08:30:22.631 : --92-->   0092.jpg | 22.45dB
22-11-06 08:30:23.915 : --93-->   0093.jpg | 27.68dB
22-11-06 08:30:29.982 : --94-->   0094.jpg | 30.49dB
22-11-06 08:30:36.215 : --95-->   0095.jpg | 25.13dB
22-11-06 08:30:39.468 : --96-->   0096.jpg | 20.83dB
22-11-06 08:30:46.313 : --97-->   0097.jpg | 32.52dB
22-11-06 08:30:52.190 : --98-->   0098.jpg | 25.53dB
22-11-06 08:30:53.834 : --99-->   0099.jpg | 24.86dB
22-11-06 08:30:59.129 : -100-->   0100.jpg | 23.48dB
22-11-06 08:30:59.376 : <epoch:303, iter:  90,000, Average PSNR : 26.28dB

22-11-06 08:33:30.684 : <epoch:303, iter:  90,200, lr:2.000e-04> G_loss: 2.740e-02 
22-11-06 08:35:48.656 : <epoch:304, iter:  90,400, lr:2.000e-04> G_loss: 3.319e-02 
22-11-06 08:38:15.420 : <epoch:305, iter:  90,600, lr:2.000e-04> G_loss: 3.780e-02 
22-11-06 08:40:42.452 : <epoch:305, iter:  90,800, lr:2.000e-04> G_loss: 3.638e-02 
22-11-06 08:43:09.924 : <epoch:306, iter:  91,000, lr:2.000e-04> G_loss: 3.308e-02 
22-11-06 08:45:40.300 : <epoch:307, iter:  91,200, lr:2.000e-04> G_loss: 2.806e-02 
22-11-06 08:48:05.357 : <epoch:307, iter:  91,400, lr:2.000e-04> G_loss: 3.001e-02 
22-11-06 08:50:31.712 : <epoch:308, iter:  91,600, lr:2.000e-04> G_loss: 3.820e-02 
22-11-06 08:52:58.885 : <epoch:309, iter:  91,800, lr:2.000e-04> G_loss: 3.016e-02 
22-11-06 08:55:30.420 : <epoch:309, iter:  92,000, lr:2.000e-04> G_loss: 2.739e-02 
22-11-06 08:57:52.668 : <epoch:310, iter:  92,200, lr:2.000e-04> G_loss: 3.531e-02 
22-11-06 09:00:11.943 : <epoch:311, iter:  92,400, lr:2.000e-04> G_loss: 2.644e-02 
22-11-06 09:02:34.111 : <epoch:311, iter:  92,600, lr:2.000e-04> G_loss: 3.813e-02 
22-11-06 09:05:03.169 : <epoch:312, iter:  92,800, lr:2.000e-04> G_loss: 3.697e-02 
22-11-06 09:07:35.900 : <epoch:313, iter:  93,000, lr:2.000e-04> G_loss: 3.466e-02 
22-11-06 09:10:07.962 : <epoch:313, iter:  93,200, lr:2.000e-04> G_loss: 3.085e-02 
22-11-06 09:12:47.787 : <epoch:314, iter:  93,400, lr:2.000e-04> G_loss: 3.275e-02 
22-11-06 09:15:17.278 : <epoch:315, iter:  93,600, lr:2.000e-04> G_loss: 2.680e-02 
22-11-06 09:17:42.926 : <epoch:315, iter:  93,800, lr:2.000e-04> G_loss: 3.335e-02 
22-11-06 09:20:06.924 : <epoch:316, iter:  94,000, lr:2.000e-04> G_loss: 3.853e-02 
22-11-06 09:22:23.552 : <epoch:317, iter:  94,200, lr:2.000e-04> G_loss: 3.813e-02 
22-11-06 09:24:44.105 : <epoch:317, iter:  94,400, lr:2.000e-04> G_loss: 2.938e-02 
22-11-06 09:27:05.636 : <epoch:318, iter:  94,600, lr:2.000e-04> G_loss: 3.933e-02 
22-11-06 09:29:35.037 : <epoch:319, iter:  94,800, lr:2.000e-04> G_loss: 2.926e-02 
22-11-06 09:31:59.108 : <epoch:319, iter:  95,000, lr:2.000e-04> G_loss: 2.726e-02 
22-11-06 09:31:59.110 : Saving the model.
22-11-06 09:32:07.431 : ---1-->   0001.jpg | 24.73dB
22-11-06 09:32:11.024 : ---2-->   0002.jpg | 25.66dB
22-11-06 09:32:16.066 : ---3-->   0003.jpg | 29.93dB
22-11-06 09:32:21.097 : ---4-->   0004.jpg | 28.27dB
22-11-06 09:32:22.484 : ---5-->   0005.jpg | 29.01dB
22-11-06 09:32:23.784 : ---6-->   0006.jpg | 32.96dB
22-11-06 09:32:26.236 : ---7-->   0007.jpg | 35.14dB
22-11-06 09:32:29.570 : ---8-->   0008.jpg | 24.14dB
22-11-06 09:32:34.983 : ---9-->   0009.jpg | 23.54dB
22-11-06 09:32:42.459 : --10-->   0010.jpg | 28.21dB
22-11-06 09:32:45.440 : --11-->   0011.jpg | 25.96dB
22-11-06 09:32:49.553 : --12-->   0012.jpg | 24.19dB
22-11-06 09:32:52.866 : --13-->   0013.jpg | 24.07dB
22-11-06 09:32:56.956 : --14-->   0014.jpg | 23.44dB
22-11-06 09:33:02.916 : --15-->   0015.jpg | 21.64dB
22-11-06 09:33:05.720 : --16-->   0016.jpg | 29.87dB
22-11-06 09:33:13.595 : --17-->   0017.jpg | 29.16dB
22-11-06 09:33:15.598 : --18-->   0018.jpg | 26.89dB
22-11-06 09:33:17.541 : --19-->   0019.jpg | 32.50dB
22-11-06 09:33:19.891 : --20-->   0020.jpg | 23.84dB
22-11-06 09:33:26.576 : --21-->   0021.jpg | 26.45dB
22-11-06 09:33:31.832 : --22-->   0022.jpg | 38.76dB
22-11-06 09:33:37.066 : --23-->   0023.jpg | 25.47dB
22-11-06 09:33:43.924 : --24-->   0024.jpg | 33.97dB
22-11-06 09:33:51.790 : --25-->   0025.jpg | 26.05dB
22-11-06 09:33:58.791 : --26-->   0026.jpg | 26.80dB
22-11-06 09:34:04.956 : --27-->   0027.jpg | 29.14dB
22-11-06 09:34:08.492 : --28-->   0028.jpg | 30.53dB
22-11-06 09:34:12.003 : --29-->   0029.jpg | 24.20dB
22-11-06 09:34:16.898 : --30-->   0030.jpg | 29.14dB
22-11-06 09:34:18.908 : --31-->   0031.jpg | 28.06dB
22-11-06 09:34:21.216 : --32-->   0032.jpg | 26.11dB
22-11-06 09:34:23.445 : --33-->   0033.jpg | 23.49dB
22-11-06 09:34:29.526 : --34-->   0034.jpg | 23.36dB
22-11-06 09:34:33.738 : --35-->   0035.jpg | 26.82dB
22-11-06 09:34:36.984 : --36-->   0036.jpg | 23.22dB
22-11-06 09:34:41.549 : --37-->   0037.jpg | 22.94dB
22-11-06 09:34:46.492 : --38-->   0038.jpg | 26.49dB
22-11-06 09:34:54.690 : --39-->   0039.jpg | 24.07dB
22-11-06 09:34:56.601 : --40-->   0040.jpg | 28.08dB
22-11-06 09:34:59.567 : --41-->   0041.jpg | 21.11dB
22-11-06 09:35:06.289 : --42-->   0042.jpg | 26.77dB
22-11-06 09:35:12.535 : --43-->   0043.jpg | 27.30dB
22-11-06 09:35:17.172 : --44-->   0044.jpg | 24.61dB
22-11-06 09:35:23.585 : --45-->   0045.jpg | 21.64dB
22-11-06 09:35:29.801 : --46-->   0046.jpg | 23.24dB
22-11-06 09:35:34.402 : --47-->   0047.jpg | 24.30dB
22-11-06 09:35:37.240 : --48-->   0048.jpg | 28.36dB
22-11-06 09:35:44.273 : --49-->   0049.jpg | 28.19dB
22-11-06 09:35:51.048 : --50-->   0050.jpg | 21.11dB
22-11-06 09:35:55.210 : --51-->   0051.jpg | 25.82dB
22-11-06 09:36:01.020 : --52-->   0052.jpg | 23.46dB
22-11-06 09:36:04.490 : --53-->   0053.jpg | 24.45dB
22-11-06 09:36:06.572 : --54-->   0054.jpg | 22.07dB
22-11-06 09:36:09.128 : --55-->   0055.jpg | 36.21dB
22-11-06 09:36:12.988 : --56-->   0056.jpg | 24.62dB
22-11-06 09:36:19.203 : --57-->   0057.jpg | 30.57dB
22-11-06 09:36:25.105 : --58-->   0058.jpg | 26.64dB
22-11-06 09:36:30.335 : --59-->   0059.jpg | 27.98dB
22-11-06 09:36:32.433 : --60-->   0060.jpg | 24.76dB
22-11-06 09:36:37.112 : --61-->   0061.jpg | 26.33dB
22-11-06 09:36:43.146 : --62-->   0062.jpg | 24.23dB
22-11-06 09:36:47.101 : --63-->   0063.jpg | 23.17dB
22-11-06 09:36:53.432 : --64-->   0064.jpg | 29.02dB
22-11-06 09:37:02.116 : --65-->   0065.jpg | 34.23dB
22-11-06 09:37:06.495 : --66-->   0066.jpg | 29.93dB
22-11-06 09:37:14.040 : --67-->   0067.jpg | 26.68dB
22-11-06 09:37:20.848 : --68-->   0068.jpg | 25.27dB
22-11-06 09:37:25.404 : --69-->   0069.jpg | 19.43dB
22-11-06 09:37:27.984 : --70-->   0070.jpg | 21.19dB
22-11-06 09:37:30.806 : --71-->   0071.jpg | 22.56dB
22-11-06 09:37:36.301 : --72-->   0072.jpg | 28.01dB
22-11-06 09:37:45.272 : --73-->   0073.jpg | 27.44dB
22-11-06 09:37:54.197 : --74-->   0074.jpg | 24.32dB
22-11-06 09:37:56.105 : --75-->   0075.jpg | 24.53dB
22-11-06 09:37:57.460 : --76-->   0076.jpg | 25.22dB
22-11-06 09:38:00.427 : --77-->   0077.jpg | 22.29dB
22-11-06 09:38:05.283 : --78-->   0078.jpg | 27.82dB
22-11-06 09:38:08.460 : --79-->   0079.jpg | 20.47dB
22-11-06 09:38:10.428 : --80-->   0080.jpg | 30.04dB
22-11-06 09:38:15.316 : --81-->   0081.jpg | 28.62dB
22-11-06 09:38:20.410 : --82-->   0082.jpg | 28.36dB
22-11-06 09:38:25.187 : --83-->   0083.jpg | 22.87dB
22-11-06 09:38:29.871 : --84-->   0084.jpg | 29.01dB
22-11-06 09:38:33.011 : --85-->   0085.jpg | 20.92dB
22-11-06 09:38:36.046 : --86-->   0086.jpg | 24.35dB
22-11-06 09:38:39.182 : --87-->   0087.jpg | 21.03dB
22-11-06 09:38:43.214 : --88-->   0088.jpg | 27.12dB
22-11-06 09:38:48.025 : --89-->   0089.jpg | 23.88dB
22-11-06 09:38:54.958 : --90-->   0090.jpg | 21.78dB
22-11-06 09:38:59.981 : --91-->   0091.jpg | 30.67dB
22-11-06 09:39:04.040 : --92-->   0092.jpg | 22.39dB
22-11-06 09:39:07.884 : --93-->   0093.jpg | 27.72dB
22-11-06 09:39:15.707 : --94-->   0094.jpg | 30.44dB
22-11-06 09:39:20.509 : --95-->   0095.jpg | 25.15dB
22-11-06 09:39:28.307 : --96-->   0096.jpg | 20.81dB
22-11-06 09:39:31.202 : --97-->   0097.jpg | 32.51dB
22-11-06 09:39:36.854 : --98-->   0098.jpg | 25.38dB
22-11-06 09:39:42.586 : --99-->   0099.jpg | 24.81dB
22-11-06 09:39:49.465 : -100-->   0100.jpg | 23.42dB
22-11-06 09:39:49.797 : <epoch:319, iter:  95,000, Average PSNR : 26.23dB

22-11-06 09:42:11.786 : <epoch:320, iter:  95,200, lr:2.000e-04> G_loss: 3.237e-02 
22-11-06 09:44:36.075 : <epoch:321, iter:  95,400, lr:2.000e-04> G_loss: 2.762e-02 
22-11-06 09:47:02.712 : <epoch:321, iter:  95,600, lr:2.000e-04> G_loss: 2.984e-02 
22-11-06 09:49:26.405 : <epoch:322, iter:  95,800, lr:2.000e-04> G_loss: 3.799e-02 
22-11-06 09:51:46.965 : <epoch:323, iter:  96,000, lr:2.000e-04> G_loss: 3.443e-02 
22-11-06 09:54:05.045 : <epoch:323, iter:  96,200, lr:2.000e-04> G_loss: 3.731e-02 
22-11-06 09:56:25.918 : <epoch:324, iter:  96,400, lr:2.000e-04> G_loss: 3.296e-02 
22-11-06 09:58:50.208 : <epoch:325, iter:  96,600, lr:2.000e-04> G_loss: 3.263e-02 
22-11-06 10:01:09.214 : <epoch:325, iter:  96,800, lr:2.000e-04> G_loss: 3.115e-02 
22-11-06 10:03:37.833 : <epoch:326, iter:  97,000, lr:2.000e-04> G_loss: 3.117e-02 
22-11-06 10:05:59.698 : <epoch:327, iter:  97,200, lr:2.000e-04> G_loss: 4.016e-02 
22-11-06 10:08:13.460 : <epoch:327, iter:  97,400, lr:2.000e-04> G_loss: 3.040e-02 
22-11-06 10:10:44.985 : <epoch:328, iter:  97,600, lr:2.000e-04> G_loss: 3.041e-02 
22-11-06 10:13:03.815 : <epoch:329, iter:  97,800, lr:2.000e-04> G_loss: 3.403e-02 
22-11-06 10:15:27.237 : <epoch:329, iter:  98,000, lr:2.000e-04> G_loss: 3.451e-02 
22-11-06 10:17:50.712 : <epoch:330, iter:  98,200, lr:2.000e-04> G_loss: 3.116e-02 
22-11-06 10:20:10.207 : <epoch:331, iter:  98,400, lr:2.000e-04> G_loss: 2.891e-02 
22-11-06 10:22:28.738 : <epoch:331, iter:  98,600, lr:2.000e-04> G_loss: 3.677e-02 
22-11-06 10:25:01.936 : <epoch:332, iter:  98,800, lr:2.000e-04> G_loss: 3.588e-02 
22-11-06 10:27:30.173 : <epoch:333, iter:  99,000, lr:2.000e-04> G_loss: 3.394e-02 
22-11-06 10:29:50.118 : <epoch:334, iter:  99,200, lr:2.000e-04> G_loss: 3.682e-02 
22-11-06 10:32:04.153 : <epoch:334, iter:  99,400, lr:2.000e-04> G_loss: 3.257e-02 
22-11-06 10:34:27.627 : <epoch:335, iter:  99,600, lr:2.000e-04> G_loss: 2.841e-02 
22-11-06 10:36:48.945 : <epoch:336, iter:  99,800, lr:2.000e-04> G_loss: 3.415e-02 
22-11-06 10:39:07.761 : <epoch:336, iter: 100,000, lr:2.000e-04> G_loss: 3.018e-02 
22-11-06 10:39:07.764 : Saving the model.
22-11-06 10:39:15.207 : ---1-->   0001.jpg | 24.84dB
22-11-06 10:39:20.082 : ---2-->   0002.jpg | 25.71dB
22-11-06 10:39:21.395 : ---3-->   0003.jpg | 30.09dB
22-11-06 10:39:25.668 : ---4-->   0004.jpg | 28.23dB
22-11-06 10:39:31.650 : ---5-->   0005.jpg | 29.17dB
22-11-06 10:39:33.558 : ---6-->   0006.jpg | 33.07dB
22-11-06 10:39:36.946 : ---7-->   0007.jpg | 35.29dB
22-11-06 10:39:42.775 : ---8-->   0008.jpg | 24.24dB
22-11-06 10:39:47.601 : ---9-->   0009.jpg | 23.47dB
22-11-06 10:39:51.568 : --10-->   0010.jpg | 28.18dB
22-11-06 10:39:59.797 : --11-->   0011.jpg | 26.16dB
22-11-06 10:40:04.903 : --12-->   0012.jpg | 24.26dB
22-11-06 10:40:11.479 : --13-->   0013.jpg | 24.07dB
22-11-06 10:40:15.878 : --14-->   0014.jpg | 23.34dB
22-11-06 10:40:20.967 : --15-->   0015.jpg | 21.88dB
22-11-06 10:40:23.099 : --16-->   0016.jpg | 30.02dB
22-11-06 10:40:30.954 : --17-->   0017.jpg | 29.41dB
22-11-06 10:40:34.984 : --18-->   0018.jpg | 27.35dB
22-11-06 10:40:43.618 : --19-->   0019.jpg | 32.60dB
22-11-06 10:40:48.275 : --20-->   0020.jpg | 23.95dB
22-11-06 10:40:53.994 : --21-->   0021.jpg | 26.62dB
22-11-06 10:40:59.176 : --22-->   0022.jpg | 38.68dB
22-11-06 10:41:03.289 : --23-->   0023.jpg | 25.39dB
22-11-06 10:41:07.943 : --24-->   0024.jpg | 34.27dB
22-11-06 10:41:10.953 : --25-->   0025.jpg | 26.07dB
22-11-06 10:41:13.038 : --26-->   0026.jpg | 27.03dB
22-11-06 10:41:20.439 : --27-->   0027.jpg | 29.13dB
22-11-06 10:41:23.838 : --28-->   0028.jpg | 30.30dB
22-11-06 10:41:27.938 : --29-->   0029.jpg | 24.21dB
22-11-06 10:41:33.496 : --30-->   0030.jpg | 29.32dB
22-11-06 10:41:39.530 : --31-->   0031.jpg | 28.23dB
22-11-06 10:41:47.368 : --32-->   0032.jpg | 26.24dB
22-11-06 10:41:53.877 : --33-->   0033.jpg | 23.54dB
22-11-06 10:41:57.400 : --34-->   0034.jpg | 23.41dB
22-11-06 10:42:04.511 : --35-->   0035.jpg | 26.79dB
22-11-06 10:42:13.161 : --36-->   0036.jpg | 23.29dB
22-11-06 10:42:17.827 : --37-->   0037.jpg | 22.96dB
22-11-06 10:42:24.076 : --38-->   0038.jpg | 26.67dB
22-11-06 10:42:30.551 : --39-->   0039.jpg | 24.09dB
22-11-06 10:42:34.419 : --40-->   0040.jpg | 28.18dB
22-11-06 10:42:38.832 : --41-->   0041.jpg | 20.95dB
22-11-06 10:42:47.353 : --42-->   0042.jpg | 26.77dB
22-11-06 10:42:53.146 : --43-->   0043.jpg | 27.35dB
22-11-06 10:43:01.141 : --44-->   0044.jpg | 24.73dB
22-11-06 10:43:05.260 : --45-->   0045.jpg | 21.59dB
22-11-06 10:43:10.761 : --46-->   0046.jpg | 23.31dB
22-11-06 10:43:16.724 : --47-->   0047.jpg | 24.42dB
22-11-06 10:43:18.837 : --48-->   0048.jpg | 28.24dB
22-11-06 10:43:24.229 : --49-->   0049.jpg | 28.30dB
22-11-06 10:43:27.775 : --50-->   0050.jpg | 21.12dB
22-11-06 10:43:32.021 : --51-->   0051.jpg | 25.87dB
22-11-06 10:43:35.609 : --52-->   0052.jpg | 23.57dB
22-11-06 10:43:40.554 : --53-->   0053.jpg | 24.59dB
22-11-06 10:43:47.577 : --54-->   0054.jpg | 22.06dB
22-11-06 10:43:53.803 : --55-->   0055.jpg | 36.31dB
22-11-06 10:44:01.049 : --56-->   0056.jpg | 24.83dB
22-11-06 10:44:05.513 : --57-->   0057.jpg | 30.41dB
22-11-06 10:44:10.616 : --58-->   0058.jpg | 26.71dB
22-11-06 10:44:12.573 : --59-->   0059.jpg | 27.97dB
22-11-06 10:44:14.801 : --60-->   0060.jpg | 24.82dB
22-11-06 10:44:21.232 : --61-->   0061.jpg | 26.51dB
22-11-06 10:44:28.256 : --62-->   0062.jpg | 24.42dB
22-11-06 10:44:31.935 : --63-->   0063.jpg | 23.17dB
22-11-06 10:44:35.193 : --64-->   0064.jpg | 28.88dB
22-11-06 10:44:41.976 : --65-->   0065.jpg | 35.09dB
22-11-06 10:44:47.208 : --66-->   0066.jpg | 30.07dB
22-11-06 10:44:53.193 : --67-->   0067.jpg | 26.79dB
22-11-06 10:44:58.365 : --68-->   0068.jpg | 25.20dB
22-11-06 10:45:01.954 : --69-->   0069.jpg | 19.47dB
22-11-06 10:45:05.815 : --70-->   0070.jpg | 21.19dB
22-11-06 10:45:08.864 : --71-->   0071.jpg | 22.54dB
22-11-06 10:45:11.020 : --72-->   0072.jpg | 28.12dB
22-11-06 10:45:14.746 : --73-->   0073.jpg | 27.43dB
22-11-06 10:45:17.329 : --74-->   0074.jpg | 24.35dB
22-11-06 10:45:21.445 : --75-->   0075.jpg | 24.53dB
22-11-06 10:45:25.853 : --76-->   0076.jpg | 25.20dB
22-11-06 10:45:27.417 : --77-->   0077.jpg | 22.51dB
22-11-06 10:45:29.049 : --78-->   0078.jpg | 27.87dB
22-11-06 10:45:36.034 : --79-->   0079.jpg | 20.47dB
22-11-06 10:45:38.639 : --80-->   0080.jpg | 30.07dB
22-11-06 10:45:41.306 : --81-->   0081.jpg | 28.65dB
22-11-06 10:45:43.233 : --82-->   0082.jpg | 28.47dB
22-11-06 10:45:48.669 : --83-->   0083.jpg | 23.04dB
22-11-06 10:45:56.967 : --84-->   0084.jpg | 29.07dB
22-11-06 10:45:58.341 : --85-->   0085.jpg | 20.78dB
22-11-06 10:46:02.985 : --86-->   0086.jpg | 24.47dB
22-11-06 10:46:04.328 : --87-->   0087.jpg | 20.84dB
22-11-06 10:46:13.635 : --88-->   0088.jpg | 27.20dB
22-11-06 10:46:19.950 : --89-->   0089.jpg | 23.89dB
22-11-06 10:46:26.195 : --90-->   0090.jpg | 21.77dB
22-11-06 10:46:34.856 : --91-->   0091.jpg | 31.06dB
22-11-06 10:46:39.488 : --92-->   0092.jpg | 22.39dB
22-11-06 10:46:43.935 : --93-->   0093.jpg | 27.55dB
22-11-06 10:46:52.215 : --94-->   0094.jpg | 30.59dB
22-11-06 10:47:00.131 : --95-->   0095.jpg | 25.16dB
22-11-06 10:47:07.606 : --96-->   0096.jpg | 20.86dB
22-11-06 10:47:15.520 : --97-->   0097.jpg | 32.62dB
22-11-06 10:47:18.566 : --98-->   0098.jpg | 25.53dB
22-11-06 10:47:20.605 : --99-->   0099.jpg | 24.91dB
22-11-06 10:47:24.452 : -100-->   0100.jpg | 23.50dB
22-11-06 10:47:24.755 : <epoch:336, iter: 100,000, Average PSNR : 26.30dB

22-11-06 10:49:48.139 : <epoch:337, iter: 100,200, lr:2.000e-04> G_loss: 3.407e-02 
22-11-06 10:52:12.030 : <epoch:338, iter: 100,400, lr:2.000e-04> G_loss: 3.168e-02 
22-11-06 10:54:32.478 : <epoch:338, iter: 100,600, lr:2.000e-04> G_loss: 2.709e-02 
22-11-06 10:57:00.544 : <epoch:339, iter: 100,800, lr:2.000e-04> G_loss: 2.601e-02 
22-11-06 10:59:21.297 : <epoch:340, iter: 101,000, lr:2.000e-04> G_loss: 4.340e-02 
22-11-06 11:01:37.353 : <epoch:340, iter: 101,200, lr:2.000e-04> G_loss: 2.956e-02 
22-11-06 11:04:03.646 : <epoch:341, iter: 101,400, lr:2.000e-04> G_loss: 2.561e-02 
22-11-06 11:06:30.471 : <epoch:342, iter: 101,600, lr:2.000e-04> G_loss: 3.503e-02 
22-11-06 11:08:53.961 : <epoch:342, iter: 101,800, lr:2.000e-04> G_loss: 4.262e-02 
22-11-06 11:11:19.983 : <epoch:343, iter: 102,000, lr:2.000e-04> G_loss: 2.857e-02 
22-11-06 11:13:38.881 : <epoch:344, iter: 102,200, lr:2.000e-04> G_loss: 3.478e-02 
22-11-06 11:15:52.687 : <epoch:344, iter: 102,400, lr:2.000e-04> G_loss: 3.825e-02 
22-11-06 11:18:05.760 : <epoch:345, iter: 102,600, lr:2.000e-04> G_loss: 3.752e-02 
22-11-06 11:20:27.100 : <epoch:346, iter: 102,800, lr:2.000e-04> G_loss: 3.428e-02 
22-11-06 11:22:44.169 : <epoch:346, iter: 103,000, lr:2.000e-04> G_loss: 3.984e-02 
22-11-06 11:25:00.810 : <epoch:347, iter: 103,200, lr:2.000e-04> G_loss: 3.562e-02 
22-11-06 11:27:27.536 : <epoch:348, iter: 103,400, lr:2.000e-04> G_loss: 3.154e-02 
22-11-06 11:29:44.571 : <epoch:348, iter: 103,600, lr:2.000e-04> G_loss: 3.113e-02 
22-11-06 11:32:17.740 : <epoch:349, iter: 103,800, lr:2.000e-04> G_loss: 3.580e-02 
22-11-06 11:34:46.786 : <epoch:350, iter: 104,000, lr:2.000e-04> G_loss: 3.510e-02 
22-11-06 11:37:02.851 : <epoch:350, iter: 104,200, lr:2.000e-04> G_loss: 2.839e-02 
22-11-06 11:39:23.936 : <epoch:351, iter: 104,400, lr:2.000e-04> G_loss: 3.918e-02 
22-11-06 11:41:46.663 : <epoch:352, iter: 104,600, lr:2.000e-04> G_loss: 2.649e-02 
22-11-06 11:44:06.301 : <epoch:352, iter: 104,800, lr:2.000e-04> G_loss: 4.062e-02 
22-11-06 11:46:35.087 : <epoch:353, iter: 105,000, lr:2.000e-04> G_loss: 3.030e-02 
22-11-06 11:46:35.089 : Saving the model.
22-11-06 11:46:40.446 : ---1-->   0001.jpg | 24.97dB
22-11-06 11:46:47.504 : ---2-->   0002.jpg | 25.76dB
22-11-06 11:46:51.939 : ---3-->   0003.jpg | 29.86dB
22-11-06 11:46:58.799 : ---4-->   0004.jpg | 28.51dB
22-11-06 11:47:06.354 : ---5-->   0005.jpg | 29.14dB
22-11-06 11:47:12.906 : ---6-->   0006.jpg | 32.66dB
22-11-06 11:47:17.421 : ---7-->   0007.jpg | 35.30dB
22-11-06 11:47:22.620 : ---8-->   0008.jpg | 24.23dB
22-11-06 11:47:25.979 : ---9-->   0009.jpg | 23.45dB
22-11-06 11:47:29.334 : --10-->   0010.jpg | 28.25dB
22-11-06 11:47:32.997 : --11-->   0011.jpg | 26.05dB
22-11-06 11:47:35.681 : --12-->   0012.jpg | 24.18dB
22-11-06 11:47:40.276 : --13-->   0013.jpg | 24.09dB
22-11-06 11:47:44.675 : --14-->   0014.jpg | 23.46dB
22-11-06 11:47:47.876 : --15-->   0015.jpg | 21.68dB
22-11-06 11:47:53.769 : --16-->   0016.jpg | 29.99dB
22-11-06 11:47:56.847 : --17-->   0017.jpg | 29.46dB
22-11-06 11:48:01.994 : --18-->   0018.jpg | 27.16dB
22-11-06 11:48:04.102 : --19-->   0019.jpg | 32.37dB
22-11-06 11:48:07.495 : --20-->   0020.jpg | 23.90dB
22-11-06 11:48:17.023 : --21-->   0021.jpg | 26.54dB
22-11-06 11:48:26.021 : --22-->   0022.jpg | 38.86dB
22-11-06 11:48:27.833 : --23-->   0023.jpg | 25.37dB
22-11-06 11:48:29.837 : --24-->   0024.jpg | 34.28dB
22-11-06 11:48:38.720 : --25-->   0025.jpg | 26.08dB
22-11-06 11:48:44.308 : --26-->   0026.jpg | 27.05dB
22-11-06 11:48:49.316 : --27-->   0027.jpg | 29.04dB
22-11-06 11:48:55.454 : --28-->   0028.jpg | 30.33dB
22-11-06 11:49:00.579 : --29-->   0029.jpg | 24.21dB
22-11-06 11:49:07.924 : --30-->   0030.jpg | 29.30dB
22-11-06 11:49:13.498 : --31-->   0031.jpg | 28.17dB
22-11-06 11:49:18.800 : --32-->   0032.jpg | 26.22dB
22-11-06 11:49:24.009 : --33-->   0033.jpg | 23.62dB
22-11-06 11:49:26.763 : --34-->   0034.jpg | 23.42dB
22-11-06 11:49:31.741 : --35-->   0035.jpg | 26.89dB
22-11-06 11:49:39.684 : --36-->   0036.jpg | 23.25dB
22-11-06 11:49:45.294 : --37-->   0037.jpg | 22.99dB
22-11-06 11:49:50.028 : --38-->   0038.jpg | 26.78dB
22-11-06 11:49:52.455 : --39-->   0039.jpg | 24.20dB
22-11-06 11:49:54.506 : --40-->   0040.jpg | 28.24dB
22-11-06 11:50:00.473 : --41-->   0041.jpg | 21.23dB
22-11-06 11:50:06.108 : --42-->   0042.jpg | 26.72dB
22-11-06 11:50:14.934 : --43-->   0043.jpg | 27.36dB
22-11-06 11:50:21.080 : --44-->   0044.jpg | 24.68dB
22-11-06 11:50:25.273 : --45-->   0045.jpg | 21.60dB
22-11-06 11:50:30.200 : --46-->   0046.jpg | 23.32dB
22-11-06 11:50:36.182 : --47-->   0047.jpg | 24.40dB
22-11-06 11:50:37.836 : --48-->   0048.jpg | 28.44dB
22-11-06 11:50:42.212 : --49-->   0049.jpg | 28.39dB
22-11-06 11:50:47.947 : --50-->   0050.jpg | 21.18dB
22-11-06 11:50:54.950 : --51-->   0051.jpg | 25.95dB
22-11-06 11:50:58.700 : --52-->   0052.jpg | 23.61dB
22-11-06 11:51:02.180 : --53-->   0053.jpg | 24.61dB
22-11-06 11:51:03.579 : --54-->   0054.jpg | 22.11dB
22-11-06 11:51:10.502 : --55-->   0055.jpg | 36.15dB
22-11-06 11:51:13.033 : --56-->   0056.jpg | 24.74dB
22-11-06 11:51:16.692 : --57-->   0057.jpg | 30.50dB
22-11-06 11:51:24.323 : --58-->   0058.jpg | 26.69dB
22-11-06 11:51:29.948 : --59-->   0059.jpg | 27.92dB
22-11-06 11:51:37.421 : --60-->   0060.jpg | 24.93dB
22-11-06 11:51:42.922 : --61-->   0061.jpg | 26.43dB
22-11-06 11:51:46.435 : --62-->   0062.jpg | 24.41dB
22-11-06 11:51:50.257 : --63-->   0063.jpg | 23.19dB
22-11-06 11:51:52.175 : --64-->   0064.jpg | 29.20dB
22-11-06 11:51:57.671 : --65-->   0065.jpg | 35.33dB
22-11-06 11:52:00.225 : --66-->   0066.jpg | 30.35dB
22-11-06 11:52:05.945 : --67-->   0067.jpg | 26.74dB
22-11-06 11:52:11.862 : --68-->   0068.jpg | 25.22dB
22-11-06 11:52:13.978 : --69-->   0069.jpg | 19.45dB
22-11-06 11:52:16.292 : --70-->   0070.jpg | 21.17dB
22-11-06 11:52:21.836 : --71-->   0071.jpg | 22.58dB
22-11-06 11:52:23.754 : --72-->   0072.jpg | 28.23dB
22-11-06 11:52:30.443 : --73-->   0073.jpg | 27.45dB
22-11-06 11:52:33.096 : --74-->   0074.jpg | 24.59dB
22-11-06 11:52:39.198 : --75-->   0075.jpg | 24.60dB
22-11-06 11:52:45.783 : --76-->   0076.jpg | 25.29dB
22-11-06 11:52:53.059 : --77-->   0077.jpg | 22.57dB
22-11-06 11:52:58.634 : --78-->   0078.jpg | 27.94dB
22-11-06 11:53:06.661 : --79-->   0079.jpg | 20.46dB
22-11-06 11:53:12.305 : --80-->   0080.jpg | 30.05dB
22-11-06 11:53:14.448 : --81-->   0081.jpg | 28.61dB
22-11-06 11:53:16.735 : --82-->   0082.jpg | 28.42dB
22-11-06 11:53:21.983 : --83-->   0083.jpg | 23.11dB
22-11-06 11:53:29.893 : --84-->   0084.jpg | 29.01dB
22-11-06 11:53:34.463 : --85-->   0085.jpg | 20.93dB
22-11-06 11:53:39.199 : --86-->   0086.jpg | 24.43dB
22-11-06 11:53:42.507 : --87-->   0087.jpg | 20.87dB
22-11-06 11:53:50.705 : --88-->   0088.jpg | 27.10dB
22-11-06 11:53:56.209 : --89-->   0089.jpg | 23.83dB
22-11-06 11:54:05.267 : --90-->   0090.jpg | 21.80dB
22-11-06 11:54:10.354 : --91-->   0091.jpg | 31.07dB
22-11-06 11:54:11.664 : --92-->   0092.jpg | 22.45dB
22-11-06 11:54:15.221 : --93-->   0093.jpg | 27.59dB
22-11-06 11:54:23.215 : --94-->   0094.jpg | 30.45dB
22-11-06 11:54:25.756 : --95-->   0095.jpg | 25.20dB
22-11-06 11:54:31.920 : --96-->   0096.jpg | 20.90dB
22-11-06 11:54:39.661 : --97-->   0097.jpg | 32.59dB
22-11-06 11:54:42.936 : --98-->   0098.jpg | 25.50dB
22-11-06 11:54:49.836 : --99-->   0099.jpg | 24.94dB
22-11-06 11:54:53.727 : -100-->   0100.jpg | 23.55dB
22-11-06 11:54:54.010 : <epoch:353, iter: 105,000, Average PSNR : 26.31dB

22-11-06 11:57:22.031 : <epoch:354, iter: 105,200, lr:2.000e-04> G_loss: 3.006e-02 
22-11-06 11:59:42.260 : <epoch:354, iter: 105,400, lr:2.000e-04> G_loss: 3.366e-02 
22-11-06 12:01:59.814 : <epoch:355, iter: 105,600, lr:2.000e-04> G_loss: 3.897e-02 
22-11-06 12:04:23.109 : <epoch:356, iter: 105,800, lr:2.000e-04> G_loss: 2.986e-02 
22-11-06 12:06:44.347 : <epoch:356, iter: 106,000, lr:2.000e-04> G_loss: 3.575e-02 
22-11-06 12:09:09.749 : <epoch:357, iter: 106,200, lr:2.000e-04> G_loss: 3.537e-02 
22-11-06 12:11:42.395 : <epoch:358, iter: 106,400, lr:2.000e-04> G_loss: 2.920e-02 
22-11-06 12:14:06.877 : <epoch:358, iter: 106,600, lr:2.000e-04> G_loss: 3.594e-02 
22-11-06 12:16:34.061 : <epoch:359, iter: 106,800, lr:2.000e-04> G_loss: 3.478e-02 
22-11-06 12:19:00.831 : <epoch:360, iter: 107,000, lr:2.000e-04> G_loss: 3.759e-02 
22-11-06 12:21:17.566 : <epoch:360, iter: 107,200, lr:2.000e-04> G_loss: 2.749e-02 
22-11-06 12:23:41.521 : <epoch:361, iter: 107,400, lr:2.000e-04> G_loss: 3.634e-02 
22-11-06 12:26:02.534 : <epoch:362, iter: 107,600, lr:2.000e-04> G_loss: 2.991e-02 
22-11-06 12:28:13.460 : <epoch:362, iter: 107,800, lr:2.000e-04> G_loss: 3.205e-02 
22-11-06 12:30:42.860 : <epoch:363, iter: 108,000, lr:2.000e-04> G_loss: 2.904e-02 
22-11-06 12:33:08.629 : <epoch:364, iter: 108,200, lr:2.000e-04> G_loss: 2.520e-02 
22-11-06 12:35:18.055 : <epoch:364, iter: 108,400, lr:2.000e-04> G_loss: 3.514e-02 
22-11-06 12:37:45.048 : <epoch:365, iter: 108,600, lr:2.000e-04> G_loss: 3.542e-02 
22-11-06 12:40:04.421 : <epoch:366, iter: 108,800, lr:2.000e-04> G_loss: 3.730e-02 
22-11-06 12:42:21.137 : <epoch:367, iter: 109,000, lr:2.000e-04> G_loss: 3.137e-02 
22-11-06 12:44:30.167 : <epoch:367, iter: 109,200, lr:2.000e-04> G_loss: 3.234e-02 
22-11-06 12:46:51.883 : <epoch:368, iter: 109,400, lr:2.000e-04> G_loss: 3.933e-02 
22-11-06 12:49:12.966 : <epoch:369, iter: 109,600, lr:2.000e-04> G_loss: 3.818e-02 
22-11-06 12:51:33.720 : <epoch:369, iter: 109,800, lr:2.000e-04> G_loss: 3.243e-02 
22-11-06 12:54:03.077 : <epoch:370, iter: 110,000, lr:2.000e-04> G_loss: 3.674e-02 
22-11-06 12:54:03.079 : Saving the model.
22-11-06 12:54:07.196 : ---1-->   0001.jpg | 24.86dB
22-11-06 12:54:13.505 : ---2-->   0002.jpg | 25.72dB
22-11-06 12:54:21.129 : ---3-->   0003.jpg | 29.61dB
22-11-06 12:54:29.059 : ---4-->   0004.jpg | 28.35dB
22-11-06 12:54:37.662 : ---5-->   0005.jpg | 29.15dB
22-11-06 12:54:42.523 : ---6-->   0006.jpg | 33.54dB
22-11-06 12:54:46.329 : ---7-->   0007.jpg | 35.49dB
22-11-06 12:54:51.780 : ---8-->   0008.jpg | 24.24dB
22-11-06 12:54:58.270 : ---9-->   0009.jpg | 23.47dB
22-11-06 12:55:01.668 : --10-->   0010.jpg | 28.21dB
22-11-06 12:55:07.806 : --11-->   0011.jpg | 25.98dB
22-11-06 12:55:14.429 : --12-->   0012.jpg | 24.22dB
22-11-06 12:55:16.529 : --13-->   0013.jpg | 24.10dB
22-11-06 12:55:19.662 : --14-->   0014.jpg | 23.42dB
22-11-06 12:55:23.533 : --15-->   0015.jpg | 21.77dB
22-11-06 12:55:26.668 : --16-->   0016.jpg | 30.10dB
22-11-06 12:55:28.613 : --17-->   0017.jpg | 29.35dB
22-11-06 12:55:34.528 : --18-->   0018.jpg | 26.90dB
22-11-06 12:55:36.042 : --19-->   0019.jpg | 32.55dB
22-11-06 12:55:38.863 : --20-->   0020.jpg | 23.98dB
22-11-06 12:55:41.206 : --21-->   0021.jpg | 26.65dB
22-11-06 12:55:44.077 : --22-->   0022.jpg | 38.54dB
22-11-06 12:55:49.484 : --23-->   0023.jpg | 25.51dB
22-11-06 12:55:53.682 : --24-->   0024.jpg | 34.44dB
22-11-06 12:55:55.712 : --25-->   0025.jpg | 26.10dB
22-11-06 12:55:57.716 : --26-->   0026.jpg | 27.03dB
22-11-06 12:56:06.455 : --27-->   0027.jpg | 29.19dB
22-11-06 12:56:09.767 : --28-->   0028.jpg | 30.30dB
22-11-06 12:56:16.885 : --29-->   0029.jpg | 24.19dB
22-11-06 12:56:20.555 : --30-->   0030.jpg | 29.22dB
22-11-06 12:56:24.383 : --31-->   0031.jpg | 28.15dB
22-11-06 12:56:29.246 : --32-->   0032.jpg | 26.18dB
22-11-06 12:56:36.919 : --33-->   0033.jpg | 23.49dB
22-11-06 12:56:42.130 : --34-->   0034.jpg | 23.39dB
22-11-06 12:56:45.444 : --35-->   0035.jpg | 26.90dB
22-11-06 12:56:49.833 : --36-->   0036.jpg | 23.30dB
22-11-06 12:56:54.399 : --37-->   0037.jpg | 23.03dB
22-11-06 12:56:56.839 : --38-->   0038.jpg | 26.74dB
22-11-06 12:57:00.300 : --39-->   0039.jpg | 24.12dB
22-11-06 12:57:05.212 : --40-->   0040.jpg | 28.23dB
22-11-06 12:57:11.871 : --41-->   0041.jpg | 21.23dB
22-11-06 12:57:17.169 : --42-->   0042.jpg | 26.73dB
22-11-06 12:57:24.623 : --43-->   0043.jpg | 27.52dB
22-11-06 12:57:27.650 : --44-->   0044.jpg | 24.71dB
22-11-06 12:57:29.134 : --45-->   0045.jpg | 21.72dB
22-11-06 12:57:30.953 : --46-->   0046.jpg | 23.34dB
22-11-06 12:57:33.689 : --47-->   0047.jpg | 24.43dB
22-11-06 12:57:38.679 : --48-->   0048.jpg | 28.44dB
22-11-06 12:57:45.521 : --49-->   0049.jpg | 28.49dB
22-11-06 12:57:53.110 : --50-->   0050.jpg | 21.21dB
22-11-06 12:57:59.577 : --51-->   0051.jpg | 25.90dB
22-11-06 12:58:02.304 : --52-->   0052.jpg | 23.54dB
22-11-06 12:58:08.268 : --53-->   0053.jpg | 24.57dB
22-11-06 12:58:14.250 : --54-->   0054.jpg | 22.12dB
22-11-06 12:58:19.199 : --55-->   0055.jpg | 36.72dB
22-11-06 12:58:25.386 : --56-->   0056.jpg | 24.72dB
22-11-06 12:58:30.683 : --57-->   0057.jpg | 30.59dB
22-11-06 12:58:38.237 : --58-->   0058.jpg | 26.81dB
22-11-06 12:58:44.530 : --59-->   0059.jpg | 27.97dB
22-11-06 12:58:51.651 : --60-->   0060.jpg | 24.91dB
22-11-06 12:58:53.986 : --61-->   0061.jpg | 26.19dB
22-11-06 12:58:56.418 : --62-->   0062.jpg | 24.42dB
22-11-06 12:59:01.085 : --63-->   0063.jpg | 23.17dB
22-11-06 12:59:06.312 : --64-->   0064.jpg | 29.06dB
22-11-06 12:59:11.767 : --65-->   0065.jpg | 35.01dB
22-11-06 12:59:19.288 : --66-->   0066.jpg | 30.19dB
22-11-06 12:59:27.379 : --67-->   0067.jpg | 26.72dB
22-11-06 12:59:32.794 : --68-->   0068.jpg | 25.30dB
22-11-06 12:59:37.707 : --69-->   0069.jpg | 19.46dB
22-11-06 12:59:40.466 : --70-->   0070.jpg | 21.18dB
22-11-06 12:59:42.064 : --71-->   0071.jpg | 22.57dB
22-11-06 12:59:43.483 : --72-->   0072.jpg | 28.18dB
22-11-06 12:59:51.241 : --73-->   0073.jpg | 27.41dB
22-11-06 12:59:56.253 : --74-->   0074.jpg | 24.60dB
22-11-06 13:00:00.577 : --75-->   0075.jpg | 24.59dB
22-11-06 13:00:10.715 : --76-->   0076.jpg | 25.21dB
22-11-06 13:00:18.411 : --77-->   0077.jpg | 22.42dB
22-11-06 13:00:23.910 : --78-->   0078.jpg | 27.87dB
22-11-06 13:00:32.593 : --79-->   0079.jpg | 20.46dB
22-11-06 13:00:35.853 : --80-->   0080.jpg | 29.98dB
22-11-06 13:00:39.736 : --81-->   0081.jpg | 28.64dB
22-11-06 13:00:45.133 : --82-->   0082.jpg | 28.48dB
22-11-06 13:00:49.081 : --83-->   0083.jpg | 23.06dB
22-11-06 13:00:57.776 : --84-->   0084.jpg | 29.15dB
22-11-06 13:01:03.241 : --85-->   0085.jpg | 21.05dB
22-11-06 13:01:11.613 : --86-->   0086.jpg | 24.43dB
22-11-06 13:01:17.490 : --87-->   0087.jpg | 20.97dB
22-11-06 13:01:24.531 : --88-->   0088.jpg | 27.15dB
22-11-06 13:01:28.477 : --89-->   0089.jpg | 23.89dB
22-11-06 13:01:34.163 : --90-->   0090.jpg | 21.79dB
22-11-06 13:01:38.246 : --91-->   0091.jpg | 30.98dB
22-11-06 13:01:42.226 : --92-->   0092.jpg | 22.49dB
22-11-06 13:01:44.855 : --93-->   0093.jpg | 27.60dB
22-11-06 13:01:47.675 : --94-->   0094.jpg | 30.60dB
22-11-06 13:01:53.446 : --95-->   0095.jpg | 25.17dB
22-11-06 13:01:58.204 : --96-->   0096.jpg | 20.86dB
22-11-06 13:02:00.113 : --97-->   0097.jpg | 32.49dB
22-11-06 13:02:01.678 : --98-->   0098.jpg | 25.60dB
22-11-06 13:02:02.268 : --99-->   0099.jpg | 24.88dB
22-11-06 13:02:02.760 : -100-->   0100.jpg | 23.47dB
22-11-06 13:02:02.988 : <epoch:370, iter: 110,000, Average PSNR : 26.32dB

22-11-06 13:04:26.392 : <epoch:371, iter: 110,200, lr:2.000e-04> G_loss: 2.771e-02 
22-11-06 13:06:40.865 : <epoch:371, iter: 110,400, lr:2.000e-04> G_loss: 2.888e-02 
22-11-06 13:09:07.061 : <epoch:372, iter: 110,600, lr:2.000e-04> G_loss: 3.774e-02 
22-11-06 13:11:36.468 : <epoch:373, iter: 110,800, lr:2.000e-04> G_loss: 3.796e-02 
22-11-06 13:13:57.882 : <epoch:373, iter: 111,000, lr:2.000e-04> G_loss: 3.519e-02 
22-11-06 13:16:22.007 : <epoch:374, iter: 111,200, lr:2.000e-04> G_loss: 4.575e-02 
22-11-06 13:18:39.926 : <epoch:375, iter: 111,400, lr:2.000e-04> G_loss: 3.187e-02 
22-11-06 13:21:01.709 : <epoch:375, iter: 111,600, lr:2.000e-04> G_loss: 3.400e-02 
22-11-06 13:23:26.764 : <epoch:376, iter: 111,800, lr:2.000e-04> G_loss: 3.473e-02 
22-11-06 13:25:50.726 : <epoch:377, iter: 112,000, lr:2.000e-04> G_loss: 2.777e-02 
22-11-06 13:28:06.993 : <epoch:377, iter: 112,200, lr:2.000e-04> G_loss: 3.103e-02 
22-11-06 13:30:32.558 : <epoch:378, iter: 112,400, lr:2.000e-04> G_loss: 3.369e-02 
22-11-06 13:32:46.249 : <epoch:379, iter: 112,600, lr:2.000e-04> G_loss: 2.843e-02 
22-11-06 13:35:00.873 : <epoch:379, iter: 112,800, lr:2.000e-04> G_loss: 3.532e-02 
22-11-06 13:37:19.116 : <epoch:380, iter: 113,000, lr:2.000e-04> G_loss: 3.492e-02 
22-11-06 13:39:30.685 : <epoch:381, iter: 113,200, lr:2.000e-04> G_loss: 2.654e-02 
22-11-06 13:41:44.471 : <epoch:381, iter: 113,400, lr:2.000e-04> G_loss: 3.132e-02 
22-11-06 13:43:56.320 : <epoch:382, iter: 113,600, lr:2.000e-04> G_loss: 3.179e-02 
22-11-06 13:46:22.252 : <epoch:383, iter: 113,800, lr:2.000e-04> G_loss: 4.509e-02 
22-11-06 13:48:38.385 : <epoch:383, iter: 114,000, lr:2.000e-04> G_loss: 3.251e-02 
22-11-06 13:51:00.563 : <epoch:384, iter: 114,200, lr:2.000e-04> G_loss: 2.898e-02 
22-11-06 13:53:17.327 : <epoch:385, iter: 114,400, lr:2.000e-04> G_loss: 3.117e-02 
22-11-06 13:55:30.290 : <epoch:385, iter: 114,600, lr:2.000e-04> G_loss: 3.816e-02 
22-11-06 13:57:48.292 : <epoch:386, iter: 114,800, lr:2.000e-04> G_loss: 2.624e-02 
22-11-06 14:00:03.834 : <epoch:387, iter: 115,000, lr:2.000e-04> G_loss: 2.838e-02 
22-11-06 14:00:03.842 : Saving the model.
22-11-06 14:00:09.712 : ---1-->   0001.jpg | 25.07dB
22-11-06 14:00:14.369 : ---2-->   0002.jpg | 25.83dB
22-11-06 14:00:20.161 : ---3-->   0003.jpg | 29.81dB
22-11-06 14:00:23.685 : ---4-->   0004.jpg | 28.44dB
22-11-06 14:00:26.023 : ---5-->   0005.jpg | 29.11dB
22-11-06 14:00:33.086 : ---6-->   0006.jpg | 32.83dB
22-11-06 14:00:40.403 : ---7-->   0007.jpg | 35.28dB
22-11-06 14:00:43.869 : ---8-->   0008.jpg | 24.21dB
22-11-06 14:00:51.499 : ---9-->   0009.jpg | 23.47dB
22-11-06 14:01:01.789 : --10-->   0010.jpg | 28.26dB
22-11-06 14:01:06.919 : --11-->   0011.jpg | 25.98dB
22-11-06 14:01:11.492 : --12-->   0012.jpg | 24.21dB
22-11-06 14:01:13.976 : --13-->   0013.jpg | 24.10dB
22-11-06 14:01:16.985 : --14-->   0014.jpg | 23.41dB
22-11-06 14:01:22.835 : --15-->   0015.jpg | 21.52dB
22-11-06 14:01:27.905 : --16-->   0016.jpg | 30.04dB
22-11-06 14:01:33.158 : --17-->   0017.jpg | 29.49dB
22-11-06 14:01:38.310 : --18-->   0018.jpg | 26.83dB
22-11-06 14:01:44.981 : --19-->   0019.jpg | 32.60dB
22-11-06 14:01:50.922 : --20-->   0020.jpg | 23.85dB
22-11-06 14:01:57.580 : --21-->   0021.jpg | 26.66dB
22-11-06 14:02:03.418 : --22-->   0022.jpg | 39.09dB
22-11-06 14:02:07.703 : --23-->   0023.jpg | 25.43dB
22-11-06 14:02:13.340 : --24-->   0024.jpg | 34.55dB
22-11-06 14:02:20.806 : --25-->   0025.jpg | 26.12dB
22-11-06 14:02:25.109 : --26-->   0026.jpg | 27.05dB
22-11-06 14:02:30.102 : --27-->   0027.jpg | 29.08dB
22-11-06 14:02:37.202 : --28-->   0028.jpg | 30.24dB
22-11-06 14:02:40.558 : --29-->   0029.jpg | 24.21dB
22-11-06 14:02:46.310 : --30-->   0030.jpg | 29.29dB
22-11-06 14:02:51.838 : --31-->   0031.jpg | 28.23dB
22-11-06 14:02:56.443 : --32-->   0032.jpg | 26.21dB
22-11-06 14:03:02.279 : --33-->   0033.jpg | 23.71dB
22-11-06 14:03:07.682 : --34-->   0034.jpg | 23.36dB
22-11-06 14:03:11.396 : --35-->   0035.jpg | 26.78dB
22-11-06 14:03:17.301 : --36-->   0036.jpg | 23.27dB
22-11-06 14:03:20.193 : --37-->   0037.jpg | 22.82dB
22-11-06 14:03:26.309 : --38-->   0038.jpg | 26.68dB
22-11-06 14:03:29.672 : --39-->   0039.jpg | 24.29dB
22-11-06 14:03:36.932 : --40-->   0040.jpg | 28.14dB
22-11-06 14:03:43.345 : --41-->   0041.jpg | 21.24dB
22-11-06 14:03:50.184 : --42-->   0042.jpg | 26.79dB
22-11-06 14:03:56.132 : --43-->   0043.jpg | 27.35dB
22-11-06 14:04:02.044 : --44-->   0044.jpg | 24.68dB
22-11-06 14:04:04.982 : --45-->   0045.jpg | 21.53dB
22-11-06 14:04:08.732 : --46-->   0046.jpg | 23.36dB
22-11-06 14:04:13.615 : --47-->   0047.jpg | 24.42dB
22-11-06 14:04:19.227 : --48-->   0048.jpg | 28.42dB
22-11-06 14:04:23.908 : --49-->   0049.jpg | 28.44dB
22-11-06 14:04:30.430 : --50-->   0050.jpg | 21.08dB
22-11-06 14:04:37.455 : --51-->   0051.jpg | 25.96dB
22-11-06 14:04:42.690 : --52-->   0052.jpg | 23.61dB
22-11-06 14:04:48.734 : --53-->   0053.jpg | 24.61dB
22-11-06 14:04:54.197 : --54-->   0054.jpg | 22.10dB
22-11-06 14:05:01.781 : --55-->   0055.jpg | 35.69dB
22-11-06 14:05:06.422 : --56-->   0056.jpg | 24.80dB
22-11-06 14:05:12.309 : --57-->   0057.jpg | 30.61dB
22-11-06 14:05:16.490 : --58-->   0058.jpg | 26.75dB
22-11-06 14:05:19.310 : --59-->   0059.jpg | 28.00dB
22-11-06 14:05:25.331 : --60-->   0060.jpg | 24.99dB
22-11-06 14:05:27.066 : --61-->   0061.jpg | 26.46dB
22-11-06 14:05:33.127 : --62-->   0062.jpg | 24.42dB
22-11-06 14:05:37.505 : --63-->   0063.jpg | 23.20dB
22-11-06 14:05:45.235 : --64-->   0064.jpg | 29.05dB
22-11-06 14:05:52.205 : --65-->   0065.jpg | 35.06dB
22-11-06 14:05:56.628 : --66-->   0066.jpg | 30.18dB
22-11-06 14:06:03.254 : --67-->   0067.jpg | 26.92dB
22-11-06 14:06:05.685 : --68-->   0068.jpg | 25.32dB
22-11-06 14:06:06.818 : --69-->   0069.jpg | 19.45dB
22-11-06 14:06:11.535 : --70-->   0070.jpg | 21.17dB
22-11-06 14:06:16.399 : --71-->   0071.jpg | 22.58dB
22-11-06 14:06:19.232 : --72-->   0072.jpg | 28.29dB
22-11-06 14:06:24.504 : --73-->   0073.jpg | 27.46dB
22-11-06 14:06:27.822 : --74-->   0074.jpg | 24.91dB
22-11-06 14:06:32.625 : --75-->   0075.jpg | 24.55dB
22-11-06 14:06:37.509 : --76-->   0076.jpg | 25.36dB
22-11-06 14:06:42.937 : --77-->   0077.jpg | 22.54dB
22-11-06 14:06:46.923 : --78-->   0078.jpg | 27.92dB
22-11-06 14:06:52.423 : --79-->   0079.jpg | 20.47dB
22-11-06 14:06:56.171 : --80-->   0080.jpg | 30.24dB
22-11-06 14:07:00.978 : --81-->   0081.jpg | 28.68dB
22-11-06 14:07:05.986 : --82-->   0082.jpg | 28.44dB
22-11-06 14:07:08.538 : --83-->   0083.jpg | 23.13dB
22-11-06 14:07:15.021 : --84-->   0084.jpg | 29.10dB
22-11-06 14:07:18.922 : --85-->   0085.jpg | 20.96dB
22-11-06 14:07:24.670 : --86-->   0086.jpg | 24.40dB
22-11-06 14:07:28.131 : --87-->   0087.jpg | 20.89dB
22-11-06 14:07:38.070 : --88-->   0088.jpg | 27.20dB
22-11-06 14:07:46.748 : --89-->   0089.jpg | 23.95dB
22-11-06 14:07:52.642 : --90-->   0090.jpg | 21.80dB
22-11-06 14:07:59.026 : --91-->   0091.jpg | 31.06dB
22-11-06 14:08:04.542 : --92-->   0092.jpg | 22.42dB
22-11-06 14:08:10.945 : --93-->   0093.jpg | 27.68dB
22-11-06 14:08:15.426 : --94-->   0094.jpg | 30.58dB
22-11-06 14:08:19.760 : --95-->   0095.jpg | 25.18dB
22-11-06 14:08:21.991 : --96-->   0096.jpg | 20.83dB
22-11-06 14:08:29.379 : --97-->   0097.jpg | 32.61dB
22-11-06 14:08:34.072 : --98-->   0098.jpg | 25.58dB
22-11-06 14:08:36.698 : --99-->   0099.jpg | 24.94dB
22-11-06 14:08:40.964 : -100-->   0100.jpg | 23.51dB
22-11-06 14:08:41.203 : <epoch:387, iter: 115,000, Average PSNR : 26.32dB

22-11-06 14:10:56.288 : <epoch:387, iter: 115,200, lr:2.000e-04> G_loss: 2.885e-02 
22-11-06 14:13:12.794 : <epoch:388, iter: 115,400, lr:2.000e-04> G_loss: 3.067e-02 
22-11-06 14:15:31.153 : <epoch:389, iter: 115,600, lr:2.000e-04> G_loss: 2.481e-02 
22-11-06 14:17:46.436 : <epoch:389, iter: 115,800, lr:2.000e-04> G_loss: 3.831e-02 
22-11-06 14:20:03.252 : <epoch:390, iter: 116,000, lr:2.000e-04> G_loss: 3.442e-02 
22-11-06 14:22:18.518 : <epoch:391, iter: 116,200, lr:2.000e-04> G_loss: 3.283e-02 
22-11-06 14:24:29.288 : <epoch:391, iter: 116,400, lr:2.000e-04> G_loss: 2.876e-02 
22-11-06 14:26:44.944 : <epoch:392, iter: 116,600, lr:2.000e-04> G_loss: 3.026e-02 
22-11-06 14:29:07.182 : <epoch:393, iter: 116,800, lr:2.000e-04> G_loss: 3.249e-02 
22-11-06 14:31:15.528 : <epoch:393, iter: 117,000, lr:2.000e-04> G_loss: 3.436e-02 
22-11-06 14:33:38.548 : <epoch:394, iter: 117,200, lr:2.000e-04> G_loss: 3.234e-02 
22-11-06 14:36:00.524 : <epoch:395, iter: 117,400, lr:2.000e-04> G_loss: 3.216e-02 
22-11-06 14:38:09.304 : <epoch:395, iter: 117,600, lr:2.000e-04> G_loss: 3.633e-02 
22-11-06 14:40:30.837 : <epoch:396, iter: 117,800, lr:2.000e-04> G_loss: 3.627e-02 
22-11-06 14:42:52.188 : <epoch:397, iter: 118,000, lr:2.000e-04> G_loss: 2.784e-02 
22-11-06 14:44:59.217 : <epoch:397, iter: 118,200, lr:2.000e-04> G_loss: 3.648e-02 
22-11-06 14:47:20.286 : <epoch:398, iter: 118,400, lr:2.000e-04> G_loss: 3.166e-02 
22-11-06 14:49:38.063 : <epoch:399, iter: 118,600, lr:2.000e-04> G_loss: 3.578e-02 
22-11-06 14:51:45.655 : <epoch:399, iter: 118,800, lr:2.000e-04> G_loss: 3.315e-02 
22-11-06 14:54:07.452 : <epoch:400, iter: 119,000, lr:2.000e-04> G_loss: 3.075e-02 
22-11-06 14:56:21.579 : <epoch:401, iter: 119,200, lr:2.000e-04> G_loss: 3.409e-02 
22-11-06 14:58:40.612 : <epoch:402, iter: 119,400, lr:2.000e-04> G_loss: 2.803e-02 
22-11-06 15:00:54.035 : <epoch:402, iter: 119,600, lr:2.000e-04> G_loss: 3.096e-02 
22-11-06 15:03:12.324 : <epoch:403, iter: 119,800, lr:2.000e-04> G_loss: 3.049e-02 
22-11-06 15:05:32.231 : <epoch:404, iter: 120,000, lr:2.000e-04> G_loss: 3.053e-02 
22-11-06 15:05:32.232 : Saving the model.
22-11-06 15:05:39.154 : ---1-->   0001.jpg | 24.87dB
22-11-06 15:05:48.010 : ---2-->   0002.jpg | 25.77dB
22-11-06 15:05:51.900 : ---3-->   0003.jpg | 30.06dB
22-11-06 15:05:55.718 : ---4-->   0004.jpg | 28.46dB
22-11-06 15:05:59.470 : ---5-->   0005.jpg | 29.15dB
22-11-06 15:06:02.086 : ---6-->   0006.jpg | 33.79dB
22-11-06 15:06:07.247 : ---7-->   0007.jpg | 35.50dB
22-11-06 15:06:13.731 : ---8-->   0008.jpg | 24.29dB
22-11-06 15:06:20.249 : ---9-->   0009.jpg | 23.53dB
22-11-06 15:06:26.526 : --10-->   0010.jpg | 28.24dB
22-11-06 15:06:32.341 : --11-->   0011.jpg | 26.02dB
22-11-06 15:06:36.957 : --12-->   0012.jpg | 24.22dB
22-11-06 15:06:39.182 : --13-->   0013.jpg | 24.17dB
22-11-06 15:06:41.223 : --14-->   0014.jpg | 23.47dB
22-11-06 15:06:46.281 : --15-->   0015.jpg | 21.79dB
22-11-06 15:06:48.508 : --16-->   0016.jpg | 30.01dB
22-11-06 15:06:50.260 : --17-->   0017.jpg | 29.45dB
22-11-06 15:06:54.805 : --18-->   0018.jpg | 27.11dB
22-11-06 15:07:04.013 : --19-->   0019.jpg | 32.62dB
22-11-06 15:07:08.011 : --20-->   0020.jpg | 23.83dB
22-11-06 15:07:09.565 : --21-->   0021.jpg | 26.68dB
22-11-06 15:07:14.218 : --22-->   0022.jpg | 38.63dB
22-11-06 15:07:17.609 : --23-->   0023.jpg | 25.54dB
22-11-06 15:07:22.378 : --24-->   0024.jpg | 34.38dB
22-11-06 15:07:23.960 : --25-->   0025.jpg | 26.11dB
22-11-06 15:07:29.309 : --26-->   0026.jpg | 27.11dB
22-11-06 15:07:33.218 : --27-->   0027.jpg | 29.08dB
22-11-06 15:07:38.137 : --28-->   0028.jpg | 30.30dB
22-11-06 15:07:41.218 : --29-->   0029.jpg | 24.23dB
22-11-06 15:07:46.462 : --30-->   0030.jpg | 29.17dB
22-11-06 15:07:51.567 : --31-->   0031.jpg | 28.28dB
22-11-06 15:07:56.291 : --32-->   0032.jpg | 26.10dB
22-11-06 15:07:59.835 : --33-->   0033.jpg | 23.55dB
22-11-06 15:08:04.293 : --34-->   0034.jpg | 23.38dB
22-11-06 15:08:10.340 : --35-->   0035.jpg | 27.13dB
22-11-06 15:08:15.075 : --36-->   0036.jpg | 23.30dB
22-11-06 15:08:18.614 : --37-->   0037.jpg | 23.04dB
22-11-06 15:08:23.765 : --38-->   0038.jpg | 26.67dB
22-11-06 15:08:30.774 : --39-->   0039.jpg | 24.04dB
22-11-06 15:08:32.313 : --40-->   0040.jpg | 28.20dB
22-11-06 15:08:37.941 : --41-->   0041.jpg | 21.21dB
22-11-06 15:08:46.078 : --42-->   0042.jpg | 26.76dB
22-11-06 15:08:51.643 : --43-->   0043.jpg | 27.41dB
22-11-06 15:08:56.839 : --44-->   0044.jpg | 24.70dB
22-11-06 15:09:02.166 : --45-->   0045.jpg | 21.67dB
22-11-06 15:09:06.174 : --46-->   0046.jpg | 23.31dB
22-11-06 15:09:11.136 : --47-->   0047.jpg | 24.37dB
22-11-06 15:09:17.076 : --48-->   0048.jpg | 28.55dB
22-11-06 15:09:23.208 : --49-->   0049.jpg | 28.46dB
22-11-06 15:09:27.954 : --50-->   0050.jpg | 21.25dB
22-11-06 15:09:31.719 : --51-->   0051.jpg | 25.90dB
22-11-06 15:09:33.538 : --52-->   0052.jpg | 23.59dB
22-11-06 15:09:38.774 : --53-->   0053.jpg | 24.56dB
22-11-06 15:09:43.066 : --54-->   0054.jpg | 22.08dB
22-11-06 15:09:48.420 : --55-->   0055.jpg | 36.25dB
22-11-06 15:09:50.433 : --56-->   0056.jpg | 24.85dB
22-11-06 15:09:53.528 : --57-->   0057.jpg | 30.63dB
22-11-06 15:09:59.532 : --58-->   0058.jpg | 26.83dB
22-11-06 15:10:02.030 : --59-->   0059.jpg | 27.97dB
22-11-06 15:10:06.781 : --60-->   0060.jpg | 24.98dB
22-11-06 15:10:08.636 : --61-->   0061.jpg | 26.38dB
22-11-06 15:10:11.163 : --62-->   0062.jpg | 24.47dB
22-11-06 15:10:14.017 : --63-->   0063.jpg | 23.16dB
22-11-06 15:10:19.046 : --64-->   0064.jpg | 29.08dB
22-11-06 15:10:24.951 : --65-->   0065.jpg | 35.06dB
22-11-06 15:10:29.452 : --66-->   0066.jpg | 30.18dB
22-11-06 15:10:39.020 : --67-->   0067.jpg | 26.66dB
22-11-06 15:10:45.348 : --68-->   0068.jpg | 25.31dB
22-11-06 15:10:49.508 : --69-->   0069.jpg | 19.45dB
22-11-06 15:10:53.373 : --70-->   0070.jpg | 21.19dB
22-11-06 15:11:00.276 : --71-->   0071.jpg | 22.56dB
22-11-06 15:11:03.183 : --72-->   0072.jpg | 28.25dB
22-11-06 15:11:09.259 : --73-->   0073.jpg | 27.47dB
22-11-06 15:11:16.157 : --74-->   0074.jpg | 24.43dB
22-11-06 15:11:20.717 : --75-->   0075.jpg | 24.59dB
22-11-06 15:11:29.016 : --76-->   0076.jpg | 25.37dB
22-11-06 15:11:36.755 : --77-->   0077.jpg | 22.44dB
22-11-06 15:11:42.221 : --78-->   0078.jpg | 27.81dB
22-11-06 15:11:43.918 : --79-->   0079.jpg | 20.48dB
22-11-06 15:11:45.619 : --80-->   0080.jpg | 30.15dB
22-11-06 15:11:49.679 : --81-->   0081.jpg | 28.65dB
22-11-06 15:11:53.582 : --82-->   0082.jpg | 28.54dB
22-11-06 15:11:59.189 : --83-->   0083.jpg | 23.13dB
22-11-06 15:12:05.666 : --84-->   0084.jpg | 29.18dB
22-11-06 15:12:12.953 : --85-->   0085.jpg | 21.09dB
22-11-06 15:12:17.436 : --86-->   0086.jpg | 24.47dB
22-11-06 15:12:23.296 : --87-->   0087.jpg | 20.93dB
22-11-06 15:12:29.013 : --88-->   0088.jpg | 27.19dB
22-11-06 15:12:32.662 : --89-->   0089.jpg | 23.93dB
22-11-06 15:12:37.933 : --90-->   0090.jpg | 21.81dB
22-11-06 15:12:39.661 : --91-->   0091.jpg | 31.00dB
22-11-06 15:12:43.382 : --92-->   0092.jpg | 22.43dB
22-11-06 15:12:46.153 : --93-->   0093.jpg | 27.85dB
22-11-06 15:12:51.588 : --94-->   0094.jpg | 30.53dB
22-11-06 15:12:54.788 : --95-->   0095.jpg | 25.21dB
22-11-06 15:13:01.268 : --96-->   0096.jpg | 20.86dB
22-11-06 15:13:06.965 : --97-->   0097.jpg | 32.58dB
22-11-06 15:13:10.019 : --98-->   0098.jpg | 25.50dB
22-11-06 15:13:16.230 : --99-->   0099.jpg | 24.95dB
22-11-06 15:13:19.712 : -100-->   0100.jpg | 23.49dB
22-11-06 15:13:19.961 : <epoch:404, iter: 120,000, Average PSNR : 26.34dB

22-11-06 15:15:32.406 : <epoch:404, iter: 120,200, lr:2.000e-04> G_loss: 3.842e-02 
22-11-06 15:17:53.697 : <epoch:405, iter: 120,400, lr:2.000e-04> G_loss: 3.309e-02 
22-11-06 15:20:11.294 : <epoch:406, iter: 120,600, lr:2.000e-04> G_loss: 3.459e-02 
22-11-06 15:22:21.101 : <epoch:406, iter: 120,800, lr:2.000e-04> G_loss: 3.850e-02 
22-11-06 15:24:39.341 : <epoch:407, iter: 121,000, lr:2.000e-04> G_loss: 4.039e-02 
22-11-06 15:26:50.681 : <epoch:408, iter: 121,200, lr:2.000e-04> G_loss: 2.689e-02 
22-11-06 15:28:58.259 : <epoch:408, iter: 121,400, lr:2.000e-04> G_loss: 3.382e-02 
22-11-06 15:31:20.504 : <epoch:409, iter: 121,600, lr:2.000e-04> G_loss: 3.491e-02 
22-11-06 15:33:37.651 : <epoch:410, iter: 121,800, lr:2.000e-04> G_loss: 3.194e-02 
22-11-06 15:35:50.781 : <epoch:410, iter: 122,000, lr:2.000e-04> G_loss: 3.792e-02 
22-11-06 15:37:57.939 : <epoch:411, iter: 122,200, lr:2.000e-04> G_loss: 2.768e-02 
22-11-06 15:40:11.741 : <epoch:412, iter: 122,400, lr:2.000e-04> G_loss: 3.228e-02 
22-11-06 15:42:21.212 : <epoch:412, iter: 122,600, lr:2.000e-04> G_loss: 2.867e-02 
22-11-06 15:44:35.087 : <epoch:413, iter: 122,800, lr:2.000e-04> G_loss: 2.788e-02 
22-11-06 15:46:54.790 : <epoch:414, iter: 123,000, lr:2.000e-04> G_loss: 3.583e-02 
22-11-06 15:49:05.951 : <epoch:414, iter: 123,200, lr:2.000e-04> G_loss: 2.891e-02 
22-11-06 15:51:18.000 : <epoch:415, iter: 123,400, lr:2.000e-04> G_loss: 3.158e-02 
22-11-06 15:53:30.451 : <epoch:416, iter: 123,600, lr:2.000e-04> G_loss: 2.701e-02 
22-11-06 15:55:34.809 : <epoch:416, iter: 123,800, lr:2.000e-04> G_loss: 2.773e-02 
22-11-06 15:57:52.990 : <epoch:417, iter: 124,000, lr:2.000e-04> G_loss: 2.723e-02 
22-11-06 16:00:07.810 : <epoch:418, iter: 124,200, lr:2.000e-04> G_loss: 2.693e-02 
22-11-06 16:02:25.154 : <epoch:418, iter: 124,400, lr:2.000e-04> G_loss: 3.059e-02 
22-11-06 16:04:39.658 : <epoch:419, iter: 124,600, lr:2.000e-04> G_loss: 3.219e-02 
22-11-06 16:06:53.374 : <epoch:420, iter: 124,800, lr:2.000e-04> G_loss: 3.957e-02 
22-11-06 16:09:06.563 : <epoch:420, iter: 125,000, lr:2.000e-04> G_loss: 3.453e-02 
22-11-06 16:09:06.565 : Saving the model.
22-11-06 16:09:13.893 : ---1-->   0001.jpg | 24.92dB
22-11-06 16:09:20.248 : ---2-->   0002.jpg | 25.69dB
22-11-06 16:09:27.239 : ---3-->   0003.jpg | 29.65dB
22-11-06 16:09:35.335 : ---4-->   0004.jpg | 28.53dB
22-11-06 16:09:38.623 : ---5-->   0005.jpg | 29.14dB
22-11-06 16:09:45.445 : ---6-->   0006.jpg | 32.70dB
22-11-06 16:09:46.898 : ---7-->   0007.jpg | 35.52dB
22-11-06 16:09:53.741 : ---8-->   0008.jpg | 24.26dB
22-11-06 16:10:00.374 : ---9-->   0009.jpg | 23.46dB
22-11-06 16:10:06.068 : --10-->   0010.jpg | 28.28dB
22-11-06 16:10:09.706 : --11-->   0011.jpg | 26.41dB
22-11-06 16:10:11.391 : --12-->   0012.jpg | 24.12dB
22-11-06 16:10:12.771 : --13-->   0013.jpg | 24.16dB
22-11-06 16:10:14.165 : --14-->   0014.jpg | 23.50dB
22-11-06 16:10:21.185 : --15-->   0015.jpg | 21.84dB
22-11-06 16:10:25.978 : --16-->   0016.jpg | 30.06dB
22-11-06 16:10:34.165 : --17-->   0017.jpg | 29.45dB
22-11-06 16:10:40.148 : --18-->   0018.jpg | 27.64dB
22-11-06 16:10:42.650 : --19-->   0019.jpg | 32.36dB
22-11-06 16:10:45.203 : --20-->   0020.jpg | 23.76dB
22-11-06 16:10:48.337 : --21-->   0021.jpg | 26.58dB
22-11-06 16:10:51.000 : --22-->   0022.jpg | 38.52dB
22-11-06 16:10:53.112 : --23-->   0023.jpg | 25.45dB
22-11-06 16:10:56.517 : --24-->   0024.jpg | 34.34dB
22-11-06 16:11:02.077 : --25-->   0025.jpg | 26.01dB
22-11-06 16:11:06.563 : --26-->   0026.jpg | 27.02dB
22-11-06 16:11:10.446 : --27-->   0027.jpg | 29.16dB
22-11-06 16:11:13.832 : --28-->   0028.jpg | 29.34dB
22-11-06 16:11:16.483 : --29-->   0029.jpg | 24.22dB
22-11-06 16:11:17.718 : --30-->   0030.jpg | 29.17dB
22-11-06 16:11:22.672 : --31-->   0031.jpg | 28.27dB
22-11-06 16:11:27.239 : --32-->   0032.jpg | 25.99dB
22-11-06 16:11:33.154 : --33-->   0033.jpg | 23.68dB
22-11-06 16:11:37.728 : --34-->   0034.jpg | 23.32dB
22-11-06 16:11:42.923 : --35-->   0035.jpg | 26.87dB
22-11-06 16:11:48.578 : --36-->   0036.jpg | 23.27dB
22-11-06 16:11:52.099 : --37-->   0037.jpg | 22.98dB
22-11-06 16:11:53.622 : --38-->   0038.jpg | 26.81dB
22-11-06 16:11:58.341 : --39-->   0039.jpg | 24.14dB
22-11-06 16:11:59.721 : --40-->   0040.jpg | 28.23dB
22-11-06 16:12:02.921 : --41-->   0041.jpg | 21.14dB
22-11-06 16:12:08.300 : --42-->   0042.jpg | 26.51dB
22-11-06 16:12:12.329 : --43-->   0043.jpg | 27.25dB
22-11-06 16:12:17.140 : --44-->   0044.jpg | 24.72dB
22-11-06 16:12:18.484 : --45-->   0045.jpg | 21.64dB
22-11-06 16:12:22.366 : --46-->   0046.jpg | 23.30dB
22-11-06 16:12:27.161 : --47-->   0047.jpg | 24.50dB
22-11-06 16:12:33.236 : --48-->   0048.jpg | 28.46dB
22-11-06 16:12:36.157 : --49-->   0049.jpg | 28.41dB
22-11-06 16:12:37.582 : --50-->   0050.jpg | 21.07dB
22-11-06 16:12:42.989 : --51-->   0051.jpg | 26.00dB
22-11-06 16:12:47.036 : --52-->   0052.jpg | 23.58dB
22-11-06 16:12:52.739 : --53-->   0053.jpg | 24.64dB
22-11-06 16:12:57.681 : --54-->   0054.jpg | 22.11dB
22-11-06 16:13:04.469 : --55-->   0055.jpg | 36.16dB
22-11-06 16:13:08.234 : --56-->   0056.jpg | 24.70dB
22-11-06 16:13:11.304 : --57-->   0057.jpg | 30.32dB
22-11-06 16:13:13.800 : --58-->   0058.jpg | 26.82dB
22-11-06 16:13:16.797 : --59-->   0059.jpg | 28.00dB
22-11-06 16:13:20.037 : --60-->   0060.jpg | 24.94dB
22-11-06 16:13:24.322 : --61-->   0061.jpg | 26.19dB
22-11-06 16:13:25.464 : --62-->   0062.jpg | 24.38dB
22-11-06 16:13:31.135 : --63-->   0063.jpg | 23.21dB
22-11-06 16:13:39.308 : --64-->   0064.jpg | 29.05dB
22-11-06 16:13:43.473 : --65-->   0065.jpg | 34.98dB
22-11-06 16:13:46.939 : --66-->   0066.jpg | 30.25dB
22-11-06 16:13:51.746 : --67-->   0067.jpg | 26.72dB
22-11-06 16:13:58.657 : --68-->   0068.jpg | 25.28dB
22-11-06 16:14:02.769 : --69-->   0069.jpg | 19.45dB
22-11-06 16:14:09.584 : --70-->   0070.jpg | 21.13dB
22-11-06 16:14:14.627 : --71-->   0071.jpg | 22.55dB
22-11-06 16:14:17.150 : --72-->   0072.jpg | 28.28dB
22-11-06 16:14:20.987 : --73-->   0073.jpg | 27.45dB
22-11-06 16:14:24.111 : --74-->   0074.jpg | 24.54dB
22-11-06 16:14:30.295 : --75-->   0075.jpg | 24.47dB
22-11-06 16:14:33.393 : --76-->   0076.jpg | 25.28dB
22-11-06 16:14:37.899 : --77-->   0077.jpg | 22.53dB
22-11-06 16:14:41.123 : --78-->   0078.jpg | 27.81dB
22-11-06 16:14:48.058 : --79-->   0079.jpg | 20.46dB
22-11-06 16:14:52.244 : --80-->   0080.jpg | 30.01dB
22-11-06 16:14:58.657 : --81-->   0081.jpg | 28.64dB
22-11-06 16:15:05.167 : --82-->   0082.jpg | 28.46dB
22-11-06 16:15:11.403 : --83-->   0083.jpg | 23.15dB
22-11-06 16:15:13.677 : --84-->   0084.jpg | 28.95dB
22-11-06 16:15:15.841 : --85-->   0085.jpg | 20.88dB
22-11-06 16:15:19.318 : --86-->   0086.jpg | 24.37dB
22-11-06 16:15:24.908 : --87-->   0087.jpg | 20.84dB
22-11-06 16:15:27.267 : --88-->   0088.jpg | 27.14dB
22-11-06 16:15:30.032 : --89-->   0089.jpg | 23.92dB
22-11-06 16:15:37.920 : --90-->   0090.jpg | 21.79dB
22-11-06 16:15:40.822 : --91-->   0091.jpg | 30.99dB
22-11-06 16:15:44.283 : --92-->   0092.jpg | 22.41dB
22-11-06 16:15:49.775 : --93-->   0093.jpg | 27.75dB
22-11-06 16:15:53.883 : --94-->   0094.jpg | 30.51dB
22-11-06 16:15:56.747 : --95-->   0095.jpg | 25.16dB
22-11-06 16:16:02.020 : --96-->   0096.jpg | 20.87dB
22-11-06 16:16:08.334 : --97-->   0097.jpg | 32.56dB
22-11-06 16:16:13.355 : --98-->   0098.jpg | 25.51dB
22-11-06 16:16:16.850 : --99-->   0099.jpg | 24.88dB
22-11-06 16:16:18.860 : -100-->   0100.jpg | 23.50dB
22-11-06 16:16:19.204 : <epoch:420, iter: 125,000, Average PSNR : 26.29dB

22-11-06 16:18:41.648 : <epoch:421, iter: 125,200, lr:2.000e-04> G_loss: 2.972e-02 
22-11-06 16:20:57.426 : <epoch:422, iter: 125,400, lr:2.000e-04> G_loss: 4.045e-02 
22-11-06 16:23:06.686 : <epoch:422, iter: 125,600, lr:2.000e-04> G_loss: 2.684e-02 
22-11-06 16:25:23.212 : <epoch:423, iter: 125,800, lr:2.000e-04> G_loss: 3.577e-02 
22-11-06 16:27:42.460 : <epoch:424, iter: 126,000, lr:2.000e-04> G_loss: 3.332e-02 
22-11-06 16:29:55.668 : <epoch:424, iter: 126,200, lr:2.000e-04> G_loss: 2.903e-02 
22-11-06 16:32:13.125 : <epoch:425, iter: 126,400, lr:2.000e-04> G_loss: 2.853e-02 
22-11-06 16:34:30.026 : <epoch:426, iter: 126,600, lr:2.000e-04> G_loss: 3.600e-02 
22-11-06 16:36:43.079 : <epoch:426, iter: 126,800, lr:2.000e-04> G_loss: 3.335e-02 
22-11-06 16:38:56.975 : <epoch:427, iter: 127,000, lr:2.000e-04> G_loss: 3.274e-02 
22-11-06 16:41:15.587 : <epoch:428, iter: 127,200, lr:2.000e-04> G_loss: 3.543e-02 
22-11-06 16:43:25.874 : <epoch:428, iter: 127,400, lr:2.000e-04> G_loss: 3.125e-02 
22-11-06 16:45:43.714 : <epoch:429, iter: 127,600, lr:2.000e-04> G_loss: 3.106e-02 
22-11-06 16:47:59.078 : <epoch:430, iter: 127,800, lr:2.000e-04> G_loss: 2.936e-02 
22-11-06 16:50:10.329 : <epoch:430, iter: 128,000, lr:2.000e-04> G_loss: 2.907e-02 
22-11-06 16:52:33.352 : <epoch:431, iter: 128,200, lr:2.000e-04> G_loss: 3.098e-02 
22-11-06 16:54:56.051 : <epoch:432, iter: 128,400, lr:2.000e-04> G_loss: 4.035e-02 
22-11-06 16:57:02.405 : <epoch:432, iter: 128,600, lr:2.000e-04> G_loss: 2.661e-02 
22-11-06 16:59:23.066 : <epoch:433, iter: 128,800, lr:2.000e-04> G_loss: 3.236e-02 
22-11-06 17:01:42.977 : <epoch:434, iter: 129,000, lr:2.000e-04> G_loss: 4.277e-02 
22-11-06 17:04:00.582 : <epoch:435, iter: 129,200, lr:2.000e-04> G_loss: 3.254e-02 
22-11-06 17:06:15.135 : <epoch:435, iter: 129,400, lr:2.000e-04> G_loss: 2.997e-02 
22-11-06 17:08:37.899 : <epoch:436, iter: 129,600, lr:2.000e-04> G_loss: 2.866e-02 
22-11-06 17:10:51.409 : <epoch:437, iter: 129,800, lr:2.000e-04> G_loss: 3.840e-02 
22-11-06 17:13:07.724 : <epoch:437, iter: 130,000, lr:2.000e-04> G_loss: 2.892e-02 
22-11-06 17:13:07.789 : Saving the model.
22-11-06 17:13:15.668 : ---1-->   0001.jpg | 25.09dB
22-11-06 17:13:21.366 : ---2-->   0002.jpg | 25.77dB
22-11-06 17:13:25.887 : ---3-->   0003.jpg | 30.08dB
22-11-06 17:13:30.532 : ---4-->   0004.jpg | 28.42dB
22-11-06 17:13:35.441 : ---5-->   0005.jpg | 29.17dB
22-11-06 17:13:41.405 : ---6-->   0006.jpg | 33.66dB
22-11-06 17:13:45.569 : ---7-->   0007.jpg | 35.49dB
22-11-06 17:13:49.713 : ---8-->   0008.jpg | 24.28dB
22-11-06 17:13:54.263 : ---9-->   0009.jpg | 23.58dB
22-11-06 17:14:00.458 : --10-->   0010.jpg | 28.44dB
22-11-06 17:14:05.068 : --11-->   0011.jpg | 25.90dB
22-11-06 17:14:09.816 : --12-->   0012.jpg | 24.05dB
22-11-06 17:14:15.056 : --13-->   0013.jpg | 24.20dB
22-11-06 17:14:22.468 : --14-->   0014.jpg | 23.26dB
22-11-06 17:14:31.460 : --15-->   0015.jpg | 21.76dB
22-11-06 17:14:36.412 : --16-->   0016.jpg | 30.18dB
22-11-06 17:14:39.336 : --17-->   0017.jpg | 29.52dB
22-11-06 17:14:46.657 : --18-->   0018.jpg | 27.26dB
22-11-06 17:14:51.462 : --19-->   0019.jpg | 32.40dB
22-11-06 17:14:56.834 : --20-->   0020.jpg | 23.96dB
22-11-06 17:14:59.838 : --21-->   0021.jpg | 26.64dB
22-11-06 17:15:03.281 : --22-->   0022.jpg | 38.91dB
22-11-06 17:15:08.490 : --23-->   0023.jpg | 25.52dB
22-11-06 17:15:11.071 : --24-->   0024.jpg | 34.23dB
22-11-06 17:15:14.844 : --25-->   0025.jpg | 26.12dB
22-11-06 17:15:17.321 : --26-->   0026.jpg | 27.17dB
22-11-06 17:15:21.650 : --27-->   0027.jpg | 29.06dB
22-11-06 17:15:27.753 : --28-->   0028.jpg | 30.38dB
22-11-06 17:15:33.358 : --29-->   0029.jpg | 24.23dB
22-11-06 17:15:38.950 : --30-->   0030.jpg | 29.29dB
22-11-06 17:15:45.671 : --31-->   0031.jpg | 28.29dB
22-11-06 17:15:51.908 : --32-->   0032.jpg | 26.39dB
22-11-06 17:15:57.756 : --33-->   0033.jpg | 23.81dB
22-11-06 17:16:02.970 : --34-->   0034.jpg | 23.37dB
22-11-06 17:16:08.889 : --35-->   0035.jpg | 26.82dB
22-11-06 17:16:14.161 : --36-->   0036.jpg | 23.30dB
22-11-06 17:16:20.307 : --37-->   0037.jpg | 23.06dB
22-11-06 17:16:24.123 : --38-->   0038.jpg | 26.65dB
22-11-06 17:16:26.824 : --39-->   0039.jpg | 24.23dB
22-11-06 17:16:33.162 : --40-->   0040.jpg | 28.24dB
22-11-06 17:16:38.500 : --41-->   0041.jpg | 21.32dB
22-11-06 17:16:45.604 : --42-->   0042.jpg | 26.73dB
22-11-06 17:16:51.859 : --43-->   0043.jpg | 27.40dB
22-11-06 17:16:55.220 : --44-->   0044.jpg | 24.74dB
22-11-06 17:16:59.874 : --45-->   0045.jpg | 21.70dB
22-11-06 17:17:03.589 : --46-->   0046.jpg | 23.30dB
22-11-06 17:17:09.939 : --47-->   0047.jpg | 24.44dB
22-11-06 17:17:13.772 : --48-->   0048.jpg | 28.16dB
22-11-06 17:17:16.543 : --49-->   0049.jpg | 28.52dB
22-11-06 17:17:21.838 : --50-->   0050.jpg | 21.17dB
22-11-06 17:17:25.333 : --51-->   0051.jpg | 25.98dB
22-11-06 17:17:26.877 : --52-->   0052.jpg | 23.53dB
22-11-06 17:17:28.423 : --53-->   0053.jpg | 24.54dB
22-11-06 17:17:29.914 : --54-->   0054.jpg | 22.12dB
22-11-06 17:17:34.776 : --55-->   0055.jpg | 36.28dB
22-11-06 17:17:41.840 : --56-->   0056.jpg | 24.58dB
22-11-06 17:17:45.694 : --57-->   0057.jpg | 30.75dB
22-11-06 17:17:52.489 : --58-->   0058.jpg | 26.77dB
22-11-06 17:17:56.835 : --59-->   0059.jpg | 28.03dB
22-11-06 17:17:59.995 : --60-->   0060.jpg | 25.08dB
22-11-06 17:18:03.245 : --61-->   0061.jpg | 26.55dB
22-11-06 17:18:10.347 : --62-->   0062.jpg | 24.43dB
22-11-06 17:18:14.928 : --63-->   0063.jpg | 23.22dB
22-11-06 17:18:18.307 : --64-->   0064.jpg | 29.12dB
22-11-06 17:18:22.865 : --65-->   0065.jpg | 35.25dB
22-11-06 17:18:25.790 : --66-->   0066.jpg | 30.19dB
22-11-06 17:18:27.791 : --67-->   0067.jpg | 26.58dB
22-11-06 17:18:30.525 : --68-->   0068.jpg | 25.28dB
22-11-06 17:18:34.522 : --69-->   0069.jpg | 19.46dB
22-11-06 17:18:37.398 : --70-->   0070.jpg | 21.16dB
22-11-06 17:18:41.965 : --71-->   0071.jpg | 22.58dB
22-11-06 17:18:45.056 : --72-->   0072.jpg | 28.14dB
22-11-06 17:18:49.680 : --73-->   0073.jpg | 27.43dB
22-11-06 17:18:53.286 : --74-->   0074.jpg | 24.64dB
22-11-06 17:18:59.214 : --75-->   0075.jpg | 24.62dB
22-11-06 17:19:03.629 : --76-->   0076.jpg | 25.39dB
22-11-06 17:19:09.968 : --77-->   0077.jpg | 22.46dB
22-11-06 17:19:16.736 : --78-->   0078.jpg | 27.95dB
22-11-06 17:19:21.607 : --79-->   0079.jpg | 20.48dB
22-11-06 17:19:23.248 : --80-->   0080.jpg | 30.15dB
22-11-06 17:19:29.838 : --81-->   0081.jpg | 28.66dB
22-11-06 17:19:35.628 : --82-->   0082.jpg | 28.52dB
22-11-06 17:19:39.644 : --83-->   0083.jpg | 23.30dB
22-11-06 17:19:43.749 : --84-->   0084.jpg | 29.13dB
22-11-06 17:19:49.125 : --85-->   0085.jpg | 21.01dB
22-11-06 17:19:53.593 : --86-->   0086.jpg | 24.23dB
22-11-06 17:19:58.618 : --87-->   0087.jpg | 20.98dB
22-11-06 17:20:06.777 : --88-->   0088.jpg | 27.23dB
22-11-06 17:20:07.896 : --89-->   0089.jpg | 23.90dB
22-11-06 17:20:12.530 : --90-->   0090.jpg | 21.80dB
22-11-06 17:20:17.160 : --91-->   0091.jpg | 31.25dB
22-11-06 17:20:22.369 : --92-->   0092.jpg | 22.51dB
22-11-06 17:20:25.927 : --93-->   0093.jpg | 27.76dB
22-11-06 17:20:33.606 : --94-->   0094.jpg | 30.64dB
22-11-06 17:20:39.104 : --95-->   0095.jpg | 25.18dB
22-11-06 17:20:42.811 : --96-->   0096.jpg | 20.87dB
22-11-06 17:20:46.282 : --97-->   0097.jpg | 32.62dB
22-11-06 17:20:49.340 : --98-->   0098.jpg | 25.67dB
22-11-06 17:20:52.291 : --99-->   0099.jpg | 24.95dB
22-11-06 17:21:00.962 : -100-->   0100.jpg | 23.53dB
22-11-06 17:21:01.155 : <epoch:437, iter: 130,000, Average PSNR : 26.36dB

22-11-06 17:23:12.328 : <epoch:438, iter: 130,200, lr:2.000e-04> G_loss: 3.391e-02 
22-11-06 17:25:28.222 : <epoch:439, iter: 130,400, lr:2.000e-04> G_loss: 2.886e-02 
22-11-06 17:27:32.549 : <epoch:439, iter: 130,600, lr:2.000e-04> G_loss: 2.497e-02 
22-11-06 17:29:51.550 : <epoch:440, iter: 130,800, lr:2.000e-04> G_loss: 4.006e-02 
22-11-06 17:32:07.672 : <epoch:441, iter: 131,000, lr:2.000e-04> G_loss: 3.180e-02 
22-11-06 17:34:21.786 : <epoch:441, iter: 131,200, lr:2.000e-04> G_loss: 3.461e-02 
22-11-06 17:36:29.914 : <epoch:442, iter: 131,400, lr:2.000e-04> G_loss: 3.228e-02 
22-11-06 17:38:48.691 : <epoch:443, iter: 131,600, lr:2.000e-04> G_loss: 3.734e-02 
22-11-06 17:40:59.278 : <epoch:443, iter: 131,800, lr:2.000e-04> G_loss: 2.407e-02 
22-11-06 17:43:23.282 : <epoch:444, iter: 132,000, lr:2.000e-04> G_loss: 3.413e-02 
22-11-06 17:45:37.885 : <epoch:445, iter: 132,200, lr:2.000e-04> G_loss: 2.712e-02 
22-11-06 17:47:56.190 : <epoch:445, iter: 132,400, lr:2.000e-04> G_loss: 3.040e-02 
22-11-06 17:50:12.603 : <epoch:446, iter: 132,600, lr:2.000e-04> G_loss: 3.324e-02 
22-11-06 17:52:23.465 : <epoch:447, iter: 132,800, lr:2.000e-04> G_loss: 3.764e-02 
22-11-06 17:54:37.485 : <epoch:447, iter: 133,000, lr:2.000e-04> G_loss: 3.570e-02 
22-11-06 17:56:50.779 : <epoch:448, iter: 133,200, lr:2.000e-04> G_loss: 3.044e-02 
22-11-06 17:59:07.930 : <epoch:449, iter: 133,400, lr:2.000e-04> G_loss: 3.593e-02 
22-11-06 18:01:19.732 : <epoch:449, iter: 133,600, lr:2.000e-04> G_loss: 3.605e-02 
22-11-06 18:03:41.763 : <epoch:450, iter: 133,800, lr:2.000e-04> G_loss: 2.644e-02 
22-11-06 18:05:55.480 : <epoch:451, iter: 134,000, lr:2.000e-04> G_loss: 3.208e-02 
22-11-06 18:08:08.528 : <epoch:451, iter: 134,200, lr:2.000e-04> G_loss: 3.120e-02 
22-11-06 18:10:30.423 : <epoch:452, iter: 134,400, lr:2.000e-04> G_loss: 3.396e-02 
22-11-06 18:12:44.757 : <epoch:453, iter: 134,600, lr:2.000e-04> G_loss: 3.629e-02 
22-11-06 18:14:50.025 : <epoch:453, iter: 134,800, lr:2.000e-04> G_loss: 3.651e-02 
22-11-06 18:17:02.000 : <epoch:454, iter: 135,000, lr:2.000e-04> G_loss: 3.378e-02 
22-11-06 18:17:02.002 : Saving the model.
22-11-06 18:17:06.824 : ---1-->   0001.jpg | 24.97dB
22-11-06 18:17:12.736 : ---2-->   0002.jpg | 25.85dB
22-11-06 18:17:20.215 : ---3-->   0003.jpg | 29.69dB
22-11-06 18:17:25.814 : ---4-->   0004.jpg | 28.32dB
22-11-06 18:17:31.175 : ---5-->   0005.jpg | 29.13dB
22-11-06 18:17:38.123 : ---6-->   0006.jpg | 33.33dB
22-11-06 18:17:39.579 : ---7-->   0007.jpg | 35.59dB
22-11-06 18:17:45.055 : ---8-->   0008.jpg | 24.22dB
22-11-06 18:17:49.197 : ---9-->   0009.jpg | 23.51dB
22-11-06 18:17:52.393 : --10-->   0010.jpg | 28.26dB
22-11-06 18:17:55.086 : --11-->   0011.jpg | 26.35dB
22-11-06 18:18:01.858 : --12-->   0012.jpg | 24.10dB
22-11-06 18:18:04.807 : --13-->   0013.jpg | 24.04dB
22-11-06 18:18:12.402 : --14-->   0014.jpg | 23.44dB
22-11-06 18:18:20.237 : --15-->   0015.jpg | 21.74dB
22-11-06 18:18:25.057 : --16-->   0016.jpg | 30.08dB
22-11-06 18:18:29.524 : --17-->   0017.jpg | 29.52dB
22-11-06 18:18:36.238 : --18-->   0018.jpg | 27.27dB
22-11-06 18:18:45.755 : --19-->   0019.jpg | 32.42dB
22-11-06 18:18:51.597 : --20-->   0020.jpg | 23.97dB
22-11-06 18:18:58.346 : --21-->   0021.jpg | 26.68dB
22-11-06 18:19:04.226 : --22-->   0022.jpg | 38.70dB
22-11-06 18:19:09.287 : --23-->   0023.jpg | 25.53dB
22-11-06 18:19:17.096 : --24-->   0024.jpg | 34.34dB
22-11-06 18:19:19.339 : --25-->   0025.jpg | 26.09dB
22-11-06 18:19:25.105 : --26-->   0026.jpg | 27.04dB
22-11-06 18:19:33.928 : --27-->   0027.jpg | 29.01dB
22-11-06 18:19:37.017 : --28-->   0028.jpg | 30.13dB
22-11-06 18:19:41.105 : --29-->   0029.jpg | 24.21dB
22-11-06 18:19:42.237 : --30-->   0030.jpg | 29.26dB
22-11-06 18:19:44.014 : --31-->   0031.jpg | 28.20dB
22-11-06 18:19:45.792 : --32-->   0032.jpg | 26.19dB
22-11-06 18:19:47.836 : --33-->   0033.jpg | 23.64dB
22-11-06 18:19:55.731 : --34-->   0034.jpg | 23.36dB
22-11-06 18:20:01.416 : --35-->   0035.jpg | 26.96dB
22-11-06 18:20:06.302 : --36-->   0036.jpg | 23.28dB
22-11-06 18:20:10.007 : --37-->   0037.jpg | 23.02dB
22-11-06 18:20:13.246 : --38-->   0038.jpg | 26.81dB
22-11-06 18:20:15.478 : --39-->   0039.jpg | 24.11dB
22-11-06 18:20:20.456 : --40-->   0040.jpg | 28.24dB
22-11-06 18:20:23.274 : --41-->   0041.jpg | 21.22dB
22-11-06 18:20:28.171 : --42-->   0042.jpg | 26.62dB
22-11-06 18:20:31.520 : --43-->   0043.jpg | 27.32dB
22-11-06 18:20:39.370 : --44-->   0044.jpg | 24.72dB
22-11-06 18:20:45.734 : --45-->   0045.jpg | 21.62dB
22-11-06 18:20:52.631 : --46-->   0046.jpg | 23.30dB
22-11-06 18:20:59.800 : --47-->   0047.jpg | 24.42dB
22-11-06 18:21:02.147 : --48-->   0048.jpg | 28.70dB
22-11-06 18:21:07.059 : --49-->   0049.jpg | 28.42dB
22-11-06 18:21:10.253 : --50-->   0050.jpg | 21.20dB
22-11-06 18:21:11.876 : --51-->   0051.jpg | 25.79dB
22-11-06 18:21:13.951 : --52-->   0052.jpg | 23.61dB
22-11-06 18:21:15.715 : --53-->   0053.jpg | 24.64dB
22-11-06 18:21:21.835 : --54-->   0054.jpg | 22.15dB
22-11-06 18:21:28.637 : --55-->   0055.jpg | 36.20dB
22-11-06 18:21:34.950 : --56-->   0056.jpg | 24.71dB
22-11-06 18:21:37.973 : --57-->   0057.jpg | 30.55dB
22-11-06 18:21:43.314 : --58-->   0058.jpg | 26.80dB
22-11-06 18:21:47.859 : --59-->   0059.jpg | 27.92dB
22-11-06 18:21:50.333 : --60-->   0060.jpg | 24.96dB
22-11-06 18:21:58.147 : --61-->   0061.jpg | 26.44dB
22-11-06 18:22:04.070 : --62-->   0062.jpg | 24.40dB
22-11-06 18:22:11.729 : --63-->   0063.jpg | 23.22dB
22-11-06 18:22:15.653 : --64-->   0064.jpg | 29.15dB
22-11-06 18:22:23.543 : --65-->   0065.jpg | 35.23dB
22-11-06 18:22:29.968 : --66-->   0066.jpg | 30.16dB
22-11-06 18:22:36.641 : --67-->   0067.jpg | 26.71dB
22-11-06 18:22:41.940 : --68-->   0068.jpg | 25.29dB
22-11-06 18:22:47.171 : --69-->   0069.jpg | 19.40dB
22-11-06 18:22:52.266 : --70-->   0070.jpg | 21.19dB
22-11-06 18:22:55.628 : --71-->   0071.jpg | 22.55dB
22-11-06 18:22:58.966 : --72-->   0072.jpg | 28.33dB
22-11-06 18:23:01.037 : --73-->   0073.jpg | 27.60dB
22-11-06 18:23:03.544 : --74-->   0074.jpg | 24.46dB
22-11-06 18:23:06.705 : --75-->   0075.jpg | 24.58dB
22-11-06 18:23:11.253 : --76-->   0076.jpg | 25.32dB
22-11-06 18:23:19.118 : --77-->   0077.jpg | 22.54dB
22-11-06 18:23:24.201 : --78-->   0078.jpg | 27.92dB
22-11-06 18:23:29.457 : --79-->   0079.jpg | 20.46dB
22-11-06 18:23:35.038 : --80-->   0080.jpg | 29.99dB
22-11-06 18:23:40.417 : --81-->   0081.jpg | 28.71dB
22-11-06 18:23:45.705 : --82-->   0082.jpg | 28.48dB
22-11-06 18:23:49.161 : --83-->   0083.jpg | 23.15dB
22-11-06 18:23:53.679 : --84-->   0084.jpg | 29.26dB
22-11-06 18:23:57.790 : --85-->   0085.jpg | 20.98dB
22-11-06 18:24:02.398 : --86-->   0086.jpg | 24.41dB
22-11-06 18:24:08.176 : --87-->   0087.jpg | 21.02dB
22-11-06 18:24:12.161 : --88-->   0088.jpg | 27.25dB
22-11-06 18:24:16.106 : --89-->   0089.jpg | 23.85dB
22-11-06 18:24:24.187 : --90-->   0090.jpg | 21.82dB
22-11-06 18:24:26.849 : --91-->   0091.jpg | 31.05dB
22-11-06 18:24:29.496 : --92-->   0092.jpg | 22.47dB
22-11-06 18:24:35.003 : --93-->   0093.jpg | 27.82dB
22-11-06 18:24:42.948 : --94-->   0094.jpg | 30.54dB
22-11-06 18:24:47.779 : --95-->   0095.jpg | 25.18dB
22-11-06 18:24:54.777 : --96-->   0096.jpg | 20.87dB
22-11-06 18:24:59.223 : --97-->   0097.jpg | 32.58dB
22-11-06 18:25:02.135 : --98-->   0098.jpg | 25.57dB
22-11-06 18:25:08.001 : --99-->   0099.jpg | 24.98dB
22-11-06 18:25:10.535 : -100-->   0100.jpg | 23.56dB
22-11-06 18:25:10.774 : <epoch:454, iter: 135,000, Average PSNR : 26.34dB

22-11-06 18:27:30.293 : <epoch:455, iter: 135,200, lr:2.000e-04> G_loss: 3.363e-02 
22-11-06 18:29:41.706 : <epoch:455, iter: 135,400, lr:2.000e-04> G_loss: 3.508e-02 
22-11-06 18:31:54.310 : <epoch:456, iter: 135,600, lr:2.000e-04> G_loss: 2.918e-02 
22-11-06 18:34:08.820 : <epoch:457, iter: 135,800, lr:2.000e-04> G_loss: 2.678e-02 
22-11-06 18:36:19.503 : <epoch:457, iter: 136,000, lr:2.000e-04> G_loss: 3.309e-02 
22-11-06 18:38:30.738 : <epoch:458, iter: 136,200, lr:2.000e-04> G_loss: 3.478e-02 
22-11-06 18:40:41.990 : <epoch:459, iter: 136,400, lr:2.000e-04> G_loss: 3.473e-02 
22-11-06 18:42:57.536 : <epoch:459, iter: 136,600, lr:2.000e-04> G_loss: 2.575e-02 
22-11-06 18:45:17.631 : <epoch:460, iter: 136,800, lr:2.000e-04> G_loss: 3.036e-02 
22-11-06 18:47:27.940 : <epoch:461, iter: 137,000, lr:2.000e-04> G_loss: 3.047e-02 
22-11-06 18:49:36.199 : <epoch:461, iter: 137,200, lr:2.000e-04> G_loss: 3.491e-02 
22-11-06 18:51:54.577 : <epoch:462, iter: 137,400, lr:2.000e-04> G_loss: 3.424e-02 
22-11-06 18:54:12.644 : <epoch:463, iter: 137,600, lr:2.000e-04> G_loss: 3.089e-02 
22-11-06 18:56:20.784 : <epoch:463, iter: 137,800, lr:2.000e-04> G_loss: 3.106e-02 
22-11-06 18:58:42.645 : <epoch:464, iter: 138,000, lr:2.000e-04> G_loss: 3.609e-02 
22-11-06 19:00:51.961 : <epoch:465, iter: 138,200, lr:2.000e-04> G_loss: 2.951e-02 
22-11-06 19:03:04.893 : <epoch:465, iter: 138,400, lr:2.000e-04> G_loss: 2.799e-02 
22-11-06 19:05:20.734 : <epoch:466, iter: 138,600, lr:2.000e-04> G_loss: 3.163e-02 
22-11-06 19:07:40.375 : <epoch:467, iter: 138,800, lr:2.000e-04> G_loss: 4.299e-02 
22-11-06 19:10:04.357 : <epoch:468, iter: 139,000, lr:2.000e-04> G_loss: 3.471e-02 
22-11-06 19:12:11.785 : <epoch:468, iter: 139,200, lr:2.000e-04> G_loss: 3.164e-02 
22-11-06 19:14:18.733 : <epoch:469, iter: 139,400, lr:2.000e-04> G_loss: 3.133e-02 
22-11-06 19:16:43.545 : <epoch:470, iter: 139,600, lr:2.000e-04> G_loss: 2.725e-02 
22-11-06 19:18:50.740 : <epoch:470, iter: 139,800, lr:2.000e-04> G_loss: 2.986e-02 
22-11-06 19:21:08.993 : <epoch:471, iter: 140,000, lr:2.000e-04> G_loss: 2.662e-02 
22-11-06 19:21:08.995 : Saving the model.
22-11-06 19:21:16.548 : ---1-->   0001.jpg | 25.15dB
22-11-06 19:21:19.926 : ---2-->   0002.jpg | 25.82dB
22-11-06 19:21:23.976 : ---3-->   0003.jpg | 29.89dB
22-11-06 19:21:26.176 : ---4-->   0004.jpg | 28.48dB
22-11-06 19:21:27.611 : ---5-->   0005.jpg | 29.22dB
22-11-06 19:21:29.826 : ---6-->   0006.jpg | 33.23dB
22-11-06 19:21:33.393 : ---7-->   0007.jpg | 35.65dB
22-11-06 19:21:36.797 : ---8-->   0008.jpg | 24.29dB
22-11-06 19:21:39.757 : ---9-->   0009.jpg | 23.52dB
22-11-06 19:21:48.246 : --10-->   0010.jpg | 28.40dB
22-11-06 19:21:53.998 : --11-->   0011.jpg | 26.55dB
22-11-06 19:22:00.113 : --12-->   0012.jpg | 24.32dB
22-11-06 19:22:04.039 : --13-->   0013.jpg | 24.15dB
22-11-06 19:22:09.613 : --14-->   0014.jpg | 23.47dB
22-11-06 19:22:16.124 : --15-->   0015.jpg | 21.98dB
22-11-06 19:22:21.172 : --16-->   0016.jpg | 30.18dB
22-11-06 19:22:23.743 : --17-->   0017.jpg | 29.51dB
22-11-06 19:22:30.452 : --18-->   0018.jpg | 27.90dB
22-11-06 19:22:34.148 : --19-->   0019.jpg | 32.60dB
22-11-06 19:22:35.884 : --20-->   0020.jpg | 23.97dB
22-11-06 19:22:41.831 : --21-->   0021.jpg | 26.66dB
22-11-06 19:22:47.842 : --22-->   0022.jpg | 39.09dB
22-11-06 19:22:54.434 : --23-->   0023.jpg | 25.64dB
22-11-06 19:23:03.058 : --24-->   0024.jpg | 34.35dB
22-11-06 19:23:08.104 : --25-->   0025.jpg | 26.11dB
22-11-06 19:23:14.266 : --26-->   0026.jpg | 27.18dB
22-11-06 19:23:22.254 : --27-->   0027.jpg | 29.06dB
22-11-06 19:23:25.612 : --28-->   0028.jpg | 30.20dB
22-11-06 19:23:27.940 : --29-->   0029.jpg | 24.25dB
22-11-06 19:23:31.068 : --30-->   0030.jpg | 29.35dB
22-11-06 19:23:36.106 : --31-->   0031.jpg | 28.30dB
22-11-06 19:23:41.300 : --32-->   0032.jpg | 26.19dB
22-11-06 19:23:47.390 : --33-->   0033.jpg | 23.82dB
22-11-06 19:23:53.761 : --34-->   0034.jpg | 23.42dB
22-11-06 19:24:00.200 : --35-->   0035.jpg | 26.91dB
22-11-06 19:24:06.740 : --36-->   0036.jpg | 23.33dB
22-11-06 19:24:10.934 : --37-->   0037.jpg | 23.03dB
22-11-06 19:24:16.429 : --38-->   0038.jpg | 26.71dB
22-11-06 19:24:23.089 : --39-->   0039.jpg | 24.24dB
22-11-06 19:24:29.208 : --40-->   0040.jpg | 28.25dB
22-11-06 19:24:36.909 : --41-->   0041.jpg | 21.34dB
22-11-06 19:24:43.594 : --42-->   0042.jpg | 26.76dB
22-11-06 19:24:51.332 : --43-->   0043.jpg | 27.45dB
22-11-06 19:25:01.635 : --44-->   0044.jpg | 24.72dB
22-11-06 19:25:05.702 : --45-->   0045.jpg | 21.64dB
22-11-06 19:25:10.780 : --46-->   0046.jpg | 23.38dB
22-11-06 19:25:15.190 : --47-->   0047.jpg | 24.52dB
22-11-06 19:25:19.261 : --48-->   0048.jpg | 28.61dB
22-11-06 19:25:24.133 : --49-->   0049.jpg | 28.58dB
22-11-06 19:25:30.004 : --50-->   0050.jpg | 21.23dB
22-11-06 19:25:36.109 : --51-->   0051.jpg | 26.09dB
22-11-06 19:25:41.514 : --52-->   0052.jpg | 23.66dB
22-11-06 19:25:44.741 : --53-->   0053.jpg | 24.70dB
22-11-06 19:25:46.038 : --54-->   0054.jpg | 22.10dB
22-11-06 19:25:52.494 : --55-->   0055.jpg | 36.37dB
22-11-06 19:25:59.518 : --56-->   0056.jpg | 24.61dB
22-11-06 19:26:02.812 : --57-->   0057.jpg | 30.60dB
22-11-06 19:26:11.081 : --58-->   0058.jpg | 26.88dB
22-11-06 19:26:14.466 : --59-->   0059.jpg | 28.03dB
22-11-06 19:26:19.333 : --60-->   0060.jpg | 24.99dB
22-11-06 19:26:25.671 : --61-->   0061.jpg | 26.46dB
22-11-06 19:26:30.715 : --62-->   0062.jpg | 24.51dB
22-11-06 19:26:36.033 : --63-->   0063.jpg | 23.24dB
22-11-06 19:26:42.549 : --64-->   0064.jpg | 29.19dB
22-11-06 19:26:47.504 : --65-->   0065.jpg | 35.11dB
22-11-06 19:26:50.881 : --66-->   0066.jpg | 30.23dB
22-11-06 19:26:58.483 : --67-->   0067.jpg | 26.83dB
22-11-06 19:27:04.288 : --68-->   0068.jpg | 25.29dB
22-11-06 19:27:05.982 : --69-->   0069.jpg | 19.48dB
22-11-06 19:27:08.500 : --70-->   0070.jpg | 21.21dB
22-11-06 19:27:12.970 : --71-->   0071.jpg | 22.60dB
22-11-06 19:27:18.862 : --72-->   0072.jpg | 28.35dB
22-11-06 19:27:27.037 : --73-->   0073.jpg | 27.55dB
22-11-06 19:27:31.614 : --74-->   0074.jpg | 24.55dB
22-11-06 19:27:38.865 : --75-->   0075.jpg | 24.61dB
22-11-06 19:27:44.159 : --76-->   0076.jpg | 25.35dB
22-11-06 19:27:52.016 : --77-->   0077.jpg | 22.53dB
22-11-06 19:27:57.031 : --78-->   0078.jpg | 27.94dB
22-11-06 19:28:00.291 : --79-->   0079.jpg | 20.46dB
22-11-06 19:28:02.105 : --80-->   0080.jpg | 30.23dB
22-11-06 19:28:05.871 : --81-->   0081.jpg | 28.70dB
22-11-06 19:28:13.408 : --82-->   0082.jpg | 28.56dB
22-11-06 19:28:17.375 : --83-->   0083.jpg | 23.23dB
22-11-06 19:28:25.244 : --84-->   0084.jpg | 29.12dB
22-11-06 19:28:31.666 : --85-->   0085.jpg | 21.03dB
22-11-06 19:28:33.377 : --86-->   0086.jpg | 24.40dB
22-11-06 19:28:34.814 : --87-->   0087.jpg | 20.93dB
22-11-06 19:28:36.715 : --88-->   0088.jpg | 27.23dB
22-11-06 19:28:40.017 : --89-->   0089.jpg | 23.95dB
22-11-06 19:28:46.301 : --90-->   0090.jpg | 21.84dB
22-11-06 19:28:48.241 : --91-->   0091.jpg | 31.18dB
22-11-06 19:28:51.273 : --92-->   0092.jpg | 22.52dB
22-11-06 19:28:55.116 : --93-->   0093.jpg | 27.81dB
22-11-06 19:29:03.329 : --94-->   0094.jpg | 30.64dB
22-11-06 19:29:08.246 : --95-->   0095.jpg | 25.20dB
22-11-06 19:29:14.207 : --96-->   0096.jpg | 20.87dB
22-11-06 19:29:18.422 : --97-->   0097.jpg | 32.59dB
22-11-06 19:29:22.644 : --98-->   0098.jpg | 25.58dB
22-11-06 19:29:26.627 : --99-->   0099.jpg | 25.02dB
22-11-06 19:29:28.018 : -100-->   0100.jpg | 23.53dB
22-11-06 19:29:28.244 : <epoch:471, iter: 140,000, Average PSNR : 26.40dB

22-11-06 19:31:48.119 : <epoch:472, iter: 140,200, lr:2.000e-04> G_loss: 2.832e-02 
22-11-06 19:34:04.750 : <epoch:472, iter: 140,400, lr:2.000e-04> G_loss: 3.274e-02 
22-11-06 19:36:27.489 : <epoch:473, iter: 140,600, lr:2.000e-04> G_loss: 2.990e-02 
22-11-06 19:38:42.266 : <epoch:474, iter: 140,800, lr:2.000e-04> G_loss: 3.351e-02 
22-11-06 19:40:55.142 : <epoch:474, iter: 141,000, lr:2.000e-04> G_loss: 3.428e-02 
22-11-06 19:43:12.400 : <epoch:475, iter: 141,200, lr:2.000e-04> G_loss: 3.079e-02 
22-11-06 19:45:24.318 : <epoch:476, iter: 141,400, lr:2.000e-04> G_loss: 3.034e-02 
22-11-06 19:47:37.218 : <epoch:476, iter: 141,600, lr:2.000e-04> G_loss: 3.465e-02 
22-11-06 19:49:50.911 : <epoch:477, iter: 141,800, lr:2.000e-04> G_loss: 3.401e-02 
22-11-06 19:52:08.169 : <epoch:478, iter: 142,000, lr:2.000e-04> G_loss: 2.940e-02 
22-11-06 19:54:24.041 : <epoch:478, iter: 142,200, lr:2.000e-04> G_loss: 2.896e-02 
22-11-06 19:56:39.164 : <epoch:479, iter: 142,400, lr:2.000e-04> G_loss: 3.243e-02 
22-11-06 19:58:54.806 : <epoch:480, iter: 142,600, lr:2.000e-04> G_loss: 3.929e-02 
22-11-06 20:01:05.749 : <epoch:480, iter: 142,800, lr:2.000e-04> G_loss: 3.236e-02 
22-11-06 20:03:21.423 : <epoch:481, iter: 143,000, lr:2.000e-04> G_loss: 3.345e-02 
22-11-06 20:05:41.121 : <epoch:482, iter: 143,200, lr:2.000e-04> G_loss: 3.124e-02 
22-11-06 20:07:57.079 : <epoch:482, iter: 143,400, lr:2.000e-04> G_loss: 2.804e-02 
22-11-06 20:10:14.841 : <epoch:483, iter: 143,600, lr:2.000e-04> G_loss: 3.129e-02 
22-11-06 20:12:24.022 : <epoch:484, iter: 143,800, lr:2.000e-04> G_loss: 3.845e-02 
22-11-06 20:14:41.447 : <epoch:484, iter: 144,000, lr:2.000e-04> G_loss: 3.269e-02 
22-11-06 20:17:00.522 : <epoch:485, iter: 144,200, lr:2.000e-04> G_loss: 3.702e-02 
22-11-06 20:19:23.577 : <epoch:486, iter: 144,400, lr:2.000e-04> G_loss: 3.325e-02 
22-11-06 20:21:35.047 : <epoch:486, iter: 144,600, lr:2.000e-04> G_loss: 4.134e-02 
22-11-06 20:23:47.467 : <epoch:487, iter: 144,800, lr:2.000e-04> G_loss: 2.947e-02 
22-11-06 20:26:02.689 : <epoch:488, iter: 145,000, lr:2.000e-04> G_loss: 3.436e-02 
22-11-06 20:26:02.691 : Saving the model.
22-11-06 20:26:11.487 : ---1-->   0001.jpg | 25.09dB
22-11-06 20:26:14.628 : ---2-->   0002.jpg | 25.85dB
22-11-06 20:26:20.360 : ---3-->   0003.jpg | 29.74dB
22-11-06 20:26:26.289 : ---4-->   0004.jpg | 28.37dB
22-11-06 20:26:30.272 : ---5-->   0005.jpg | 29.31dB
22-11-06 20:26:37.658 : ---6-->   0006.jpg | 33.51dB
22-11-06 20:26:39.631 : ---7-->   0007.jpg | 35.47dB
22-11-06 20:26:45.106 : ---8-->   0008.jpg | 24.31dB
22-11-06 20:26:53.802 : ---9-->   0009.jpg | 23.59dB
22-11-06 20:27:00.139 : --10-->   0010.jpg | 28.37dB
22-11-06 20:27:05.132 : --11-->   0011.jpg | 25.68dB
22-11-06 20:27:10.289 : --12-->   0012.jpg | 24.29dB
22-11-06 20:27:16.019 : --13-->   0013.jpg | 24.15dB
22-11-06 20:27:21.528 : --14-->   0014.jpg | 23.56dB
22-11-06 20:27:25.628 : --15-->   0015.jpg | 21.83dB
22-11-06 20:27:29.946 : --16-->   0016.jpg | 29.93dB
22-11-06 20:27:37.524 : --17-->   0017.jpg | 29.53dB
22-11-06 20:27:44.335 : --18-->   0018.jpg | 27.06dB
22-11-06 20:27:49.487 : --19-->   0019.jpg | 32.37dB
22-11-06 20:27:51.726 : --20-->   0020.jpg | 23.89dB
22-11-06 20:27:58.752 : --21-->   0021.jpg | 26.69dB
22-11-06 20:28:02.959 : --22-->   0022.jpg | 38.75dB
22-11-06 20:28:08.834 : --23-->   0023.jpg | 25.62dB
22-11-06 20:28:13.203 : --24-->   0024.jpg | 34.62dB
22-11-06 20:28:16.852 : --25-->   0025.jpg | 26.02dB
22-11-06 20:28:21.561 : --26-->   0026.jpg | 27.15dB
22-11-06 20:28:26.714 : --27-->   0027.jpg | 29.02dB
22-11-06 20:28:27.860 : --28-->   0028.jpg | 30.57dB
22-11-06 20:28:28.730 : --29-->   0029.jpg | 24.23dB
22-11-06 20:28:35.520 : --30-->   0030.jpg | 29.28dB
22-11-06 20:28:40.029 : --31-->   0031.jpg | 28.30dB
22-11-06 20:28:44.080 : --32-->   0032.jpg | 26.23dB
22-11-06 20:28:47.642 : --33-->   0033.jpg | 23.78dB
22-11-06 20:28:55.062 : --34-->   0034.jpg | 23.35dB
22-11-06 20:28:58.384 : --35-->   0035.jpg | 26.93dB
22-11-06 20:29:05.019 : --36-->   0036.jpg | 23.28dB
22-11-06 20:29:08.977 : --37-->   0037.jpg | 23.01dB
22-11-06 20:29:11.917 : --38-->   0038.jpg | 26.77dB
22-11-06 20:29:18.831 : --39-->   0039.jpg | 24.26dB
22-11-06 20:29:22.015 : --40-->   0040.jpg | 28.16dB
22-11-06 20:29:28.365 : --41-->   0041.jpg | 21.13dB
22-11-06 20:29:32.908 : --42-->   0042.jpg | 26.68dB
22-11-06 20:29:39.621 : --43-->   0043.jpg | 27.40dB
22-11-06 20:29:41.512 : --44-->   0044.jpg | 24.72dB
22-11-06 20:29:45.790 : --45-->   0045.jpg | 21.66dB
22-11-06 20:29:50.564 : --46-->   0046.jpg | 23.32dB
22-11-06 20:29:58.159 : --47-->   0047.jpg | 24.48dB
22-11-06 20:30:05.919 : --48-->   0048.jpg | 28.70dB
22-11-06 20:30:13.016 : --49-->   0049.jpg | 28.50dB
22-11-06 20:30:17.783 : --50-->   0050.jpg | 21.11dB
22-11-06 20:30:20.011 : --51-->   0051.jpg | 26.11dB
22-11-06 20:30:26.929 : --52-->   0052.jpg | 23.57dB
22-11-06 20:30:28.312 : --53-->   0053.jpg | 24.71dB
22-11-06 20:30:31.510 : --54-->   0054.jpg | 22.13dB
22-11-06 20:30:34.894 : --55-->   0055.jpg | 36.52dB
22-11-06 20:30:36.705 : --56-->   0056.jpg | 24.73dB
22-11-06 20:30:40.684 : --57-->   0057.jpg | 30.60dB
22-11-06 20:30:49.219 : --58-->   0058.jpg | 26.69dB
22-11-06 20:30:52.992 : --59-->   0059.jpg | 28.01dB
22-11-06 20:30:59.308 : --60-->   0060.jpg | 25.03dB
22-11-06 20:31:04.926 : --61-->   0061.jpg | 26.70dB
22-11-06 20:31:11.626 : --62-->   0062.jpg | 24.48dB
22-11-06 20:31:16.366 : --63-->   0063.jpg | 23.26dB
22-11-06 20:31:22.726 : --64-->   0064.jpg | 29.02dB
22-11-06 20:31:32.593 : --65-->   0065.jpg | 35.13dB
22-11-06 20:31:41.130 : --66-->   0066.jpg | 30.26dB
22-11-06 20:31:47.516 : --67-->   0067.jpg | 26.78dB
22-11-06 20:31:58.159 : --68-->   0068.jpg | 25.27dB
22-11-06 20:32:03.283 : --69-->   0069.jpg | 19.46dB
22-11-06 20:32:07.213 : --70-->   0070.jpg | 21.21dB
22-11-06 20:32:10.607 : --71-->   0071.jpg | 22.58dB
22-11-06 20:32:16.406 : --72-->   0072.jpg | 28.18dB
22-11-06 20:32:21.149 : --73-->   0073.jpg | 27.48dB
22-11-06 20:32:26.262 : --74-->   0074.jpg | 24.53dB
22-11-06 20:32:32.827 : --75-->   0075.jpg | 24.59dB
22-11-06 20:32:34.968 : --76-->   0076.jpg | 25.35dB
22-11-06 20:32:38.763 : --77-->   0077.jpg | 22.57dB
22-11-06 20:32:40.806 : --78-->   0078.jpg | 27.96dB
22-11-06 20:32:42.621 : --79-->   0079.jpg | 20.47dB
22-11-06 20:32:43.814 : --80-->   0080.jpg | 30.25dB
22-11-06 20:32:47.352 : --81-->   0081.jpg | 28.67dB
22-11-06 20:32:49.894 : --82-->   0082.jpg | 28.53dB
22-11-06 20:32:55.757 : --83-->   0083.jpg | 23.21dB
22-11-06 20:33:01.650 : --84-->   0084.jpg | 29.12dB
22-11-06 20:33:04.747 : --85-->   0085.jpg | 20.88dB
22-11-06 20:33:06.803 : --86-->   0086.jpg | 24.44dB
22-11-06 20:33:11.262 : --87-->   0087.jpg | 21.01dB
22-11-06 20:33:18.265 : --88-->   0088.jpg | 27.18dB
22-11-06 20:33:25.637 : --89-->   0089.jpg | 23.96dB
22-11-06 20:33:30.068 : --90-->   0090.jpg | 21.81dB
22-11-06 20:33:33.980 : --91-->   0091.jpg | 31.15dB
22-11-06 20:33:38.174 : --92-->   0092.jpg | 22.50dB
22-11-06 20:33:42.934 : --93-->   0093.jpg | 27.80dB
22-11-06 20:33:49.259 : --94-->   0094.jpg | 30.59dB
22-11-06 20:33:52.357 : --95-->   0095.jpg | 25.17dB
22-11-06 20:33:58.375 : --96-->   0096.jpg | 20.93dB
22-11-06 20:34:04.470 : --97-->   0097.jpg | 32.58dB
22-11-06 20:34:08.810 : --98-->   0098.jpg | 25.71dB
22-11-06 20:34:13.888 : --99-->   0099.jpg | 25.04dB
22-11-06 20:34:20.564 : -100-->   0100.jpg | 23.55dB
22-11-06 20:34:20.859 : <epoch:488, iter: 145,000, Average PSNR : 26.37dB

22-11-06 20:36:27.179 : <epoch:488, iter: 145,200, lr:2.000e-04> G_loss: 2.481e-02 
22-11-06 20:38:43.279 : <epoch:489, iter: 145,400, lr:2.000e-04> G_loss: 3.175e-02 
22-11-06 20:41:00.135 : <epoch:490, iter: 145,600, lr:2.000e-04> G_loss: 3.497e-02 
22-11-06 20:43:09.488 : <epoch:490, iter: 145,800, lr:2.000e-04> G_loss: 3.296e-02 
22-11-06 20:45:27.093 : <epoch:491, iter: 146,000, lr:2.000e-04> G_loss: 3.181e-02 
22-11-06 20:47:46.282 : <epoch:492, iter: 146,200, lr:2.000e-04> G_loss: 3.391e-02 
22-11-06 20:50:07.890 : <epoch:492, iter: 146,400, lr:2.000e-04> G_loss: 2.465e-02 
22-11-06 20:52:25.457 : <epoch:493, iter: 146,600, lr:2.000e-04> G_loss: 4.684e-02 
22-11-06 20:54:47.372 : <epoch:494, iter: 146,800, lr:2.000e-04> G_loss: 2.990e-02 
22-11-06 20:57:00.695 : <epoch:494, iter: 147,000, lr:2.000e-04> G_loss: 3.114e-02 
22-11-06 20:59:23.391 : <epoch:495, iter: 147,200, lr:2.000e-04> G_loss: 3.817e-02 
22-11-06 21:01:37.352 : <epoch:496, iter: 147,400, lr:2.000e-04> G_loss: 2.904e-02 
22-11-06 21:03:48.339 : <epoch:496, iter: 147,600, lr:2.000e-04> G_loss: 2.878e-02 
22-11-06 21:06:07.268 : <epoch:497, iter: 147,800, lr:2.000e-04> G_loss: 2.933e-02 
22-11-06 21:08:22.940 : <epoch:498, iter: 148,000, lr:2.000e-04> G_loss: 3.556e-02 
22-11-06 21:10:36.819 : <epoch:498, iter: 148,200, lr:2.000e-04> G_loss: 3.623e-02 
22-11-06 21:13:08.629 : <epoch:499, iter: 148,400, lr:2.000e-04> G_loss: 3.520e-02 
22-11-06 21:15:46.976 : <epoch:500, iter: 148,600, lr:2.000e-04> G_loss: 2.882e-02 
22-11-06 21:18:12.691 : <epoch:501, iter: 148,800, lr:2.000e-04> G_loss: 3.167e-02 
22-11-06 21:20:37.428 : <epoch:501, iter: 149,000, lr:2.000e-04> G_loss: 3.954e-02 
22-11-06 21:23:03.338 : <epoch:502, iter: 149,200, lr:2.000e-04> G_loss: 2.988e-02 
22-11-06 21:25:36.873 : <epoch:503, iter: 149,400, lr:2.000e-04> G_loss: 3.662e-02 
22-11-06 21:28:06.651 : <epoch:503, iter: 149,600, lr:2.000e-04> G_loss: 2.704e-02 
22-11-06 21:30:47.588 : <epoch:504, iter: 149,800, lr:2.000e-04> G_loss: 3.074e-02 
22-11-06 21:33:19.738 : <epoch:505, iter: 150,000, lr:2.000e-04> G_loss: 3.825e-02 
22-11-06 21:33:19.742 : Saving the model.
22-11-06 21:33:26.016 : ---1-->   0001.jpg | 25.11dB
22-11-06 21:33:28.836 : ---2-->   0002.jpg | 25.75dB
22-11-06 21:33:31.003 : ---3-->   0003.jpg | 30.04dB
22-11-06 21:33:34.719 : ---4-->   0004.jpg | 28.45dB
22-11-06 21:33:39.123 : ---5-->   0005.jpg | 29.20dB
22-11-06 21:33:43.753 : ---6-->   0006.jpg | 33.44dB
22-11-06 21:33:49.409 : ---7-->   0007.jpg | 35.74dB
22-11-06 21:33:56.328 : ---8-->   0008.jpg | 24.30dB
22-11-06 21:34:00.691 : ---9-->   0009.jpg | 23.56dB
22-11-06 21:34:06.842 : --10-->   0010.jpg | 28.38dB
22-11-06 21:34:14.487 : --11-->   0011.jpg | 26.68dB
22-11-06 21:34:19.481 : --12-->   0012.jpg | 23.97dB
22-11-06 21:34:24.125 : --13-->   0013.jpg | 24.00dB
22-11-06 21:34:27.433 : --14-->   0014.jpg | 23.49dB
22-11-06 21:34:33.461 : --15-->   0015.jpg | 21.72dB
22-11-06 21:34:35.640 : --16-->   0016.jpg | 30.08dB
22-11-06 21:34:37.512 : --17-->   0017.jpg | 29.48dB
22-11-06 21:34:42.051 : --18-->   0018.jpg | 27.19dB
22-11-06 21:34:45.423 : --19-->   0019.jpg | 32.68dB
22-11-06 21:34:48.673 : --20-->   0020.jpg | 24.16dB
22-11-06 21:34:51.660 : --21-->   0021.jpg | 26.65dB
22-11-06 21:34:54.287 : --22-->   0022.jpg | 39.09dB
22-11-06 21:35:00.850 : --23-->   0023.jpg | 25.61dB
22-11-06 21:35:04.449 : --24-->   0024.jpg | 34.27dB
22-11-06 21:35:06.025 : --25-->   0025.jpg | 26.14dB
22-11-06 21:35:08.950 : --26-->   0026.jpg | 27.13dB
22-11-06 21:35:12.513 : --27-->   0027.jpg | 29.02dB
22-11-06 21:35:14.426 : --28-->   0028.jpg | 30.52dB
22-11-06 21:35:16.349 : --29-->   0029.jpg | 24.22dB
22-11-06 21:35:21.358 : --30-->   0030.jpg | 29.29dB
22-11-06 21:35:27.492 : --31-->   0031.jpg | 28.26dB
22-11-06 21:35:31.240 : --32-->   0032.jpg | 26.33dB
22-11-06 21:35:35.526 : --33-->   0033.jpg | 23.81dB
22-11-06 21:35:38.176 : --34-->   0034.jpg | 23.30dB
22-11-06 21:35:40.021 : --35-->   0035.jpg | 27.07dB
22-11-06 21:35:44.137 : --36-->   0036.jpg | 23.31dB
22-11-06 21:35:46.468 : --37-->   0037.jpg | 23.03dB
22-11-06 21:35:49.359 : --38-->   0038.jpg | 26.65dB
22-11-06 21:35:52.814 : --39-->   0039.jpg | 24.25dB
22-11-06 21:35:57.152 : --40-->   0040.jpg | 28.27dB
22-11-06 21:35:59.090 : --41-->   0041.jpg | 21.31dB
22-11-06 21:36:01.759 : --42-->   0042.jpg | 26.75dB
22-11-06 21:36:09.969 : --43-->   0043.jpg | 27.38dB
22-11-06 21:36:13.703 : --44-->   0044.jpg | 24.70dB
22-11-06 21:36:15.078 : --45-->   0045.jpg | 21.61dB
22-11-06 21:36:16.683 : --46-->   0046.jpg | 23.29dB
22-11-06 21:36:24.451 : --47-->   0047.jpg | 24.37dB
22-11-06 21:36:27.100 : --48-->   0048.jpg | 28.63dB
22-11-06 21:36:28.864 : --49-->   0049.jpg | 28.54dB
22-11-06 21:36:30.389 : --50-->   0050.jpg | 21.17dB
22-11-06 21:36:31.038 : --51-->   0051.jpg | 26.03dB
22-11-06 21:36:35.640 : --52-->   0052.jpg | 23.56dB
22-11-06 21:36:40.535 : --53-->   0053.jpg | 24.62dB
22-11-06 21:36:44.875 : --54-->   0054.jpg | 22.06dB
22-11-06 21:36:51.155 : --55-->   0055.jpg | 36.70dB
22-11-06 21:36:54.759 : --56-->   0056.jpg | 24.80dB
22-11-06 21:36:56.427 : --57-->   0057.jpg | 30.44dB
22-11-06 21:36:58.524 : --58-->   0058.jpg | 26.85dB
22-11-06 21:36:59.787 : --59-->   0059.jpg | 28.03dB
22-11-06 21:37:01.841 : --60-->   0060.jpg | 25.03dB
22-11-06 21:37:03.872 : --61-->   0061.jpg | 26.46dB
22-11-06 21:37:09.381 : --62-->   0062.jpg | 24.47dB
22-11-06 21:37:13.322 : --63-->   0063.jpg | 23.24dB
22-11-06 21:37:18.045 : --64-->   0064.jpg | 29.15dB
22-11-06 21:37:23.045 : --65-->   0065.jpg | 34.96dB
22-11-06 21:37:27.151 : --66-->   0066.jpg | 30.32dB
22-11-06 21:37:29.356 : --67-->   0067.jpg | 26.87dB
22-11-06 21:37:33.950 : --68-->   0068.jpg | 25.34dB
22-11-06 21:37:37.365 : --69-->   0069.jpg | 19.45dB
22-11-06 21:37:40.758 : --70-->   0070.jpg | 21.21dB
22-11-06 21:37:42.979 : --71-->   0071.jpg | 22.58dB
22-11-06 21:37:46.882 : --72-->   0072.jpg | 28.36dB
22-11-06 21:37:51.664 : --73-->   0073.jpg | 27.63dB
22-11-06 21:37:52.555 : --74-->   0074.jpg | 24.47dB
22-11-06 21:37:54.357 : --75-->   0075.jpg | 24.62dB
22-11-06 21:37:57.271 : --76-->   0076.jpg | 25.26dB
22-11-06 21:38:03.813 : --77-->   0077.jpg | 22.46dB
22-11-06 21:38:07.305 : --78-->   0078.jpg | 27.97dB
22-11-06 21:38:11.064 : --79-->   0079.jpg | 20.47dB
22-11-06 21:38:13.547 : --80-->   0080.jpg | 30.29dB
22-11-06 21:38:15.068 : --81-->   0081.jpg | 28.69dB
22-11-06 21:38:18.832 : --82-->   0082.jpg | 28.51dB
22-11-06 21:38:24.323 : --83-->   0083.jpg | 23.30dB
22-11-06 21:38:31.366 : --84-->   0084.jpg | 29.21dB
22-11-06 21:38:35.131 : --85-->   0085.jpg | 21.03dB
22-11-06 21:38:38.869 : --86-->   0086.jpg | 24.42dB
22-11-06 21:38:41.157 : --87-->   0087.jpg | 20.97dB
22-11-06 21:38:45.304 : --88-->   0088.jpg | 27.20dB
22-11-06 21:38:46.715 : --89-->   0089.jpg | 23.97dB
22-11-06 21:38:48.706 : --90-->   0090.jpg | 21.83dB
22-11-06 21:38:50.637 : --91-->   0091.jpg | 31.18dB
22-11-06 21:38:55.150 : --92-->   0092.jpg | 22.61dB
22-11-06 21:38:56.872 : --93-->   0093.jpg | 27.72dB
22-11-06 21:39:00.344 : --94-->   0094.jpg | 30.56dB
22-11-06 21:39:02.796 : --95-->   0095.jpg | 25.18dB
22-11-06 21:39:05.127 : --96-->   0096.jpg | 20.87dB
22-11-06 21:39:07.839 : --97-->   0097.jpg | 32.60dB
22-11-06 21:39:08.744 : --98-->   0098.jpg | 25.60dB
22-11-06 21:39:09.198 : --99-->   0099.jpg | 24.96dB
22-11-06 21:39:09.841 : -100-->   0100.jpg | 23.53dB
22-11-06 21:39:10.137 : <epoch:505, iter: 150,000, Average PSNR : 26.39dB

22-11-06 21:41:38.046 : <epoch:505, iter: 150,200, lr:2.000e-04> G_loss: 3.908e-02 
22-11-06 21:44:01.957 : <epoch:506, iter: 150,400, lr:2.000e-04> G_loss: 2.995e-02 
22-11-06 21:46:29.237 : <epoch:507, iter: 150,600, lr:2.000e-04> G_loss: 3.261e-02 
22-11-06 21:48:40.181 : <epoch:507, iter: 150,800, lr:2.000e-04> G_loss: 3.203e-02 
22-11-06 21:50:49.736 : <epoch:508, iter: 151,000, lr:2.000e-04> G_loss: 3.019e-02 
22-11-06 21:53:01.868 : <epoch:509, iter: 151,200, lr:2.000e-04> G_loss: 3.357e-02 
22-11-06 21:55:12.991 : <epoch:509, iter: 151,400, lr:2.000e-04> G_loss: 3.038e-02 
22-11-06 21:57:16.596 : <epoch:510, iter: 151,600, lr:2.000e-04> G_loss: 2.946e-02 
22-11-06 21:59:21.290 : <epoch:511, iter: 151,800, lr:2.000e-04> G_loss: 3.520e-02 
22-11-06 22:01:30.733 : <epoch:511, iter: 152,000, lr:2.000e-04> G_loss: 3.014e-02 
22-11-06 22:03:38.831 : <epoch:512, iter: 152,200, lr:2.000e-04> G_loss: 2.433e-02 
22-11-06 22:05:39.712 : <epoch:513, iter: 152,400, lr:2.000e-04> G_loss: 3.500e-02 
22-11-06 22:07:43.690 : <epoch:513, iter: 152,600, lr:2.000e-04> G_loss: 3.256e-02 
22-11-06 22:09:57.806 : <epoch:514, iter: 152,800, lr:2.000e-04> G_loss: 2.279e-02 
22-11-06 22:12:01.209 : <epoch:515, iter: 153,000, lr:2.000e-04> G_loss: 2.760e-02 
22-11-06 22:14:06.538 : <epoch:515, iter: 153,200, lr:2.000e-04> G_loss: 3.615e-02 
22-11-06 22:16:11.218 : <epoch:516, iter: 153,400, lr:2.000e-04> G_loss: 3.133e-02 
22-11-06 22:18:25.904 : <epoch:517, iter: 153,600, lr:2.000e-04> G_loss: 2.496e-02 
22-11-06 22:20:35.161 : <epoch:517, iter: 153,800, lr:2.000e-04> G_loss: 3.257e-02 
22-11-06 22:22:52.436 : <epoch:518, iter: 154,000, lr:2.000e-04> G_loss: 3.699e-02 
22-11-06 22:24:57.338 : <epoch:519, iter: 154,200, lr:2.000e-04> G_loss: 3.290e-02 
22-11-06 22:27:03.699 : <epoch:519, iter: 154,400, lr:2.000e-04> G_loss: 3.111e-02 
22-11-06 22:29:00.769 : <epoch:520, iter: 154,600, lr:2.000e-04> G_loss: 2.747e-02 
22-11-06 22:31:05.027 : <epoch:521, iter: 154,800, lr:2.000e-04> G_loss: 3.307e-02 
22-11-06 22:33:08.910 : <epoch:521, iter: 155,000, lr:2.000e-04> G_loss: 4.188e-02 
22-11-06 22:33:08.911 : Saving the model.
22-11-06 22:33:14.298 : ---1-->   0001.jpg | 25.07dB
22-11-06 22:33:17.662 : ---2-->   0002.jpg | 25.80dB
22-11-06 22:33:22.151 : ---3-->   0003.jpg | 29.85dB
22-11-06 22:33:25.229 : ---4-->   0004.jpg | 28.45dB
22-11-06 22:33:29.124 : ---5-->   0005.jpg | 29.22dB
22-11-06 22:33:35.733 : ---6-->   0006.jpg | 33.39dB
22-11-06 22:33:40.437 : ---7-->   0007.jpg | 35.55dB
22-11-06 22:33:46.926 : ---8-->   0008.jpg | 24.27dB
22-11-06 22:33:50.740 : ---9-->   0009.jpg | 23.52dB
22-11-06 22:33:58.380 : --10-->   0010.jpg | 28.35dB
22-11-06 22:34:03.481 : --11-->   0011.jpg | 26.56dB
22-11-06 22:34:07.858 : --12-->   0012.jpg | 24.36dB
22-11-06 22:34:14.357 : --13-->   0013.jpg | 24.10dB
22-11-06 22:34:18.228 : --14-->   0014.jpg | 23.47dB
22-11-06 22:34:22.908 : --15-->   0015.jpg | 21.85dB
22-11-06 22:34:24.321 : --16-->   0016.jpg | 30.22dB
22-11-06 22:34:25.985 : --17-->   0017.jpg | 29.53dB
22-11-06 22:34:28.883 : --18-->   0018.jpg | 27.58dB
22-11-06 22:34:33.571 : --19-->   0019.jpg | 32.53dB
22-11-06 22:34:38.238 : --20-->   0020.jpg | 23.96dB
22-11-06 22:34:42.874 : --21-->   0021.jpg | 26.48dB
22-11-06 22:34:45.194 : --22-->   0022.jpg | 39.16dB
22-11-06 22:34:49.727 : --23-->   0023.jpg | 25.55dB
22-11-06 22:34:54.153 : --24-->   0024.jpg | 34.24dB
22-11-06 22:35:01.245 : --25-->   0025.jpg | 26.12dB
22-11-06 22:35:04.955 : --26-->   0026.jpg | 27.13dB
22-11-06 22:35:11.974 : --27-->   0027.jpg | 29.07dB
22-11-06 22:35:14.048 : --28-->   0028.jpg | 30.38dB
22-11-06 22:35:16.545 : --29-->   0029.jpg | 24.22dB
22-11-06 22:35:21.761 : --30-->   0030.jpg | 29.24dB
22-11-06 22:35:26.204 : --31-->   0031.jpg | 28.24dB
22-11-06 22:35:31.450 : --32-->   0032.jpg | 26.30dB
22-11-06 22:35:35.476 : --33-->   0033.jpg | 23.68dB
22-11-06 22:35:41.904 : --34-->   0034.jpg | 23.36dB
22-11-06 22:35:44.445 : --35-->   0035.jpg | 27.15dB
22-11-06 22:35:49.136 : --36-->   0036.jpg | 23.27dB
22-11-06 22:35:54.871 : --37-->   0037.jpg | 23.07dB
22-11-06 22:36:00.462 : --38-->   0038.jpg | 26.74dB
22-11-06 22:36:04.238 : --39-->   0039.jpg | 24.12dB
22-11-06 22:36:07.631 : --40-->   0040.jpg | 28.21dB
22-11-06 22:36:11.918 : --41-->   0041.jpg | 21.26dB
22-11-06 22:36:17.885 : --42-->   0042.jpg | 26.65dB
22-11-06 22:36:20.034 : --43-->   0043.jpg | 27.44dB
22-11-06 22:36:25.691 : --44-->   0044.jpg | 24.70dB
22-11-06 22:36:29.148 : --45-->   0045.jpg | 21.61dB
22-11-06 22:36:32.866 : --46-->   0046.jpg | 23.34dB
22-11-06 22:36:38.118 : --47-->   0047.jpg | 24.41dB
22-11-06 22:36:41.814 : --48-->   0048.jpg | 28.56dB
22-11-06 22:36:45.637 : --49-->   0049.jpg | 28.48dB
22-11-06 22:36:48.126 : --50-->   0050.jpg | 21.15dB
22-11-06 22:36:52.266 : --51-->   0051.jpg | 26.06dB
22-11-06 22:36:57.397 : --52-->   0052.jpg | 23.60dB
22-11-06 22:36:59.304 : --53-->   0053.jpg | 24.56dB
22-11-06 22:37:00.961 : --54-->   0054.jpg | 22.10dB
22-11-06 22:37:06.350 : --55-->   0055.jpg | 36.44dB
22-11-06 22:37:11.957 : --56-->   0056.jpg | 24.69dB
22-11-06 22:37:17.767 : --57-->   0057.jpg | 30.73dB
22-11-06 22:37:21.306 : --58-->   0058.jpg | 26.77dB
22-11-06 22:37:24.086 : --59-->   0059.jpg | 27.89dB
22-11-06 22:37:28.049 : --60-->   0060.jpg | 24.87dB
22-11-06 22:37:34.080 : --61-->   0061.jpg | 26.56dB
22-11-06 22:37:41.052 : --62-->   0062.jpg | 24.45dB
22-11-06 22:37:44.847 : --63-->   0063.jpg | 23.18dB
22-11-06 22:37:50.214 : --64-->   0064.jpg | 29.03dB
22-11-06 22:37:53.635 : --65-->   0065.jpg | 35.17dB
22-11-06 22:37:57.212 : --66-->   0066.jpg | 30.18dB
22-11-06 22:38:03.852 : --67-->   0067.jpg | 26.65dB
22-11-06 22:38:07.579 : --68-->   0068.jpg | 25.31dB
22-11-06 22:38:10.645 : --69-->   0069.jpg | 19.43dB
22-11-06 22:38:16.175 : --70-->   0070.jpg | 21.17dB
22-11-06 22:38:18.782 : --71-->   0071.jpg | 22.56dB
22-11-06 22:38:23.915 : --72-->   0072.jpg | 28.39dB
22-11-06 22:38:29.550 : --73-->   0073.jpg | 27.46dB
22-11-06 22:38:34.498 : --74-->   0074.jpg | 24.49dB
22-11-06 22:38:36.469 : --75-->   0075.jpg | 24.61dB
22-11-06 22:38:41.227 : --76-->   0076.jpg | 25.36dB
22-11-06 22:38:45.486 : --77-->   0077.jpg | 22.53dB
22-11-06 22:38:49.928 : --78-->   0078.jpg | 28.02dB
22-11-06 22:38:55.299 : --79-->   0079.jpg | 20.47dB
22-11-06 22:38:58.312 : --80-->   0080.jpg | 30.14dB
22-11-06 22:39:04.636 : --81-->   0081.jpg | 28.70dB
22-11-06 22:39:07.513 : --82-->   0082.jpg | 28.54dB
22-11-06 22:39:11.514 : --83-->   0083.jpg | 23.24dB
22-11-06 22:39:17.761 : --84-->   0084.jpg | 28.96dB
22-11-06 22:39:21.138 : --85-->   0085.jpg | 20.96dB
22-11-06 22:39:26.354 : --86-->   0086.jpg | 24.41dB
22-11-06 22:39:31.302 : --87-->   0087.jpg | 20.94dB
22-11-06 22:39:37.562 : --88-->   0088.jpg | 27.34dB
22-11-06 22:39:40.420 : --89-->   0089.jpg | 23.86dB
22-11-06 22:39:44.694 : --90-->   0090.jpg | 21.81dB
22-11-06 22:39:48.981 : --91-->   0091.jpg | 31.11dB
22-11-06 22:39:51.037 : --92-->   0092.jpg | 22.57dB
22-11-06 22:39:56.230 : --93-->   0093.jpg | 27.75dB
22-11-06 22:40:03.214 : --94-->   0094.jpg | 30.47dB
22-11-06 22:40:08.514 : --95-->   0095.jpg | 25.20dB
22-11-06 22:40:16.365 : --96-->   0096.jpg | 20.89dB
22-11-06 22:40:21.565 : --97-->   0097.jpg | 32.57dB
22-11-06 22:40:26.592 : --98-->   0098.jpg | 25.63dB
22-11-06 22:40:34.064 : --99-->   0099.jpg | 24.99dB
22-11-06 22:40:38.414 : -100-->   0100.jpg | 23.53dB
22-11-06 22:40:39.077 : <epoch:521, iter: 155,000, Average PSNR : 26.37dB

22-11-06 22:42:41.526 : <epoch:522, iter: 155,200, lr:2.000e-04> G_loss: 2.675e-02 
22-11-06 22:44:45.591 : <epoch:523, iter: 155,400, lr:2.000e-04> G_loss: 3.467e-02 
22-11-06 22:46:49.949 : <epoch:523, iter: 155,600, lr:2.000e-04> G_loss: 2.994e-02 
22-11-06 22:48:52.892 : <epoch:524, iter: 155,800, lr:2.000e-04> G_loss: 3.408e-02 
22-11-06 22:50:57.242 : <epoch:525, iter: 156,000, lr:2.000e-04> G_loss: 3.804e-02 
22-11-06 22:52:54.231 : <epoch:525, iter: 156,200, lr:2.000e-04> G_loss: 2.682e-02 
22-11-06 22:54:58.124 : <epoch:526, iter: 156,400, lr:2.000e-04> G_loss: 2.755e-02 
22-11-06 22:56:54.909 : <epoch:527, iter: 156,600, lr:2.000e-04> G_loss: 3.761e-02 
22-11-06 22:58:54.364 : <epoch:527, iter: 156,800, lr:2.000e-04> G_loss: 3.072e-02 
22-11-06 23:01:01.362 : <epoch:528, iter: 157,000, lr:2.000e-04> G_loss: 3.016e-02 
22-11-06 23:03:07.892 : <epoch:529, iter: 157,200, lr:2.000e-04> G_loss: 3.313e-02 
22-11-06 23:05:07.165 : <epoch:529, iter: 157,400, lr:2.000e-04> G_loss: 3.348e-02 
22-11-06 23:07:10.455 : <epoch:530, iter: 157,600, lr:2.000e-04> G_loss: 3.661e-02 
22-11-06 23:09:12.809 : <epoch:531, iter: 157,800, lr:2.000e-04> G_loss: 3.581e-02 
22-11-06 23:11:13.080 : <epoch:531, iter: 158,000, lr:2.000e-04> G_loss: 3.793e-02 
22-11-06 23:13:24.518 : <epoch:532, iter: 158,200, lr:2.000e-04> G_loss: 3.321e-02 
22-11-06 23:15:32.682 : <epoch:533, iter: 158,400, lr:2.000e-04> G_loss: 3.048e-02 
22-11-06 23:17:38.515 : <epoch:534, iter: 158,600, lr:2.000e-04> G_loss: 2.722e-02 
22-11-06 23:19:40.330 : <epoch:534, iter: 158,800, lr:2.000e-04> G_loss: 3.364e-02 
22-11-06 23:21:41.095 : <epoch:535, iter: 159,000, lr:2.000e-04> G_loss: 3.361e-02 
22-11-06 23:23:45.694 : <epoch:536, iter: 159,200, lr:2.000e-04> G_loss: 4.373e-02 
22-11-06 23:25:51.063 : <epoch:536, iter: 159,400, lr:2.000e-04> G_loss: 3.522e-02 
22-11-06 23:27:57.279 : <epoch:537, iter: 159,600, lr:2.000e-04> G_loss: 2.789e-02 
22-11-06 23:30:02.187 : <epoch:538, iter: 159,800, lr:2.000e-04> G_loss: 4.094e-02 
22-11-06 23:32:07.572 : <epoch:538, iter: 160,000, lr:2.000e-04> G_loss: 3.229e-02 
22-11-06 23:32:07.583 : Saving the model.
22-11-06 23:32:12.739 : ---1-->   0001.jpg | 25.20dB
22-11-06 23:32:16.369 : ---2-->   0002.jpg | 25.88dB
22-11-06 23:32:18.991 : ---3-->   0003.jpg | 29.81dB
22-11-06 23:32:22.073 : ---4-->   0004.jpg | 28.55dB
22-11-06 23:32:26.751 : ---5-->   0005.jpg | 29.25dB
22-11-06 23:32:30.695 : ---6-->   0006.jpg | 33.52dB
22-11-06 23:32:35.910 : ---7-->   0007.jpg | 35.68dB
22-11-06 23:32:41.023 : ---8-->   0008.jpg | 24.32dB
22-11-06 23:32:46.730 : ---9-->   0009.jpg | 23.47dB
22-11-06 23:32:53.055 : --10-->   0010.jpg | 28.40dB
22-11-06 23:32:58.310 : --11-->   0011.jpg | 26.63dB
22-11-06 23:33:02.048 : --12-->   0012.jpg | 24.37dB
22-11-06 23:33:05.414 : --13-->   0013.jpg | 24.12dB
22-11-06 23:33:08.504 : --14-->   0014.jpg | 23.53dB
22-11-06 23:33:13.759 : --15-->   0015.jpg | 21.93dB
22-11-06 23:33:19.494 : --16-->   0016.jpg | 30.23dB
22-11-06 23:33:23.995 : --17-->   0017.jpg | 29.53dB
22-11-06 23:33:28.345 : --18-->   0018.jpg | 26.96dB
22-11-06 23:33:35.029 : --19-->   0019.jpg | 32.74dB
22-11-06 23:33:40.145 : --20-->   0020.jpg | 24.06dB
22-11-06 23:33:43.113 : --21-->   0021.jpg | 26.69dB
22-11-06 23:33:45.616 : --22-->   0022.jpg | 39.18dB
22-11-06 23:33:51.565 : --23-->   0023.jpg | 25.61dB
22-11-06 23:33:54.204 : --24-->   0024.jpg | 34.23dB
22-11-06 23:33:58.055 : --25-->   0025.jpg | 26.18dB
22-11-06 23:34:01.432 : --26-->   0026.jpg | 27.12dB
22-11-06 23:34:07.298 : --27-->   0027.jpg | 29.21dB
22-11-06 23:34:09.408 : --28-->   0028.jpg | 30.57dB
22-11-06 23:34:11.073 : --29-->   0029.jpg | 24.23dB
22-11-06 23:34:15.738 : --30-->   0030.jpg | 29.37dB
22-11-06 23:34:17.887 : --31-->   0031.jpg | 28.25dB
22-11-06 23:34:19.889 : --32-->   0032.jpg | 26.33dB
22-11-06 23:34:26.622 : --33-->   0033.jpg | 23.80dB
22-11-06 23:34:28.902 : --34-->   0034.jpg | 23.42dB
22-11-06 23:34:34.177 : --35-->   0035.jpg | 27.01dB
22-11-06 23:34:42.478 : --36-->   0036.jpg | 23.31dB
22-11-06 23:34:45.867 : --37-->   0037.jpg | 23.11dB
22-11-06 23:34:52.516 : --38-->   0038.jpg | 26.88dB
22-11-06 23:34:56.674 : --39-->   0039.jpg | 24.18dB
22-11-06 23:35:02.825 : --40-->   0040.jpg | 28.23dB
22-11-06 23:35:07.432 : --41-->   0041.jpg | 21.24dB
22-11-06 23:35:08.955 : --42-->   0042.jpg | 26.81dB
22-11-06 23:35:12.308 : --43-->   0043.jpg | 27.42dB
22-11-06 23:35:16.774 : --44-->   0044.jpg | 24.74dB
22-11-06 23:35:19.624 : --45-->   0045.jpg | 21.70dB
22-11-06 23:35:22.457 : --46-->   0046.jpg | 23.43dB
22-11-06 23:35:25.718 : --47-->   0047.jpg | 24.47dB
22-11-06 23:35:30.931 : --48-->   0048.jpg | 28.66dB
22-11-06 23:35:34.841 : --49-->   0049.jpg | 28.55dB
22-11-06 23:35:38.810 : --50-->   0050.jpg | 21.22dB
22-11-06 23:35:42.925 : --51-->   0051.jpg | 26.17dB
22-11-06 23:35:46.853 : --52-->   0052.jpg | 23.66dB
22-11-06 23:35:54.539 : --53-->   0053.jpg | 24.72dB
22-11-06 23:35:57.686 : --54-->   0054.jpg | 22.14dB
22-11-06 23:36:01.881 : --55-->   0055.jpg | 36.44dB
22-11-06 23:36:06.722 : --56-->   0056.jpg | 24.66dB
22-11-06 23:36:10.926 : --57-->   0057.jpg | 30.63dB
22-11-06 23:36:18.066 : --58-->   0058.jpg | 26.86dB
22-11-06 23:36:21.263 : --59-->   0059.jpg | 28.05dB
22-11-06 23:36:26.337 : --60-->   0060.jpg | 24.99dB
22-11-06 23:36:27.443 : --61-->   0061.jpg | 26.34dB
22-11-06 23:36:30.095 : --62-->   0062.jpg | 24.50dB
22-11-06 23:36:35.167 : --63-->   0063.jpg | 23.26dB
22-11-06 23:36:39.867 : --64-->   0064.jpg | 28.99dB
22-11-06 23:36:45.257 : --65-->   0065.jpg | 35.10dB
22-11-06 23:36:50.581 : --66-->   0066.jpg | 30.16dB
22-11-06 23:36:57.884 : --67-->   0067.jpg | 26.95dB
22-11-06 23:37:02.075 : --68-->   0068.jpg | 25.32dB
22-11-06 23:37:05.670 : --69-->   0069.jpg | 19.46dB
22-11-06 23:37:08.105 : --70-->   0070.jpg | 21.24dB
22-11-06 23:37:11.552 : --71-->   0071.jpg | 22.58dB
22-11-06 23:37:13.809 : --72-->   0072.jpg | 28.49dB
22-11-06 23:37:19.570 : --73-->   0073.jpg | 27.56dB
22-11-06 23:37:22.233 : --74-->   0074.jpg | 24.65dB
22-11-06 23:37:27.498 : --75-->   0075.jpg | 24.70dB
22-11-06 23:37:34.729 : --76-->   0076.jpg | 25.29dB
22-11-06 23:37:41.261 : --77-->   0077.jpg | 22.62dB
22-11-06 23:37:42.787 : --78-->   0078.jpg | 28.00dB
22-11-06 23:37:44.457 : --79-->   0079.jpg | 20.49dB
22-11-06 23:37:46.573 : --80-->   0080.jpg | 30.32dB
22-11-06 23:37:48.655 : --81-->   0081.jpg | 28.69dB
22-11-06 23:37:51.836 : --82-->   0082.jpg | 28.50dB
22-11-06 23:37:55.558 : --83-->   0083.jpg | 23.24dB
22-11-06 23:37:58.329 : --84-->   0084.jpg | 29.11dB
22-11-06 23:38:00.795 : --85-->   0085.jpg | 21.05dB
22-11-06 23:38:03.233 : --86-->   0086.jpg | 24.48dB
22-11-06 23:38:08.532 : --87-->   0087.jpg | 20.90dB
22-11-06 23:38:15.227 : --88-->   0088.jpg | 27.35dB
22-11-06 23:38:19.215 : --89-->   0089.jpg | 23.96dB
22-11-06 23:38:20.798 : --90-->   0090.jpg | 21.84dB
22-11-06 23:38:23.434 : --91-->   0091.jpg | 31.21dB
22-11-06 23:38:29.794 : --92-->   0092.jpg | 22.55dB
22-11-06 23:38:33.026 : --93-->   0093.jpg | 27.87dB
22-11-06 23:38:39.043 : --94-->   0094.jpg | 30.65dB
22-11-06 23:38:42.787 : --95-->   0095.jpg | 25.21dB
22-11-06 23:38:44.824 : --96-->   0096.jpg | 20.91dB
22-11-06 23:38:51.396 : --97-->   0097.jpg | 32.64dB
22-11-06 23:38:54.016 : --98-->   0098.jpg | 25.67dB
22-11-06 23:39:00.301 : --99-->   0099.jpg | 24.95dB
22-11-06 23:39:04.721 : -100-->   0100.jpg | 23.57dB
22-11-06 23:39:04.931 : <epoch:538, iter: 160,000, Average PSNR : 26.42dB

22-11-06 23:41:13.767 : <epoch:539, iter: 160,200, lr:2.000e-04> G_loss: 2.585e-02 
22-11-06 23:43:21.865 : <epoch:540, iter: 160,400, lr:2.000e-04> G_loss: 2.957e-02 
22-11-06 23:45:24.198 : <epoch:540, iter: 160,600, lr:2.000e-04> G_loss: 4.128e-02 
22-11-06 23:47:29.442 : <epoch:541, iter: 160,800, lr:2.000e-04> G_loss: 3.145e-02 
22-11-06 23:49:33.048 : <epoch:542, iter: 161,000, lr:2.000e-04> G_loss: 2.760e-02 
22-11-06 23:51:39.429 : <epoch:542, iter: 161,200, lr:2.000e-04> G_loss: 3.354e-02 
22-11-06 23:53:33.536 : <epoch:543, iter: 161,400, lr:2.000e-04> G_loss: 3.867e-02 
22-11-06 23:55:35.009 : <epoch:544, iter: 161,600, lr:2.000e-04> G_loss: 4.011e-02 
22-11-06 23:57:33.177 : <epoch:544, iter: 161,800, lr:2.000e-04> G_loss: 3.200e-02 
22-11-06 23:59:37.341 : <epoch:545, iter: 162,000, lr:2.000e-04> G_loss: 3.554e-02 
22-11-07 00:01:40.445 : <epoch:546, iter: 162,200, lr:2.000e-04> G_loss: 2.962e-02 
22-11-07 00:03:39.075 : <epoch:546, iter: 162,400, lr:2.000e-04> G_loss: 3.204e-02 
22-11-07 00:05:41.903 : <epoch:547, iter: 162,600, lr:2.000e-04> G_loss: 3.050e-02 
22-11-07 00:07:43.890 : <epoch:548, iter: 162,800, lr:2.000e-04> G_loss: 3.178e-02 
22-11-07 00:09:47.175 : <epoch:548, iter: 163,000, lr:2.000e-04> G_loss: 3.555e-02 
22-11-07 00:11:51.512 : <epoch:549, iter: 163,200, lr:2.000e-04> G_loss: 3.251e-02 
22-11-07 00:13:54.395 : <epoch:550, iter: 163,400, lr:2.000e-04> G_loss: 3.196e-02 
22-11-07 00:16:00.846 : <epoch:550, iter: 163,600, lr:2.000e-04> G_loss: 3.217e-02 
22-11-07 00:18:03.722 : <epoch:551, iter: 163,800, lr:2.000e-04> G_loss: 3.559e-02 
22-11-07 00:20:10.856 : <epoch:552, iter: 164,000, lr:2.000e-04> G_loss: 3.322e-02 
22-11-07 00:22:13.201 : <epoch:552, iter: 164,200, lr:2.000e-04> G_loss: 3.182e-02 
22-11-07 00:24:19.948 : <epoch:553, iter: 164,400, lr:2.000e-04> G_loss: 3.814e-02 
22-11-07 00:26:29.519 : <epoch:554, iter: 164,600, lr:2.000e-04> G_loss: 3.033e-02 
22-11-07 00:28:30.770 : <epoch:554, iter: 164,800, lr:2.000e-04> G_loss: 3.311e-02 
22-11-07 00:30:37.378 : <epoch:555, iter: 165,000, lr:2.000e-04> G_loss: 3.204e-02 
22-11-07 00:30:37.380 : Saving the model.
22-11-07 00:30:45.740 : ---1-->   0001.jpg | 25.10dB
22-11-07 00:30:52.546 : ---2-->   0002.jpg | 25.72dB
22-11-07 00:30:54.749 : ---3-->   0003.jpg | 29.72dB
22-11-07 00:30:55.582 : ---4-->   0004.jpg | 28.41dB
22-11-07 00:30:57.650 : ---5-->   0005.jpg | 29.24dB
22-11-07 00:31:02.295 : ---6-->   0006.jpg | 32.92dB
22-11-07 00:31:06.570 : ---7-->   0007.jpg | 35.50dB
22-11-07 00:31:10.275 : ---8-->   0008.jpg | 24.31dB
22-11-07 00:31:15.927 : ---9-->   0009.jpg | 23.49dB
22-11-07 00:31:20.121 : --10-->   0010.jpg | 28.22dB
22-11-07 00:31:25.815 : --11-->   0011.jpg | 26.46dB
22-11-07 00:31:31.933 : --12-->   0012.jpg | 24.27dB
22-11-07 00:31:35.976 : --13-->   0013.jpg | 24.24dB
22-11-07 00:31:40.521 : --14-->   0014.jpg | 23.56dB
22-11-07 00:31:46.616 : --15-->   0015.jpg | 21.97dB
22-11-07 00:31:51.203 : --16-->   0016.jpg | 30.11dB
22-11-07 00:31:53.207 : --17-->   0017.jpg | 29.45dB
22-11-07 00:31:58.725 : --18-->   0018.jpg | 27.37dB
22-11-07 00:32:04.303 : --19-->   0019.jpg | 32.57dB
22-11-07 00:32:11.392 : --20-->   0020.jpg | 23.97dB
22-11-07 00:32:16.092 : --21-->   0021.jpg | 26.69dB
22-11-07 00:32:20.495 : --22-->   0022.jpg | 39.05dB
22-11-07 00:32:23.820 : --23-->   0023.jpg | 25.60dB
22-11-07 00:32:27.931 : --24-->   0024.jpg | 34.39dB
22-11-07 00:32:29.431 : --25-->   0025.jpg | 26.13dB
22-11-07 00:32:35.378 : --26-->   0026.jpg | 27.08dB
22-11-07 00:32:42.991 : --27-->   0027.jpg | 29.10dB
22-11-07 00:32:48.609 : --28-->   0028.jpg | 30.33dB
22-11-07 00:32:52.447 : --29-->   0029.jpg | 24.22dB
22-11-07 00:32:57.303 : --30-->   0030.jpg | 29.26dB
22-11-07 00:33:00.624 : --31-->   0031.jpg | 28.26dB
22-11-07 00:33:04.049 : --32-->   0032.jpg | 26.17dB
22-11-07 00:33:10.137 : --33-->   0033.jpg | 23.90dB
22-11-07 00:33:15.284 : --34-->   0034.jpg | 23.39dB
22-11-07 00:33:19.119 : --35-->   0035.jpg | 26.93dB
22-11-07 00:33:21.252 : --36-->   0036.jpg | 23.32dB
22-11-07 00:33:22.958 : --37-->   0037.jpg | 23.14dB
22-11-07 00:33:28.151 : --38-->   0038.jpg | 26.81dB
22-11-07 00:33:33.548 : --39-->   0039.jpg | 24.23dB
22-11-07 00:33:35.457 : --40-->   0040.jpg | 28.21dB
22-11-07 00:33:36.852 : --41-->   0041.jpg | 21.24dB
22-11-07 00:33:43.980 : --42-->   0042.jpg | 26.72dB
22-11-07 00:33:50.481 : --43-->   0043.jpg | 27.33dB
22-11-07 00:33:55.216 : --44-->   0044.jpg | 24.71dB
22-11-07 00:33:58.560 : --45-->   0045.jpg | 21.63dB
22-11-07 00:34:04.565 : --46-->   0046.jpg | 23.26dB
22-11-07 00:34:07.595 : --47-->   0047.jpg | 24.35dB
22-11-07 00:34:12.058 : --48-->   0048.jpg | 28.42dB
22-11-07 00:34:15.283 : --49-->   0049.jpg | 28.61dB
22-11-07 00:34:17.276 : --50-->   0050.jpg | 21.24dB
22-11-07 00:34:20.739 : --51-->   0051.jpg | 26.04dB
22-11-07 00:34:26.845 : --52-->   0052.jpg | 23.59dB
22-11-07 00:34:33.012 : --53-->   0053.jpg | 24.76dB
22-11-07 00:34:34.994 : --54-->   0054.jpg | 22.13dB
22-11-07 00:34:38.621 : --55-->   0055.jpg | 36.51dB
22-11-07 00:34:42.423 : --56-->   0056.jpg | 24.63dB
22-11-07 00:34:46.643 : --57-->   0057.jpg | 30.65dB
22-11-07 00:34:50.502 : --58-->   0058.jpg | 26.75dB
22-11-07 00:34:52.203 : --59-->   0059.jpg | 27.98dB
22-11-07 00:34:55.956 : --60-->   0060.jpg | 25.06dB
22-11-07 00:35:00.731 : --61-->   0061.jpg | 26.38dB
22-11-07 00:35:06.031 : --62-->   0062.jpg | 24.50dB
22-11-07 00:35:09.422 : --63-->   0063.jpg | 23.26dB
22-11-07 00:35:15.363 : --64-->   0064.jpg | 29.06dB
22-11-07 00:35:19.036 : --65-->   0065.jpg | 35.12dB
22-11-07 00:35:24.255 : --66-->   0066.jpg | 30.25dB
22-11-07 00:35:26.781 : --67-->   0067.jpg | 26.90dB
22-11-07 00:35:32.233 : --68-->   0068.jpg | 25.32dB
22-11-07 00:35:36.373 : --69-->   0069.jpg | 19.44dB
22-11-07 00:35:37.611 : --70-->   0070.jpg | 21.13dB
22-11-07 00:35:41.733 : --71-->   0071.jpg | 22.56dB
22-11-07 00:35:45.511 : --72-->   0072.jpg | 28.43dB
22-11-07 00:35:49.490 : --73-->   0073.jpg | 27.58dB
22-11-07 00:35:52.083 : --74-->   0074.jpg | 24.67dB
22-11-07 00:35:54.791 : --75-->   0075.jpg | 24.55dB
22-11-07 00:35:58.652 : --76-->   0076.jpg | 25.28dB
22-11-07 00:36:05.304 : --77-->   0077.jpg | 22.54dB
22-11-07 00:36:08.053 : --78-->   0078.jpg | 27.91dB
22-11-07 00:36:13.248 : --79-->   0079.jpg | 20.46dB
22-11-07 00:36:16.146 : --80-->   0080.jpg | 30.30dB
22-11-07 00:36:22.333 : --81-->   0081.jpg | 28.67dB
22-11-07 00:36:24.454 : --82-->   0082.jpg | 28.51dB
22-11-07 00:36:28.509 : --83-->   0083.jpg | 23.17dB
22-11-07 00:36:32.781 : --84-->   0084.jpg | 29.23dB
22-11-07 00:36:36.516 : --85-->   0085.jpg | 21.12dB
22-11-07 00:36:41.758 : --86-->   0086.jpg | 24.41dB
22-11-07 00:36:44.337 : --87-->   0087.jpg | 20.92dB
22-11-07 00:36:47.952 : --88-->   0088.jpg | 27.33dB
22-11-07 00:36:51.541 : --89-->   0089.jpg | 23.91dB
22-11-07 00:36:56.446 : --90-->   0090.jpg | 21.82dB
22-11-07 00:37:01.097 : --91-->   0091.jpg | 31.17dB
22-11-07 00:37:04.144 : --92-->   0092.jpg | 22.53dB
22-11-07 00:37:07.442 : --93-->   0093.jpg | 27.78dB
22-11-07 00:37:11.864 : --94-->   0094.jpg | 30.50dB
22-11-07 00:37:15.513 : --95-->   0095.jpg | 25.17dB
22-11-07 00:37:17.961 : --96-->   0096.jpg | 20.92dB
22-11-07 00:37:20.321 : --97-->   0097.jpg | 32.51dB
22-11-07 00:37:25.056 : --98-->   0098.jpg | 25.69dB
22-11-07 00:37:29.359 : --99-->   0099.jpg | 24.96dB
22-11-07 00:37:34.689 : -100-->   0100.jpg | 23.57dB
22-11-07 00:37:34.850 : <epoch:555, iter: 165,000, Average PSNR : 26.38dB

22-11-07 00:39:34.765 : <epoch:556, iter: 165,200, lr:2.000e-04> G_loss: 3.303e-02 
22-11-07 00:41:30.433 : <epoch:556, iter: 165,400, lr:2.000e-04> G_loss: 2.943e-02 
22-11-07 00:43:37.825 : <epoch:557, iter: 165,600, lr:2.000e-04> G_loss: 3.531e-02 
22-11-07 00:45:42.436 : <epoch:558, iter: 165,800, lr:2.000e-04> G_loss: 3.600e-02 
22-11-07 00:47:44.780 : <epoch:558, iter: 166,000, lr:2.000e-04> G_loss: 3.428e-02 
22-11-07 00:49:51.061 : <epoch:559, iter: 166,200, lr:2.000e-04> G_loss: 2.876e-02 
22-11-07 00:52:02.252 : <epoch:560, iter: 166,400, lr:2.000e-04> G_loss: 2.975e-02 
22-11-07 00:54:03.531 : <epoch:560, iter: 166,600, lr:2.000e-04> G_loss: 3.264e-02 
22-11-07 00:56:08.031 : <epoch:561, iter: 166,800, lr:2.000e-04> G_loss: 3.374e-02 
22-11-07 00:58:16.072 : <epoch:562, iter: 167,000, lr:2.000e-04> G_loss: 4.741e-02 
22-11-07 01:00:13.657 : <epoch:562, iter: 167,200, lr:2.000e-04> G_loss: 3.429e-02 
22-11-07 01:02:17.917 : <epoch:563, iter: 167,400, lr:2.000e-04> G_loss: 3.518e-02 
22-11-07 01:04:24.566 : <epoch:564, iter: 167,600, lr:2.000e-04> G_loss: 3.328e-02 
22-11-07 01:06:22.039 : <epoch:564, iter: 167,800, lr:2.000e-04> G_loss: 3.826e-02 
22-11-07 01:08:29.812 : <epoch:565, iter: 168,000, lr:2.000e-04> G_loss: 3.052e-02 
22-11-07 01:10:30.831 : <epoch:566, iter: 168,200, lr:2.000e-04> G_loss: 3.497e-02 
22-11-07 01:12:36.772 : <epoch:567, iter: 168,400, lr:2.000e-04> G_loss: 3.137e-02 
22-11-07 01:14:41.728 : <epoch:567, iter: 168,600, lr:2.000e-04> G_loss: 3.635e-02 
22-11-07 01:16:42.852 : <epoch:568, iter: 168,800, lr:2.000e-04> G_loss: 3.760e-02 
22-11-07 01:18:47.258 : <epoch:569, iter: 169,000, lr:2.000e-04> G_loss: 3.137e-02 
22-11-07 01:20:51.902 : <epoch:569, iter: 169,200, lr:2.000e-04> G_loss: 3.768e-02 
22-11-07 01:22:54.806 : <epoch:570, iter: 169,400, lr:2.000e-04> G_loss: 2.724e-02 
22-11-07 01:25:03.776 : <epoch:571, iter: 169,600, lr:2.000e-04> G_loss: 3.162e-02 
22-11-07 01:27:07.553 : <epoch:571, iter: 169,800, lr:2.000e-04> G_loss: 3.241e-02 
22-11-07 01:29:09.003 : <epoch:572, iter: 170,000, lr:2.000e-04> G_loss: 2.879e-02 
22-11-07 01:29:09.004 : Saving the model.
22-11-07 01:29:16.725 : ---1-->   0001.jpg | 25.24dB
22-11-07 01:29:19.396 : ---2-->   0002.jpg | 25.81dB
22-11-07 01:29:24.958 : ---3-->   0003.jpg | 29.81dB
22-11-07 01:29:29.009 : ---4-->   0004.jpg | 28.47dB
22-11-07 01:29:33.057 : ---5-->   0005.jpg | 29.24dB
22-11-07 01:29:37.085 : ---6-->   0006.jpg | 33.17dB
22-11-07 01:29:40.244 : ---7-->   0007.jpg | 35.94dB
22-11-07 01:29:45.410 : ---8-->   0008.jpg | 24.31dB
22-11-07 01:29:50.105 : ---9-->   0009.jpg | 23.53dB
22-11-07 01:29:54.060 : --10-->   0010.jpg | 28.44dB
22-11-07 01:29:58.714 : --11-->   0011.jpg | 26.55dB
22-11-07 01:30:04.465 : --12-->   0012.jpg | 24.33dB
22-11-07 01:30:09.508 : --13-->   0013.jpg | 24.27dB
22-11-07 01:30:12.761 : --14-->   0014.jpg | 23.56dB
22-11-07 01:30:16.019 : --15-->   0015.jpg | 21.97dB
22-11-07 01:30:20.049 : --16-->   0016.jpg | 30.08dB
22-11-07 01:30:24.081 : --17-->   0017.jpg | 29.60dB
22-11-07 01:30:30.961 : --18-->   0018.jpg | 27.66dB
22-11-07 01:30:34.962 : --19-->   0019.jpg | 32.64dB
22-11-07 01:30:38.798 : --20-->   0020.jpg | 23.84dB
22-11-07 01:30:41.642 : --21-->   0021.jpg | 26.62dB
22-11-07 01:30:46.880 : --22-->   0022.jpg | 38.75dB
22-11-07 01:30:50.551 : --23-->   0023.jpg | 25.68dB
22-11-07 01:30:54.771 : --24-->   0024.jpg | 34.23dB
22-11-07 01:30:59.350 : --25-->   0025.jpg | 26.19dB
22-11-07 01:31:05.131 : --26-->   0026.jpg | 27.21dB
22-11-07 01:31:10.245 : --27-->   0027.jpg | 29.20dB
22-11-07 01:31:13.866 : --28-->   0028.jpg | 30.20dB
22-11-07 01:31:19.362 : --29-->   0029.jpg | 24.24dB
22-11-07 01:31:21.743 : --30-->   0030.jpg | 29.31dB
22-11-07 01:31:27.826 : --31-->   0031.jpg | 28.34dB
22-11-07 01:31:31.435 : --32-->   0032.jpg | 26.09dB
22-11-07 01:31:34.114 : --33-->   0033.jpg | 23.83dB
22-11-07 01:31:40.092 : --34-->   0034.jpg | 23.43dB
22-11-07 01:31:43.486 : --35-->   0035.jpg | 27.16dB
22-11-07 01:31:45.331 : --36-->   0036.jpg | 23.32dB
22-11-07 01:31:47.059 : --37-->   0037.jpg | 23.02dB
22-11-07 01:31:49.153 : --38-->   0038.jpg | 26.93dB
22-11-07 01:31:50.350 : --39-->   0039.jpg | 24.33dB
22-11-07 01:31:51.253 : --40-->   0040.jpg | 28.26dB
22-11-07 01:31:53.415 : --41-->   0041.jpg | 21.14dB
22-11-07 01:31:59.037 : --42-->   0042.jpg | 26.77dB
22-11-07 01:32:01.569 : --43-->   0043.jpg | 27.40dB
22-11-07 01:32:06.194 : --44-->   0044.jpg | 24.73dB
22-11-07 01:32:09.949 : --45-->   0045.jpg | 21.64dB
22-11-07 01:32:12.708 : --46-->   0046.jpg | 23.41dB
22-11-07 01:32:15.770 : --47-->   0047.jpg | 24.48dB
22-11-07 01:32:18.358 : --48-->   0048.jpg | 28.74dB
22-11-07 01:32:21.536 : --49-->   0049.jpg | 28.49dB
22-11-07 01:32:27.455 : --50-->   0050.jpg | 21.24dB
22-11-07 01:32:32.474 : --51-->   0051.jpg | 26.13dB
22-11-07 01:32:35.645 : --52-->   0052.jpg | 23.65dB
22-11-07 01:32:40.623 : --53-->   0053.jpg | 24.90dB
22-11-07 01:32:44.776 : --54-->   0054.jpg | 22.13dB
22-11-07 01:32:48.152 : --55-->   0055.jpg | 36.63dB
22-11-07 01:32:51.647 : --56-->   0056.jpg | 24.63dB
22-11-07 01:32:57.354 : --57-->   0057.jpg | 30.69dB
22-11-07 01:32:59.564 : --58-->   0058.jpg | 26.74dB
22-11-07 01:33:04.855 : --59-->   0059.jpg | 28.00dB
22-11-07 01:33:09.378 : --60-->   0060.jpg | 25.06dB
22-11-07 01:33:11.634 : --61-->   0061.jpg | 26.62dB
22-11-07 01:33:15.134 : --62-->   0062.jpg | 24.46dB
22-11-07 01:33:17.514 : --63-->   0063.jpg | 23.29dB
22-11-07 01:33:22.342 : --64-->   0064.jpg | 29.20dB
22-11-07 01:33:26.478 : --65-->   0065.jpg | 35.21dB
22-11-07 01:33:31.693 : --66-->   0066.jpg | 30.28dB
22-11-07 01:33:34.428 : --67-->   0067.jpg | 26.72dB
22-11-07 01:33:37.919 : --68-->   0068.jpg | 25.30dB
22-11-07 01:33:42.911 : --69-->   0069.jpg | 19.49dB
22-11-07 01:33:45.653 : --70-->   0070.jpg | 21.23dB
22-11-07 01:33:49.223 : --71-->   0071.jpg | 22.57dB
22-11-07 01:33:52.033 : --72-->   0072.jpg | 28.41dB
22-11-07 01:33:56.926 : --73-->   0073.jpg | 27.32dB
22-11-07 01:34:01.690 : --74-->   0074.jpg | 24.65dB
22-11-07 01:34:06.622 : --75-->   0075.jpg | 24.57dB
22-11-07 01:34:11.290 : --76-->   0076.jpg | 25.36dB
22-11-07 01:34:17.074 : --77-->   0077.jpg | 22.60dB
22-11-07 01:34:20.236 : --78-->   0078.jpg | 27.87dB
22-11-07 01:34:24.634 : --79-->   0079.jpg | 20.50dB
22-11-07 01:34:30.456 : --80-->   0080.jpg | 30.37dB
22-11-07 01:34:32.638 : --81-->   0081.jpg | 28.72dB
22-11-07 01:34:36.353 : --82-->   0082.jpg | 28.49dB
22-11-07 01:34:39.886 : --83-->   0083.jpg | 23.40dB
22-11-07 01:34:45.418 : --84-->   0084.jpg | 29.18dB
22-11-07 01:34:46.775 : --85-->   0085.jpg | 21.00dB
22-11-07 01:34:51.962 : --86-->   0086.jpg | 24.52dB
22-11-07 01:34:57.814 : --87-->   0087.jpg | 20.84dB
22-11-07 01:35:03.347 : --88-->   0088.jpg | 27.31dB
22-11-07 01:35:07.323 : --89-->   0089.jpg | 23.98dB
22-11-07 01:35:09.278 : --90-->   0090.jpg | 21.86dB
22-11-07 01:35:11.891 : --91-->   0091.jpg | 31.15dB
22-11-07 01:35:14.351 : --92-->   0092.jpg | 22.47dB
22-11-07 01:35:19.850 : --93-->   0093.jpg | 27.77dB
22-11-07 01:35:21.717 : --94-->   0094.jpg | 30.56dB
22-11-07 01:35:28.135 : --95-->   0095.jpg | 25.18dB
22-11-07 01:35:31.683 : --96-->   0096.jpg | 20.93dB
22-11-07 01:35:32.535 : --97-->   0097.jpg | 32.61dB
22-11-07 01:35:38.403 : --98-->   0098.jpg | 25.69dB
22-11-07 01:35:41.337 : --99-->   0099.jpg | 24.93dB
22-11-07 01:35:46.769 : -100-->   0100.jpg | 23.60dB
22-11-07 01:35:47.113 : <epoch:572, iter: 170,000, Average PSNR : 26.42dB

22-11-07 01:37:51.628 : <epoch:573, iter: 170,200, lr:2.000e-04> G_loss: 2.654e-02 
22-11-07 01:39:52.843 : <epoch:573, iter: 170,400, lr:2.000e-04> G_loss: 2.950e-02 
22-11-07 01:41:54.366 : <epoch:574, iter: 170,600, lr:2.000e-04> G_loss: 3.370e-02 
22-11-07 01:43:55.421 : <epoch:575, iter: 170,800, lr:2.000e-04> G_loss: 3.736e-02 
22-11-07 01:45:50.075 : <epoch:575, iter: 171,000, lr:2.000e-04> G_loss: 3.488e-02 
22-11-07 01:47:57.565 : <epoch:576, iter: 171,200, lr:2.000e-04> G_loss: 3.864e-02 
22-11-07 01:49:57.485 : <epoch:577, iter: 171,400, lr:2.000e-04> G_loss: 3.652e-02 
22-11-07 01:51:57.678 : <epoch:577, iter: 171,600, lr:2.000e-04> G_loss: 3.321e-02 
22-11-07 01:54:03.749 : <epoch:578, iter: 171,800, lr:2.000e-04> G_loss: 3.456e-02 
22-11-07 01:56:06.001 : <epoch:579, iter: 172,000, lr:2.000e-04> G_loss: 2.938e-02 
22-11-07 01:58:05.659 : <epoch:579, iter: 172,200, lr:2.000e-04> G_loss: 3.244e-02 
22-11-07 02:00:15.628 : <epoch:580, iter: 172,400, lr:2.000e-04> G_loss: 2.959e-02 
22-11-07 02:02:17.368 : <epoch:581, iter: 172,600, lr:2.000e-04> G_loss: 3.487e-02 
22-11-07 02:04:09.814 : <epoch:581, iter: 172,800, lr:2.000e-04> G_loss: 3.262e-02 
22-11-07 02:06:13.820 : <epoch:582, iter: 173,000, lr:2.000e-04> G_loss: 2.892e-02 
22-11-07 02:08:11.248 : <epoch:583, iter: 173,200, lr:2.000e-04> G_loss: 3.615e-02 
22-11-07 02:10:01.101 : <epoch:583, iter: 173,400, lr:2.000e-04> G_loss: 2.574e-02 
22-11-07 02:11:59.829 : <epoch:584, iter: 173,600, lr:2.000e-04> G_loss: 3.714e-02 
22-11-07 02:13:56.246 : <epoch:585, iter: 173,800, lr:2.000e-04> G_loss: 2.894e-02 
22-11-07 02:15:56.755 : <epoch:585, iter: 174,000, lr:2.000e-04> G_loss: 3.527e-02 
22-11-07 02:17:53.778 : <epoch:586, iter: 174,200, lr:2.000e-04> G_loss: 2.653e-02 
22-11-07 02:19:49.970 : <epoch:587, iter: 174,400, lr:2.000e-04> G_loss: 3.175e-02 
22-11-07 02:21:45.146 : <epoch:587, iter: 174,600, lr:2.000e-04> G_loss: 2.310e-02 
22-11-07 02:23:40.874 : <epoch:588, iter: 174,800, lr:2.000e-04> G_loss: 3.217e-02 
22-11-07 02:25:32.844 : <epoch:589, iter: 175,000, lr:2.000e-04> G_loss: 2.999e-02 
22-11-07 02:25:32.846 : Saving the model.
22-11-07 02:25:39.288 : ---1-->   0001.jpg | 25.22dB
22-11-07 02:25:45.603 : ---2-->   0002.jpg | 25.88dB
22-11-07 02:25:49.066 : ---3-->   0003.jpg | 29.97dB
22-11-07 02:25:53.032 : ---4-->   0004.jpg | 28.40dB
22-11-07 02:25:57.395 : ---5-->   0005.jpg | 29.23dB
22-11-07 02:26:01.007 : ---6-->   0006.jpg | 33.77dB
22-11-07 02:26:05.585 : ---7-->   0007.jpg | 35.91dB
22-11-07 02:26:11.015 : ---8-->   0008.jpg | 24.31dB
22-11-07 02:26:18.234 : ---9-->   0009.jpg | 23.52dB
22-11-07 02:26:22.012 : --10-->   0010.jpg | 28.37dB
22-11-07 02:26:24.887 : --11-->   0011.jpg | 26.05dB
22-11-07 02:26:29.340 : --12-->   0012.jpg | 24.44dB
22-11-07 02:26:32.766 : --13-->   0013.jpg | 24.24dB
22-11-07 02:26:36.786 : --14-->   0014.jpg | 23.48dB
22-11-07 02:26:42.195 : --15-->   0015.jpg | 22.02dB
22-11-07 02:26:46.398 : --16-->   0016.jpg | 30.36dB
22-11-07 02:26:50.137 : --17-->   0017.jpg | 29.63dB
22-11-07 02:26:56.989 : --18-->   0018.jpg | 26.94dB
22-11-07 02:27:00.631 : --19-->   0019.jpg | 32.59dB
22-11-07 02:27:04.142 : --20-->   0020.jpg | 24.10dB
22-11-07 02:27:08.648 : --21-->   0021.jpg | 26.66dB
22-11-07 02:27:11.640 : --22-->   0022.jpg | 38.91dB
22-11-07 02:27:15.814 : --23-->   0023.jpg | 25.78dB
22-11-07 02:27:18.685 : --24-->   0024.jpg | 34.65dB
22-11-07 02:27:21.583 : --25-->   0025.jpg | 26.20dB
22-11-07 02:27:24.470 : --26-->   0026.jpg | 27.09dB
22-11-07 02:27:27.974 : --27-->   0027.jpg | 29.03dB
22-11-07 02:27:32.024 : --28-->   0028.jpg | 30.70dB
22-11-07 02:27:35.418 : --29-->   0029.jpg | 24.24dB
22-11-07 02:27:39.907 : --30-->   0030.jpg | 29.36dB
22-11-07 02:27:42.852 : --31-->   0031.jpg | 28.32dB
22-11-07 02:27:48.704 : --32-->   0032.jpg | 26.29dB
22-11-07 02:27:51.998 : --33-->   0033.jpg | 23.88dB
22-11-07 02:27:57.166 : --34-->   0034.jpg | 23.43dB
22-11-07 02:28:02.264 : --35-->   0035.jpg | 27.02dB
22-11-07 02:28:04.847 : --36-->   0036.jpg | 23.33dB
22-11-07 02:28:07.297 : --37-->   0037.jpg | 23.06dB
22-11-07 02:28:13.018 : --38-->   0038.jpg | 26.81dB
22-11-07 02:28:15.144 : --39-->   0039.jpg | 24.23dB
22-11-07 02:28:16.075 : --40-->   0040.jpg | 28.25dB
22-11-07 02:28:18.022 : --41-->   0041.jpg | 21.28dB
22-11-07 02:28:21.224 : --42-->   0042.jpg | 26.86dB
22-11-07 02:28:24.567 : --43-->   0043.jpg | 27.48dB
22-11-07 02:28:29.083 : --44-->   0044.jpg | 24.71dB
22-11-07 02:28:34.081 : --45-->   0045.jpg | 21.69dB
22-11-07 02:28:37.857 : --46-->   0046.jpg | 23.38dB
22-11-07 02:28:41.632 : --47-->   0047.jpg | 24.50dB
22-11-07 02:28:44.296 : --48-->   0048.jpg | 28.79dB
22-11-07 02:28:48.427 : --49-->   0049.jpg | 28.55dB
22-11-07 02:28:53.228 : --50-->   0050.jpg | 21.34dB
22-11-07 02:28:56.916 : --51-->   0051.jpg | 26.22dB
22-11-07 02:29:01.521 : --52-->   0052.jpg | 23.68dB
22-11-07 02:29:03.128 : --53-->   0053.jpg | 24.66dB
22-11-07 02:29:04.860 : --54-->   0054.jpg | 22.17dB
22-11-07 02:29:09.089 : --55-->   0055.jpg | 36.66dB
22-11-07 02:29:11.352 : --56-->   0056.jpg | 24.75dB
22-11-07 02:29:12.562 : --57-->   0057.jpg | 30.71dB
22-11-07 02:29:14.905 : --58-->   0058.jpg | 26.95dB
22-11-07 02:29:17.587 : --59-->   0059.jpg | 28.07dB
22-11-07 02:29:21.900 : --60-->   0060.jpg | 25.04dB
22-11-07 02:29:25.863 : --61-->   0061.jpg | 26.47dB
22-11-07 02:29:31.312 : --62-->   0062.jpg | 24.54dB
22-11-07 02:29:35.487 : --63-->   0063.jpg | 23.29dB
22-11-07 02:29:41.641 : --64-->   0064.jpg | 29.14dB
22-11-07 02:29:46.664 : --65-->   0065.jpg | 35.31dB
22-11-07 02:29:48.307 : --66-->   0066.jpg | 30.13dB
22-11-07 02:29:51.096 : --67-->   0067.jpg | 26.85dB
22-11-07 02:29:54.862 : --68-->   0068.jpg | 25.30dB
22-11-07 02:29:58.865 : --69-->   0069.jpg | 19.46dB
22-11-07 02:30:01.406 : --70-->   0070.jpg | 21.21dB
22-11-07 02:30:04.736 : --71-->   0071.jpg | 22.58dB
22-11-07 02:30:09.651 : --72-->   0072.jpg | 28.27dB
22-11-07 02:30:15.021 : --73-->   0073.jpg | 27.39dB
22-11-07 02:30:20.036 : --74-->   0074.jpg | 24.75dB
22-11-07 02:30:24.414 : --75-->   0075.jpg | 24.66dB
22-11-07 02:30:26.104 : --76-->   0076.jpg | 25.43dB
22-11-07 02:30:31.419 : --77-->   0077.jpg | 22.53dB
22-11-07 02:30:34.730 : --78-->   0078.jpg | 28.04dB
22-11-07 02:30:38.219 : --79-->   0079.jpg | 20.51dB
22-11-07 02:30:42.443 : --80-->   0080.jpg | 30.28dB
22-11-07 02:30:45.072 : --81-->   0081.jpg | 28.71dB
22-11-07 02:30:48.065 : --82-->   0082.jpg | 28.52dB
22-11-07 02:30:54.031 : --83-->   0083.jpg | 23.25dB
22-11-07 02:30:58.575 : --84-->   0084.jpg | 29.02dB
22-11-07 02:31:02.380 : --85-->   0085.jpg | 21.05dB
22-11-07 02:31:06.395 : --86-->   0086.jpg | 24.61dB
22-11-07 02:31:09.322 : --87-->   0087.jpg | 20.82dB
22-11-07 02:31:14.702 : --88-->   0088.jpg | 27.31dB
22-11-07 02:31:19.008 : --89-->   0089.jpg | 23.78dB
22-11-07 02:31:22.287 : --90-->   0090.jpg | 21.85dB
22-11-07 02:31:25.964 : --91-->   0091.jpg | 31.04dB
22-11-07 02:31:29.637 : --92-->   0092.jpg | 22.71dB
22-11-07 02:31:33.408 : --93-->   0093.jpg | 27.82dB
22-11-07 02:31:39.827 : --94-->   0094.jpg | 30.60dB
22-11-07 02:31:45.178 : --95-->   0095.jpg | 25.21dB
22-11-07 02:31:49.333 : --96-->   0096.jpg | 20.92dB
22-11-07 02:31:52.870 : --97-->   0097.jpg | 32.60dB
22-11-07 02:31:57.302 : --98-->   0098.jpg | 25.74dB
22-11-07 02:32:01.694 : --99-->   0099.jpg | 24.92dB
22-11-07 02:32:04.953 : -100-->   0100.jpg | 23.59dB
22-11-07 02:32:05.265 : <epoch:589, iter: 175,000, Average PSNR : 26.43dB

22-11-07 02:33:58.429 : <epoch:589, iter: 175,200, lr:2.000e-04> G_loss: 2.453e-02 
22-11-07 02:35:51.506 : <epoch:590, iter: 175,400, lr:2.000e-04> G_loss: 3.567e-02 
22-11-07 02:37:51.693 : <epoch:591, iter: 175,600, lr:2.000e-04> G_loss: 3.065e-02 
22-11-07 02:39:46.316 : <epoch:591, iter: 175,800, lr:2.000e-04> G_loss: 3.159e-02 
22-11-07 02:41:42.661 : <epoch:592, iter: 176,000, lr:2.000e-04> G_loss: 3.032e-02 
22-11-07 02:43:43.488 : <epoch:593, iter: 176,200, lr:2.000e-04> G_loss: 3.382e-02 
22-11-07 02:45:42.449 : <epoch:593, iter: 176,400, lr:2.000e-04> G_loss: 3.163e-02 
22-11-07 02:47:42.073 : <epoch:594, iter: 176,600, lr:2.000e-04> G_loss: 3.451e-02 
22-11-07 02:49:36.650 : <epoch:595, iter: 176,800, lr:2.000e-04> G_loss: 3.059e-02 
22-11-07 02:51:26.336 : <epoch:595, iter: 177,000, lr:2.000e-04> G_loss: 3.141e-02 
22-11-07 02:53:24.581 : <epoch:596, iter: 177,200, lr:2.000e-04> G_loss: 3.048e-02 
22-11-07 02:55:17.442 : <epoch:597, iter: 177,400, lr:2.000e-04> G_loss: 2.983e-02 
22-11-07 02:57:03.268 : <epoch:597, iter: 177,600, lr:2.000e-04> G_loss: 3.147e-02 
22-11-07 02:58:59.255 : <epoch:598, iter: 177,800, lr:2.000e-04> G_loss: 3.136e-02 
22-11-07 03:00:53.746 : <epoch:599, iter: 178,000, lr:2.000e-04> G_loss: 3.507e-02 
22-11-07 03:02:39.146 : <epoch:599, iter: 178,200, lr:2.000e-04> G_loss: 2.880e-02 
22-11-07 03:04:37.089 : <epoch:600, iter: 178,400, lr:2.000e-04> G_loss: 3.015e-02 
22-11-07 03:06:28.922 : <epoch:601, iter: 178,600, lr:2.000e-04> G_loss: 3.836e-02 
22-11-07 03:08:20.335 : <epoch:602, iter: 178,800, lr:2.000e-04> G_loss: 3.330e-02 
22-11-07 03:10:06.963 : <epoch:602, iter: 179,000, lr:2.000e-04> G_loss: 3.482e-02 
22-11-07 03:11:55.549 : <epoch:603, iter: 179,200, lr:2.000e-04> G_loss: 3.323e-02 
22-11-07 03:13:49.530 : <epoch:604, iter: 179,400, lr:2.000e-04> G_loss: 3.562e-02 
22-11-07 03:15:39.488 : <epoch:604, iter: 179,600, lr:2.000e-04> G_loss: 3.630e-02 
22-11-07 03:17:34.197 : <epoch:605, iter: 179,800, lr:2.000e-04> G_loss: 2.868e-02 
22-11-07 03:19:29.271 : <epoch:606, iter: 180,000, lr:2.000e-04> G_loss: 2.615e-02 
22-11-07 03:19:29.285 : Saving the model.
22-11-07 03:19:31.846 : ---1-->   0001.jpg | 25.21dB
22-11-07 03:19:37.634 : ---2-->   0002.jpg | 25.73dB
22-11-07 03:19:43.099 : ---3-->   0003.jpg | 29.83dB
22-11-07 03:19:46.339 : ---4-->   0004.jpg | 28.45dB
22-11-07 03:19:50.291 : ---5-->   0005.jpg | 29.34dB
22-11-07 03:19:53.978 : ---6-->   0006.jpg | 33.44dB
22-11-07 03:19:58.550 : ---7-->   0007.jpg | 35.78dB
22-11-07 03:20:02.486 : ---8-->   0008.jpg | 24.34dB
22-11-07 03:20:06.994 : ---9-->   0009.jpg | 23.55dB
22-11-07 03:20:12.053 : --10-->   0010.jpg | 28.39dB
22-11-07 03:20:15.297 : --11-->   0011.jpg | 26.49dB
22-11-07 03:20:18.897 : --12-->   0012.jpg | 24.41dB
22-11-07 03:20:22.287 : --13-->   0013.jpg | 24.28dB
22-11-07 03:20:26.712 : --14-->   0014.jpg | 23.58dB
22-11-07 03:20:31.233 : --15-->   0015.jpg | 21.89dB
22-11-07 03:20:36.236 : --16-->   0016.jpg | 30.12dB
22-11-07 03:20:39.653 : --17-->   0017.jpg | 29.76dB
22-11-07 03:20:41.827 : --18-->   0018.jpg | 27.35dB
22-11-07 03:20:45.536 : --19-->   0019.jpg | 32.62dB
22-11-07 03:20:48.327 : --20-->   0020.jpg | 24.07dB
22-11-07 03:20:51.802 : --21-->   0021.jpg | 26.67dB
22-11-07 03:20:54.927 : --22-->   0022.jpg | 39.11dB
22-11-07 03:20:56.469 : --23-->   0023.jpg | 25.66dB
22-11-07 03:20:59.709 : --24-->   0024.jpg | 34.59dB
22-11-07 03:21:01.746 : --25-->   0025.jpg | 26.16dB
22-11-07 03:21:05.507 : --26-->   0026.jpg | 27.33dB
22-11-07 03:21:08.338 : --27-->   0027.jpg | 29.10dB
22-11-07 03:21:12.354 : --28-->   0028.jpg | 30.33dB
22-11-07 03:21:15.380 : --29-->   0029.jpg | 24.25dB
22-11-07 03:21:19.354 : --30-->   0030.jpg | 29.37dB
22-11-07 03:21:23.371 : --31-->   0031.jpg | 28.30dB
22-11-07 03:21:26.131 : --32-->   0032.jpg | 26.29dB
22-11-07 03:21:28.560 : --33-->   0033.jpg | 23.89dB
22-11-07 03:21:33.345 : --34-->   0034.jpg | 23.46dB
22-11-07 03:21:37.938 : --35-->   0035.jpg | 27.17dB
22-11-07 03:21:41.560 : --36-->   0036.jpg | 23.33dB
22-11-07 03:21:45.334 : --37-->   0037.jpg | 23.04dB
22-11-07 03:21:48.581 : --38-->   0038.jpg | 26.91dB
22-11-07 03:21:50.804 : --39-->   0039.jpg | 24.28dB
22-11-07 03:21:54.019 : --40-->   0040.jpg | 28.25dB
22-11-07 03:21:58.435 : --41-->   0041.jpg | 20.94dB
22-11-07 03:22:02.017 : --42-->   0042.jpg | 26.71dB
22-11-07 03:22:05.820 : --43-->   0043.jpg | 27.48dB
22-11-07 03:22:08.266 : --44-->   0044.jpg | 24.72dB
22-11-07 03:22:10.703 : --45-->   0045.jpg | 21.69dB
22-11-07 03:22:14.983 : --46-->   0046.jpg | 23.34dB
22-11-07 03:22:18.605 : --47-->   0047.jpg | 24.47dB
22-11-07 03:22:23.833 : --48-->   0048.jpg | 28.70dB
22-11-07 03:22:27.041 : --49-->   0049.jpg | 28.63dB
22-11-07 03:22:30.109 : --50-->   0050.jpg | 21.30dB
22-11-07 03:22:33.176 : --51-->   0051.jpg | 26.28dB
22-11-07 03:22:36.240 : --52-->   0052.jpg | 23.65dB
22-11-07 03:22:39.727 : --53-->   0053.jpg | 24.82dB
22-11-07 03:22:42.182 : --54-->   0054.jpg | 22.18dB
22-11-07 03:22:47.631 : --55-->   0055.jpg | 36.41dB
22-11-07 03:22:50.871 : --56-->   0056.jpg | 24.79dB
22-11-07 03:22:54.962 : --57-->   0057.jpg | 30.72dB
22-11-07 03:22:59.723 : --58-->   0058.jpg | 26.85dB
22-11-07 03:23:03.812 : --59-->   0059.jpg | 28.00dB
22-11-07 03:23:06.302 : --60-->   0060.jpg | 25.18dB
22-11-07 03:23:08.854 : --61-->   0061.jpg | 26.68dB
22-11-07 03:23:12.855 : --62-->   0062.jpg | 24.51dB
22-11-07 03:23:16.923 : --63-->   0063.jpg | 23.30dB
22-11-07 03:23:20.956 : --64-->   0064.jpg | 29.20dB
22-11-07 03:23:26.485 : --65-->   0065.jpg | 35.05dB
22-11-07 03:23:30.438 : --66-->   0066.jpg | 30.28dB
22-11-07 03:23:32.776 : --67-->   0067.jpg | 26.76dB
22-11-07 03:23:36.389 : --68-->   0068.jpg | 25.30dB
22-11-07 03:23:41.135 : --69-->   0069.jpg | 19.47dB
22-11-07 03:23:46.180 : --70-->   0070.jpg | 21.21dB
22-11-07 03:23:50.188 : --71-->   0071.jpg | 22.58dB
22-11-07 03:23:54.318 : --72-->   0072.jpg | 28.27dB
22-11-07 03:23:58.499 : --73-->   0073.jpg | 27.55dB
22-11-07 03:24:01.342 : --74-->   0074.jpg | 24.67dB
22-11-07 03:24:05.674 : --75-->   0075.jpg | 24.63dB
22-11-07 03:24:08.786 : --76-->   0076.jpg | 25.43dB
22-11-07 03:24:12.243 : --77-->   0077.jpg | 22.57dB
22-11-07 03:24:16.841 : --78-->   0078.jpg | 28.07dB
22-11-07 03:24:20.868 : --79-->   0079.jpg | 20.53dB
22-11-07 03:24:22.108 : --80-->   0080.jpg | 30.29dB
22-11-07 03:24:23.768 : --81-->   0081.jpg | 28.71dB
22-11-07 03:24:24.957 : --82-->   0082.jpg | 28.56dB
22-11-07 03:24:27.905 : --83-->   0083.jpg | 23.41dB
22-11-07 03:24:32.078 : --84-->   0084.jpg | 29.28dB
22-11-07 03:24:34.767 : --85-->   0085.jpg | 20.95dB
22-11-07 03:24:37.455 : --86-->   0086.jpg | 24.60dB
22-11-07 03:24:38.795 : --87-->   0087.jpg | 20.92dB
22-11-07 03:24:42.394 : --88-->   0088.jpg | 27.30dB
22-11-07 03:24:46.078 : --89-->   0089.jpg | 23.90dB
22-11-07 03:24:51.837 : --90-->   0090.jpg | 21.84dB
22-11-07 03:24:55.048 : --91-->   0091.jpg | 31.12dB
22-11-07 03:24:57.266 : --92-->   0092.jpg | 22.55dB
22-11-07 03:25:02.195 : --93-->   0093.jpg | 27.87dB
22-11-07 03:25:07.011 : --94-->   0094.jpg | 30.72dB
22-11-07 03:25:08.883 : --95-->   0095.jpg | 25.17dB
22-11-07 03:25:12.975 : --96-->   0096.jpg | 20.92dB
22-11-07 03:25:17.772 : --97-->   0097.jpg | 32.63dB
22-11-07 03:25:23.382 : --98-->   0098.jpg | 25.69dB
22-11-07 03:25:26.609 : --99-->   0099.jpg | 24.94dB
22-11-07 03:25:31.452 : -100-->   0100.jpg | 23.56dB
22-11-07 03:25:31.721 : <epoch:606, iter: 180,000, Average PSNR : 26.44dB

22-11-07 03:27:18.405 : <epoch:606, iter: 180,200, lr:2.000e-04> G_loss: 2.602e-02 
22-11-07 03:29:09.119 : <epoch:607, iter: 180,400, lr:2.000e-04> G_loss: 3.806e-02 
22-11-07 03:30:58.327 : <epoch:608, iter: 180,600, lr:2.000e-04> G_loss: 3.663e-02 
22-11-07 03:32:46.507 : <epoch:608, iter: 180,800, lr:2.000e-04> G_loss: 3.249e-02 
22-11-07 03:34:37.266 : <epoch:609, iter: 181,000, lr:2.000e-04> G_loss: 2.871e-02 
22-11-07 03:36:30.082 : <epoch:610, iter: 181,200, lr:2.000e-04> G_loss: 3.587e-02 
22-11-07 03:38:20.916 : <epoch:610, iter: 181,400, lr:2.000e-04> G_loss: 3.289e-02 
22-11-07 03:40:11.653 : <epoch:611, iter: 181,600, lr:2.000e-04> G_loss: 2.869e-02 
22-11-07 03:42:04.139 : <epoch:612, iter: 181,800, lr:2.000e-04> G_loss: 2.361e-02 
22-11-07 03:43:49.344 : <epoch:612, iter: 182,000, lr:2.000e-04> G_loss: 3.723e-02 
22-11-07 03:45:40.408 : <epoch:613, iter: 182,200, lr:2.000e-04> G_loss: 2.791e-02 
22-11-07 03:47:39.463 : <epoch:614, iter: 182,400, lr:2.000e-04> G_loss: 2.939e-02 
22-11-07 03:49:28.754 : <epoch:614, iter: 182,600, lr:2.000e-04> G_loss: 3.441e-02 
22-11-07 03:51:18.562 : <epoch:615, iter: 182,800, lr:2.000e-04> G_loss: 4.175e-02 
22-11-07 03:53:12.674 : <epoch:616, iter: 183,000, lr:2.000e-04> G_loss: 2.727e-02 
22-11-07 03:54:56.928 : <epoch:616, iter: 183,200, lr:2.000e-04> G_loss: 3.019e-02 
22-11-07 03:56:48.756 : <epoch:617, iter: 183,400, lr:2.000e-04> G_loss: 2.607e-02 
22-11-07 03:58:39.541 : <epoch:618, iter: 183,600, lr:2.000e-04> G_loss: 3.328e-02 
22-11-07 04:00:33.488 : <epoch:618, iter: 183,800, lr:2.000e-04> G_loss: 3.246e-02 
22-11-07 04:02:27.800 : <epoch:619, iter: 184,000, lr:2.000e-04> G_loss: 3.661e-02 
22-11-07 04:04:19.531 : <epoch:620, iter: 184,200, lr:2.000e-04> G_loss: 2.977e-02 
22-11-07 04:06:03.142 : <epoch:620, iter: 184,400, lr:2.000e-04> G_loss: 3.328e-02 
22-11-07 04:07:52.355 : <epoch:621, iter: 184,600, lr:2.000e-04> G_loss: 4.477e-02 
22-11-07 04:09:38.873 : <epoch:622, iter: 184,800, lr:2.000e-04> G_loss: 2.360e-02 
22-11-07 04:11:21.908 : <epoch:622, iter: 185,000, lr:2.000e-04> G_loss: 3.428e-02 
22-11-07 04:11:21.909 : Saving the model.
22-11-07 04:11:26.057 : ---1-->   0001.jpg | 25.30dB
22-11-07 04:11:29.304 : ---2-->   0002.jpg | 25.83dB
22-11-07 04:11:33.463 : ---3-->   0003.jpg | 30.15dB
22-11-07 04:11:37.698 : ---4-->   0004.jpg | 28.43dB
22-11-07 04:11:41.973 : ---5-->   0005.jpg | 29.17dB
22-11-07 04:11:45.986 : ---6-->   0006.jpg | 33.48dB
22-11-07 04:11:48.946 : ---7-->   0007.jpg | 35.59dB
22-11-07 04:11:54.306 : ---8-->   0008.jpg | 24.26dB
22-11-07 04:11:58.470 : ---9-->   0009.jpg | 23.56dB
22-11-07 04:12:02.379 : --10-->   0010.jpg | 28.26dB
22-11-07 04:12:05.268 : --11-->   0011.jpg | 26.53dB
22-11-07 04:12:08.967 : --12-->   0012.jpg | 24.36dB
22-11-07 04:12:11.132 : --13-->   0013.jpg | 24.13dB
22-11-07 04:12:13.878 : --14-->   0014.jpg | 23.54dB
22-11-07 04:12:18.608 : --15-->   0015.jpg | 21.91dB
22-11-07 04:12:21.507 : --16-->   0016.jpg | 30.24dB
22-11-07 04:12:24.339 : --17-->   0017.jpg | 29.65dB
22-11-07 04:12:29.151 : --18-->   0018.jpg | 26.96dB
22-11-07 04:12:30.662 : --19-->   0019.jpg | 32.42dB
22-11-07 04:12:31.904 : --20-->   0020.jpg | 23.99dB
22-11-07 04:12:33.835 : --21-->   0021.jpg | 26.49dB
22-11-07 04:12:35.632 : --22-->   0022.jpg | 39.03dB
22-11-07 04:12:39.104 : --23-->   0023.jpg | 25.71dB
22-11-07 04:12:43.808 : --24-->   0024.jpg | 34.53dB
22-11-07 04:12:46.324 : --25-->   0025.jpg | 26.18dB
22-11-07 04:12:50.258 : --26-->   0026.jpg | 27.27dB
22-11-07 04:12:55.744 : --27-->   0027.jpg | 29.26dB
22-11-07 04:12:58.972 : --28-->   0028.jpg | 30.51dB
22-11-07 04:13:01.811 : --29-->   0029.jpg | 24.24dB
22-11-07 04:13:04.226 : --30-->   0030.jpg | 29.37dB
22-11-07 04:13:05.625 : --31-->   0031.jpg | 28.25dB
22-11-07 04:13:07.751 : --32-->   0032.jpg | 26.39dB
22-11-07 04:13:10.817 : --33-->   0033.jpg | 23.78dB
22-11-07 04:13:13.321 : --34-->   0034.jpg | 23.47dB
22-11-07 04:13:17.401 : --35-->   0035.jpg | 27.05dB
22-11-07 04:13:20.639 : --36-->   0036.jpg | 23.33dB
22-11-07 04:13:22.523 : --37-->   0037.jpg | 23.02dB
22-11-07 04:13:25.653 : --38-->   0038.jpg | 26.80dB
22-11-07 04:13:29.021 : --39-->   0039.jpg | 24.35dB
22-11-07 04:13:32.369 : --40-->   0040.jpg | 28.24dB
22-11-07 04:13:37.171 : --41-->   0041.jpg | 20.97dB
22-11-07 04:13:41.186 : --42-->   0042.jpg | 26.79dB
22-11-07 04:13:44.363 : --43-->   0043.jpg | 27.54dB
22-11-07 04:13:47.201 : --44-->   0044.jpg | 24.71dB
22-11-07 04:13:50.422 : --45-->   0045.jpg | 21.67dB
22-11-07 04:13:54.451 : --46-->   0046.jpg | 23.36dB
22-11-07 04:13:58.855 : --47-->   0047.jpg | 24.54dB
22-11-07 04:14:01.784 : --48-->   0048.jpg | 28.72dB
22-11-07 04:14:05.059 : --49-->   0049.jpg | 28.69dB
22-11-07 04:14:06.434 : --50-->   0050.jpg | 21.26dB
22-11-07 04:14:08.742 : --51-->   0051.jpg | 26.28dB
22-11-07 04:14:12.959 : --52-->   0052.jpg | 23.65dB
22-11-07 04:14:15.823 : --53-->   0053.jpg | 24.79dB
22-11-07 04:14:20.024 : --54-->   0054.jpg | 22.16dB
22-11-07 04:14:24.701 : --55-->   0055.jpg | 36.53dB
22-11-07 04:14:28.389 : --56-->   0056.jpg | 24.51dB
22-11-07 04:14:30.161 : --57-->   0057.jpg | 30.65dB
22-11-07 04:14:33.200 : --58-->   0058.jpg | 26.87dB
22-11-07 04:14:37.360 : --59-->   0059.jpg | 28.15dB
22-11-07 04:14:39.561 : --60-->   0060.jpg | 24.97dB
22-11-07 04:14:42.054 : --61-->   0061.jpg | 26.82dB
22-11-07 04:14:47.092 : --62-->   0062.jpg | 24.48dB
22-11-07 04:14:49.692 : --63-->   0063.jpg | 23.24dB
22-11-07 04:14:53.966 : --64-->   0064.jpg | 29.26dB
22-11-07 04:14:58.471 : --65-->   0065.jpg | 35.25dB
22-11-07 04:15:00.056 : --66-->   0066.jpg | 30.29dB
22-11-07 04:15:03.723 : --67-->   0067.jpg | 26.91dB
22-11-07 04:15:06.900 : --68-->   0068.jpg | 25.30dB
22-11-07 04:15:10.610 : --69-->   0069.jpg | 19.46dB
22-11-07 04:15:13.518 : --70-->   0070.jpg | 21.26dB
22-11-07 04:15:16.747 : --71-->   0071.jpg | 22.59dB
22-11-07 04:15:19.838 : --72-->   0072.jpg | 28.20dB
22-11-07 04:15:22.905 : --73-->   0073.jpg | 27.42dB
22-11-07 04:15:27.178 : --74-->   0074.jpg | 24.66dB
22-11-07 04:15:30.659 : --75-->   0075.jpg | 24.63dB
22-11-07 04:15:34.075 : --76-->   0076.jpg | 25.38dB
22-11-07 04:15:38.948 : --77-->   0077.jpg | 22.58dB
22-11-07 04:15:41.141 : --78-->   0078.jpg | 28.04dB
22-11-07 04:15:44.160 : --79-->   0079.jpg | 20.53dB
22-11-07 04:15:47.533 : --80-->   0080.jpg | 30.35dB
22-11-07 04:15:51.204 : --81-->   0081.jpg | 28.71dB
22-11-07 04:15:53.217 : --82-->   0082.jpg | 28.55dB
22-11-07 04:15:55.302 : --83-->   0083.jpg | 23.30dB
22-11-07 04:15:58.365 : --84-->   0084.jpg | 29.11dB
22-11-07 04:16:02.769 : --85-->   0085.jpg | 20.82dB
22-11-07 04:16:05.082 : --86-->   0086.jpg | 24.56dB
22-11-07 04:16:06.839 : --87-->   0087.jpg | 21.07dB
22-11-07 04:16:11.264 : --88-->   0088.jpg | 27.18dB
22-11-07 04:16:13.946 : --89-->   0089.jpg | 23.88dB
22-11-07 04:16:18.785 : --90-->   0090.jpg | 21.84dB
22-11-07 04:16:23.466 : --91-->   0091.jpg | 31.29dB
22-11-07 04:16:25.204 : --92-->   0092.jpg | 22.52dB
22-11-07 04:16:28.805 : --93-->   0093.jpg | 27.77dB
22-11-07 04:16:33.305 : --94-->   0094.jpg | 30.69dB
22-11-07 04:16:36.688 : --95-->   0095.jpg | 25.16dB
22-11-07 04:16:41.274 : --96-->   0096.jpg | 20.91dB
22-11-07 04:16:43.258 : --97-->   0097.jpg | 32.60dB
22-11-07 04:16:46.939 : --98-->   0098.jpg | 25.66dB
22-11-07 04:16:50.527 : --99-->   0099.jpg | 24.97dB
22-11-07 04:16:53.154 : -100-->   0100.jpg | 23.60dB
22-11-07 04:16:53.415 : <epoch:622, iter: 185,000, Average PSNR : 26.43dB

22-11-07 04:18:36.852 : <epoch:623, iter: 185,200, lr:2.000e-04> G_loss: 3.462e-02 
22-11-07 04:20:21.249 : <epoch:624, iter: 185,400, lr:2.000e-04> G_loss: 3.004e-02 
22-11-07 04:22:06.623 : <epoch:624, iter: 185,600, lr:2.000e-04> G_loss: 3.918e-02 
22-11-07 04:23:52.396 : <epoch:625, iter: 185,800, lr:2.000e-04> G_loss: 3.095e-02 
22-11-07 04:25:39.511 : <epoch:626, iter: 186,000, lr:2.000e-04> G_loss: 3.553e-02 
22-11-07 04:27:20.878 : <epoch:626, iter: 186,200, lr:2.000e-04> G_loss: 3.736e-02 
22-11-07 04:29:04.372 : <epoch:627, iter: 186,400, lr:2.000e-04> G_loss: 3.144e-02 
22-11-07 04:30:46.204 : <epoch:628, iter: 186,600, lr:2.000e-04> G_loss: 3.994e-02 
22-11-07 04:32:18.591 : <epoch:628, iter: 186,800, lr:2.000e-04> G_loss: 3.225e-02 
22-11-07 04:33:51.262 : <epoch:629, iter: 187,000, lr:2.000e-04> G_loss: 3.402e-02 
22-11-07 04:35:26.008 : <epoch:630, iter: 187,200, lr:2.000e-04> G_loss: 3.665e-02 
22-11-07 04:36:50.740 : <epoch:630, iter: 187,400, lr:2.000e-04> G_loss: 3.657e-02 
22-11-07 04:38:20.989 : <epoch:631, iter: 187,600, lr:2.000e-04> G_loss: 2.905e-02 
22-11-07 04:39:53.928 : <epoch:632, iter: 187,800, lr:2.000e-04> G_loss: 2.952e-02 
22-11-07 04:41:16.533 : <epoch:632, iter: 188,000, lr:2.000e-04> G_loss: 2.607e-02 
22-11-07 04:42:47.187 : <epoch:633, iter: 188,200, lr:2.000e-04> G_loss: 2.792e-02 
22-11-07 04:44:04.539 : <epoch:634, iter: 188,400, lr:2.000e-04> G_loss: 3.485e-02 
22-11-07 04:45:23.300 : <epoch:635, iter: 188,600, lr:2.000e-04> G_loss: 2.385e-02 
22-11-07 04:46:38.126 : <epoch:635, iter: 188,800, lr:2.000e-04> G_loss: 4.091e-02 
22-11-07 04:47:54.038 : <epoch:636, iter: 189,000, lr:2.000e-04> G_loss: 2.509e-02 
22-11-07 04:49:10.842 : <epoch:637, iter: 189,200, lr:2.000e-04> G_loss: 3.462e-02 
22-11-07 04:50:25.152 : <epoch:637, iter: 189,400, lr:2.000e-04> G_loss: 3.017e-02 
22-11-07 04:51:40.043 : <epoch:638, iter: 189,600, lr:2.000e-04> G_loss: 4.323e-02 
22-11-07 04:52:55.263 : <epoch:639, iter: 189,800, lr:2.000e-04> G_loss: 3.100e-02 
22-11-07 04:54:09.844 : <epoch:639, iter: 190,000, lr:2.000e-04> G_loss: 2.963e-02 
22-11-07 04:54:09.855 : Saving the model.
22-11-07 04:54:11.913 : ---1-->   0001.jpg | 25.18dB
22-11-07 04:54:13.751 : ---2-->   0002.jpg | 25.80dB
22-11-07 04:54:15.131 : ---3-->   0003.jpg | 30.05dB
22-11-07 04:54:16.943 : ---4-->   0004.jpg | 28.33dB
22-11-07 04:54:18.294 : ---5-->   0005.jpg | 29.25dB
22-11-07 04:54:19.652 : ---6-->   0006.jpg | 33.68dB
22-11-07 04:54:20.820 : ---7-->   0007.jpg | 35.71dB
22-11-07 04:54:22.695 : ---8-->   0008.jpg | 24.27dB
22-11-07 04:54:24.265 : ---9-->   0009.jpg | 23.51dB
22-11-07 04:54:26.758 : --10-->   0010.jpg | 28.35dB
22-11-07 04:54:28.601 : --11-->   0011.jpg | 26.30dB
22-11-07 04:54:30.444 : --12-->   0012.jpg | 24.16dB
22-11-07 04:54:32.307 : --13-->   0013.jpg | 24.20dB
22-11-07 04:54:33.866 : --14-->   0014.jpg | 23.49dB
22-11-07 04:54:35.015 : --15-->   0015.jpg | 21.85dB
22-11-07 04:54:36.424 : --16-->   0016.jpg | 30.15dB
22-11-07 04:54:37.553 : --17-->   0017.jpg | 29.58dB
22-11-07 04:54:39.219 : --18-->   0018.jpg | 27.19dB
22-11-07 04:54:40.891 : --19-->   0019.jpg | 32.56dB
22-11-07 04:54:42.538 : --20-->   0020.jpg | 24.01dB
22-11-07 04:54:44.009 : --21-->   0021.jpg | 26.65dB
22-11-07 04:54:45.396 : --22-->   0022.jpg | 39.29dB
22-11-07 04:54:46.509 : --23-->   0023.jpg | 25.70dB
22-11-07 04:54:47.966 : --24-->   0024.jpg | 34.46dB
22-11-07 04:54:49.752 : --25-->   0025.jpg | 26.14dB
22-11-07 04:54:51.038 : --26-->   0026.jpg | 27.16dB
22-11-07 04:54:52.816 : --27-->   0027.jpg | 29.08dB
22-11-07 04:54:54.128 : --28-->   0028.jpg | 30.33dB
22-11-07 04:54:54.798 : --29-->   0029.jpg | 24.25dB
22-11-07 04:54:55.842 : --30-->   0030.jpg | 29.32dB
22-11-07 04:54:56.847 : --31-->   0031.jpg | 28.27dB
22-11-07 04:54:58.381 : --32-->   0032.jpg | 26.31dB
22-11-07 04:55:00.002 : --33-->   0033.jpg | 23.77dB
22-11-07 04:55:01.590 : --34-->   0034.jpg | 23.38dB
22-11-07 04:55:02.646 : --35-->   0035.jpg | 27.04dB
22-11-07 04:55:04.426 : --36-->   0036.jpg | 23.31dB
22-11-07 04:55:06.191 : --37-->   0037.jpg | 23.10dB
22-11-07 04:55:07.998 : --38-->   0038.jpg | 26.81dB
22-11-07 04:55:09.550 : --39-->   0039.jpg | 24.28dB
22-11-07 04:55:11.563 : --40-->   0040.jpg | 28.19dB
22-11-07 04:55:12.942 : --41-->   0041.jpg | 20.97dB
22-11-07 04:55:14.238 : --42-->   0042.jpg | 26.80dB
22-11-07 04:55:15.882 : --43-->   0043.jpg | 27.48dB
22-11-07 04:55:17.063 : --44-->   0044.jpg | 24.75dB
22-11-07 04:55:18.513 : --45-->   0045.jpg | 21.65dB
22-11-07 04:55:20.146 : --46-->   0046.jpg | 23.38dB
22-11-07 04:55:22.151 : --47-->   0047.jpg | 24.36dB
22-11-07 04:55:23.577 : --48-->   0048.jpg | 28.67dB
22-11-07 04:55:24.934 : --49-->   0049.jpg | 28.65dB
22-11-07 04:55:25.981 : --50-->   0050.jpg | 21.26dB
22-11-07 04:55:26.922 : --51-->   0051.jpg | 26.26dB
22-11-07 04:55:28.672 : --52-->   0052.jpg | 23.66dB
22-11-07 04:55:30.029 : --53-->   0053.jpg | 24.65dB
22-11-07 04:55:31.162 : --54-->   0054.jpg | 22.13dB
22-11-07 04:55:32.751 : --55-->   0055.jpg | 36.55dB
22-11-07 04:55:33.793 : --56-->   0056.jpg | 24.68dB
22-11-07 04:55:35.163 : --57-->   0057.jpg | 30.69dB
22-11-07 04:55:36.954 : --58-->   0058.jpg | 26.89dB
22-11-07 04:55:37.978 : --59-->   0059.jpg | 28.04dB
22-11-07 04:55:39.681 : --60-->   0060.jpg | 24.91dB
22-11-07 04:55:41.157 : --61-->   0061.jpg | 26.73dB
22-11-07 04:55:42.418 : --62-->   0062.jpg | 24.56dB
22-11-07 04:55:44.159 : --63-->   0063.jpg | 23.23dB
22-11-07 04:55:45.556 : --64-->   0064.jpg | 29.15dB
22-11-07 04:55:47.501 : --65-->   0065.jpg | 35.16dB
22-11-07 04:55:48.626 : --66-->   0066.jpg | 30.26dB
22-11-07 04:55:50.735 : --67-->   0067.jpg | 26.82dB
22-11-07 04:55:51.978 : --68-->   0068.jpg | 25.33dB
22-11-07 04:55:53.555 : --69-->   0069.jpg | 19.48dB
22-11-07 04:55:54.760 : --70-->   0070.jpg | 21.23dB
22-11-07 04:55:55.993 : --71-->   0071.jpg | 22.57dB
22-11-07 04:55:57.358 : --72-->   0072.jpg | 28.30dB
22-11-07 04:55:59.001 : --73-->   0073.jpg | 27.43dB
22-11-07 04:55:59.841 : --74-->   0074.jpg | 24.46dB
22-11-07 04:56:01.227 : --75-->   0075.jpg | 24.59dB
22-11-07 04:56:03.051 : --76-->   0076.jpg | 25.35dB
22-11-07 04:56:04.520 : --77-->   0077.jpg | 22.52dB
22-11-07 04:56:06.131 : --78-->   0078.jpg | 27.96dB
22-11-07 04:56:08.139 : --79-->   0079.jpg | 20.53dB
22-11-07 04:56:09.298 : --80-->   0080.jpg | 30.27dB
22-11-07 04:56:10.735 : --81-->   0081.jpg | 28.72dB
22-11-07 04:56:12.505 : --82-->   0082.jpg | 28.48dB
22-11-07 04:56:13.190 : --83-->   0083.jpg | 23.30dB
22-11-07 04:56:14.278 : --84-->   0084.jpg | 29.14dB
22-11-07 04:56:15.086 : --85-->   0085.jpg | 20.96dB
22-11-07 04:56:16.254 : --86-->   0086.jpg | 24.46dB
22-11-07 04:56:16.947 : --87-->   0087.jpg | 21.03dB
22-11-07 04:56:18.472 : --88-->   0088.jpg | 27.36dB
22-11-07 04:56:20.019 : --89-->   0089.jpg | 23.86dB
22-11-07 04:56:21.628 : --90-->   0090.jpg | 21.83dB
22-11-07 04:56:22.804 : --91-->   0091.jpg | 31.28dB
22-11-07 04:56:23.961 : --92-->   0092.jpg | 22.64dB
22-11-07 04:56:25.311 : --93-->   0093.jpg | 27.90dB
22-11-07 04:56:26.815 : --94-->   0094.jpg | 30.70dB
22-11-07 04:56:28.074 : --95-->   0095.jpg | 25.19dB
22-11-07 04:56:29.409 : --96-->   0096.jpg | 20.91dB
22-11-07 04:56:30.644 : --97-->   0097.jpg | 32.64dB
22-11-07 04:56:31.958 : --98-->   0098.jpg | 25.61dB
22-11-07 04:56:33.274 : --99-->   0099.jpg | 24.98dB
22-11-07 04:56:34.237 : -100-->   0100.jpg | 23.55dB
22-11-07 04:56:34.406 : <epoch:639, iter: 190,000, Average PSNR : 26.41dB

22-11-07 04:57:52.729 : <epoch:640, iter: 190,200, lr:2.000e-04> G_loss: 3.218e-02 
22-11-07 04:59:09.739 : <epoch:641, iter: 190,400, lr:2.000e-04> G_loss: 3.394e-02 
22-11-07 05:00:21.064 : <epoch:641, iter: 190,600, lr:2.000e-04> G_loss: 3.169e-02 
22-11-07 05:01:28.943 : <epoch:642, iter: 190,800, lr:2.000e-04> G_loss: 3.213e-02 
22-11-07 05:02:38.476 : <epoch:643, iter: 191,000, lr:2.000e-04> G_loss: 2.844e-02 
22-11-07 05:03:42.767 : <epoch:643, iter: 191,200, lr:2.000e-04> G_loss: 3.138e-02 
22-11-07 05:04:43.602 : <epoch:644, iter: 191,400, lr:2.000e-04> G_loss: 2.754e-02 
22-11-07 05:05:44.350 : <epoch:645, iter: 191,600, lr:2.000e-04> G_loss: 3.287e-02 
22-11-07 05:06:40.082 : <epoch:645, iter: 191,800, lr:2.000e-04> G_loss: 3.585e-02 
22-11-07 05:07:42.160 : <epoch:646, iter: 192,000, lr:2.000e-04> G_loss: 3.365e-02 
22-11-07 05:08:43.228 : <epoch:647, iter: 192,200, lr:2.000e-04> G_loss: 2.705e-02 
22-11-07 05:09:41.971 : <epoch:647, iter: 192,400, lr:2.000e-04> G_loss: 3.420e-02 
22-11-07 05:10:42.654 : <epoch:648, iter: 192,600, lr:2.000e-04> G_loss: 2.793e-02 
22-11-07 05:11:43.381 : <epoch:649, iter: 192,800, lr:2.000e-04> G_loss: 2.807e-02 
22-11-07 05:12:42.154 : <epoch:649, iter: 193,000, lr:2.000e-04> G_loss: 3.359e-02 
22-11-07 05:13:43.604 : <epoch:650, iter: 193,200, lr:2.000e-04> G_loss: 3.826e-02 
22-11-07 05:14:43.242 : <epoch:651, iter: 193,400, lr:2.000e-04> G_loss: 3.138e-02 
22-11-07 05:15:39.675 : <epoch:651, iter: 193,600, lr:2.000e-04> G_loss: 2.894e-02 
22-11-07 05:16:34.793 : <epoch:652, iter: 193,800, lr:2.000e-04> G_loss: 2.794e-02 
22-11-07 05:17:29.664 : <epoch:653, iter: 194,000, lr:2.000e-04> G_loss: 3.028e-02 
22-11-07 05:18:22.902 : <epoch:653, iter: 194,200, lr:2.000e-04> G_loss: 3.735e-02 
22-11-07 05:19:20.334 : <epoch:654, iter: 194,400, lr:2.000e-04> G_loss: 2.935e-02 
22-11-07 05:20:17.180 : <epoch:655, iter: 194,600, lr:2.000e-04> G_loss: 3.561e-02 
22-11-07 05:21:12.837 : <epoch:655, iter: 194,800, lr:2.000e-04> G_loss: 2.685e-02 
22-11-07 05:22:09.909 : <epoch:656, iter: 195,000, lr:2.000e-04> G_loss: 2.595e-02 
22-11-07 05:22:09.911 : Saving the model.
22-11-07 05:22:11.160 : ---1-->   0001.jpg | 25.25dB
22-11-07 05:22:11.782 : ---2-->   0002.jpg | 25.94dB
22-11-07 05:22:12.250 : ---3-->   0003.jpg | 29.88dB
22-11-07 05:22:12.599 : ---4-->   0004.jpg | 28.44dB
22-11-07 05:22:12.943 : ---5-->   0005.jpg | 29.44dB
22-11-07 05:22:13.295 : ---6-->   0006.jpg | 33.16dB
22-11-07 05:22:13.516 : ---7-->   0007.jpg | 35.91dB
22-11-07 05:22:14.127 : ---8-->   0008.jpg | 24.35dB
22-11-07 05:22:14.642 : ---9-->   0009.jpg | 23.53dB
22-11-07 05:22:15.288 : --10-->   0010.jpg | 28.41dB
22-11-07 05:22:15.746 : --11-->   0011.jpg | 26.68dB
22-11-07 05:22:16.232 : --12-->   0012.jpg | 24.20dB
22-11-07 05:22:16.567 : --13-->   0013.jpg | 24.14dB
22-11-07 05:22:16.826 : --14-->   0014.jpg | 23.58dB
22-11-07 05:22:17.339 : --15-->   0015.jpg | 22.05dB
22-11-07 05:22:17.635 : --16-->   0016.jpg | 30.21dB
22-11-07 05:22:17.974 : --17-->   0017.jpg | 29.63dB
22-11-07 05:22:18.335 : --18-->   0018.jpg | 27.62dB
22-11-07 05:22:18.847 : --19-->   0019.jpg | 32.72dB
22-11-07 05:22:19.189 : --20-->   0020.jpg | 23.91dB
22-11-07 05:22:19.739 : --21-->   0021.jpg | 26.68dB
22-11-07 05:22:20.155 : --22-->   0022.jpg | 39.25dB
22-11-07 05:22:20.474 : --23-->   0023.jpg | 25.59dB
22-11-07 05:22:21.072 : --24-->   0024.jpg | 34.35dB
22-11-07 05:22:21.349 : --25-->   0025.jpg | 26.15dB
22-11-07 05:22:21.681 : --26-->   0026.jpg | 27.30dB
22-11-07 05:22:22.289 : --27-->   0027.jpg | 29.07dB
22-11-07 05:22:22.704 : --28-->   0028.jpg | 30.60dB
22-11-07 05:22:23.039 : --29-->   0029.jpg | 24.24dB
22-11-07 05:22:23.486 : --30-->   0030.jpg | 29.37dB
22-11-07 05:22:23.812 : --31-->   0031.jpg | 28.32dB
22-11-07 05:22:24.135 : --32-->   0032.jpg | 26.22dB
22-11-07 05:22:24.583 : --33-->   0033.jpg | 23.74dB
22-11-07 05:22:25.134 : --34-->   0034.jpg | 23.47dB
22-11-07 05:22:25.782 : --35-->   0035.jpg | 26.94dB
22-11-07 05:22:26.461 : --36-->   0036.jpg | 23.36dB
22-11-07 05:22:26.800 : --37-->   0037.jpg | 23.02dB
22-11-07 05:22:27.087 : --38-->   0038.jpg | 26.87dB
22-11-07 05:22:27.582 : --39-->   0039.jpg | 24.34dB
22-11-07 05:22:27.974 : --40-->   0040.jpg | 28.27dB
22-11-07 05:22:28.506 : --41-->   0041.jpg | 21.21dB
22-11-07 05:22:29.004 : --42-->   0042.jpg | 26.84dB
22-11-07 05:22:29.399 : --43-->   0043.jpg | 27.57dB
22-11-07 05:22:29.847 : --44-->   0044.jpg | 24.73dB
22-11-07 05:22:30.072 : --45-->   0045.jpg | 21.68dB
22-11-07 05:22:30.420 : --46-->   0046.jpg | 23.41dB
22-11-07 05:22:30.838 : --47-->   0047.jpg | 24.51dB
22-11-07 05:22:31.025 : --48-->   0048.jpg | 28.72dB
22-11-07 05:22:31.300 : --49-->   0049.jpg | 28.65dB
22-11-07 05:22:31.642 : --50-->   0050.jpg | 21.28dB
22-11-07 05:22:31.958 : --51-->   0051.jpg | 26.21dB
22-11-07 05:22:32.307 : --52-->   0052.jpg | 23.66dB
22-11-07 05:22:32.839 : --53-->   0053.jpg | 24.70dB
22-11-07 05:22:33.270 : --54-->   0054.jpg | 22.18dB
22-11-07 05:22:33.791 : --55-->   0055.jpg | 36.77dB
22-11-07 05:22:34.007 : --56-->   0056.jpg | 24.55dB
22-11-07 05:22:34.201 : --57-->   0057.jpg | 30.70dB
22-11-07 05:22:34.636 : --58-->   0058.jpg | 26.87dB
22-11-07 05:22:34.995 : --59-->   0059.jpg | 28.08dB
22-11-07 05:22:35.380 : --60-->   0060.jpg | 25.08dB
22-11-07 05:22:35.726 : --61-->   0061.jpg | 26.66dB
22-11-07 05:22:36.030 : --62-->   0062.jpg | 24.49dB
22-11-07 05:22:36.387 : --63-->   0063.jpg | 23.26dB
22-11-07 05:22:37.090 : --64-->   0064.jpg | 29.09dB
22-11-07 05:22:37.519 : --65-->   0065.jpg | 35.32dB
22-11-07 05:22:37.874 : --66-->   0066.jpg | 30.25dB
22-11-07 05:22:38.454 : --67-->   0067.jpg | 26.90dB
22-11-07 05:22:38.816 : --68-->   0068.jpg | 25.33dB
22-11-07 05:22:39.192 : --69-->   0069.jpg | 19.50dB
22-11-07 05:22:39.482 : --70-->   0070.jpg | 21.25dB
22-11-07 05:22:39.727 : --71-->   0071.jpg | 22.59dB
22-11-07 05:22:39.998 : --72-->   0072.jpg | 28.29dB
22-11-07 05:22:40.469 : --73-->   0073.jpg | 27.38dB
22-11-07 05:22:40.727 : --74-->   0074.jpg | 24.60dB
22-11-07 05:22:41.041 : --75-->   0075.jpg | 24.65dB
22-11-07 05:22:41.354 : --76-->   0076.jpg | 25.33dB
22-11-07 05:22:41.900 : --77-->   0077.jpg | 22.55dB
22-11-07 05:22:42.118 : --78-->   0078.jpg | 28.04dB
22-11-07 05:22:42.540 : --79-->   0079.jpg | 20.51dB
22-11-07 05:22:42.729 : --80-->   0080.jpg | 30.22dB
22-11-07 05:22:43.203 : --81-->   0081.jpg | 28.70dB
22-11-07 05:22:43.615 : --82-->   0082.jpg | 28.50dB
22-11-07 05:22:44.014 : --83-->   0083.jpg | 23.30dB
22-11-07 05:22:44.516 : --84-->   0084.jpg | 29.11dB
22-11-07 05:22:44.822 : --85-->   0085.jpg | 21.05dB
22-11-07 05:22:45.268 : --86-->   0086.jpg | 24.38dB
22-11-07 05:22:45.648 : --87-->   0087.jpg | 20.97dB
22-11-07 05:22:46.331 : --88-->   0088.jpg | 27.36dB
22-11-07 05:22:46.785 : --89-->   0089.jpg | 23.84dB
22-11-07 05:22:47.113 : --90-->   0090.jpg | 21.84dB
22-11-07 05:22:47.343 : --91-->   0091.jpg | 31.40dB
22-11-07 05:22:47.711 : --92-->   0092.jpg | 22.68dB
22-11-07 05:22:47.975 : --93-->   0093.jpg | 27.86dB
22-11-07 05:22:48.742 : --94-->   0094.jpg | 30.76dB
22-11-07 05:22:49.155 : --95-->   0095.jpg | 25.20dB
22-11-07 05:22:49.711 : --96-->   0096.jpg | 20.91dB
22-11-07 05:22:50.211 : --97-->   0097.jpg | 32.61dB
22-11-07 05:22:50.593 : --98-->   0098.jpg | 25.72dB
22-11-07 05:22:50.896 : --99-->   0099.jpg | 24.95dB
22-11-07 05:22:51.152 : -100-->   0100.jpg | 23.60dB
22-11-07 05:22:51.257 : <epoch:656, iter: 195,000, Average PSNR : 26.45dB

22-11-07 05:23:48.637 : <epoch:657, iter: 195,200, lr:2.000e-04> G_loss: 2.712e-02 
22-11-07 05:24:44.617 : <epoch:657, iter: 195,400, lr:2.000e-04> G_loss: 3.245e-02 
22-11-07 05:25:31.858 : <epoch:658, iter: 195,600, lr:2.000e-04> G_loss: 2.966e-02 
22-11-07 05:26:18.312 : <epoch:659, iter: 195,800, lr:2.000e-04> G_loss: 2.414e-02 
22-11-07 05:27:01.488 : <epoch:659, iter: 196,000, lr:2.000e-04> G_loss: 3.230e-02 
22-11-07 05:27:46.632 : <epoch:660, iter: 196,200, lr:2.000e-04> G_loss: 3.378e-02 
22-11-07 05:28:31.634 : <epoch:661, iter: 196,400, lr:2.000e-04> G_loss: 3.352e-02 
22-11-07 05:29:13.852 : <epoch:661, iter: 196,600, lr:2.000e-04> G_loss: 3.265e-02 
22-11-07 05:29:59.219 : <epoch:662, iter: 196,800, lr:2.000e-04> G_loss: 3.486e-02 
22-11-07 05:30:44.798 : <epoch:663, iter: 197,000, lr:2.000e-04> G_loss: 2.783e-02 
22-11-07 05:31:52.020 : <epoch:663, iter: 197,200, lr:2.000e-04> G_loss: 3.022e-02 
22-11-07 05:32:33.563 : <epoch:664, iter: 197,400, lr:2.000e-04> G_loss: 3.343e-02 
22-11-07 05:33:14.912 : <epoch:665, iter: 197,600, lr:2.000e-04> G_loss: 3.509e-02 
22-11-07 05:33:53.904 : <epoch:665, iter: 197,800, lr:2.000e-04> G_loss: 4.159e-02 
22-11-07 05:34:35.730 : <epoch:666, iter: 198,000, lr:2.000e-04> G_loss: 2.806e-02 
22-11-07 05:35:16.960 : <epoch:667, iter: 198,200, lr:2.000e-04> G_loss: 3.298e-02 
22-11-07 05:35:58.106 : <epoch:668, iter: 198,400, lr:2.000e-04> G_loss: 3.402e-02 
22-11-07 05:36:37.712 : <epoch:668, iter: 198,600, lr:2.000e-04> G_loss: 2.626e-02 
22-11-07 05:37:18.983 : <epoch:669, iter: 198,800, lr:2.000e-04> G_loss: 2.891e-02 
22-11-07 05:38:00.401 : <epoch:670, iter: 199,000, lr:2.000e-04> G_loss: 2.446e-02 
22-11-07 05:38:40.141 : <epoch:670, iter: 199,200, lr:2.000e-04> G_loss: 2.739e-02 
22-11-07 05:39:21.182 : <epoch:671, iter: 199,400, lr:2.000e-04> G_loss: 3.722e-02 
22-11-07 05:40:02.809 : <epoch:672, iter: 199,600, lr:2.000e-04> G_loss: 4.033e-02 
22-11-07 05:40:42.600 : <epoch:672, iter: 199,800, lr:2.000e-04> G_loss: 3.458e-02 
22-11-07 05:41:23.685 : <epoch:673, iter: 200,000, lr:2.000e-04> G_loss: 3.687e-02 
22-11-07 05:41:23.686 : Saving the model.
22-11-07 05:41:24.518 : ---1-->   0001.jpg | 25.25dB
22-11-07 05:41:24.896 : ---2-->   0002.jpg | 25.81dB
22-11-07 05:41:25.340 : ---3-->   0003.jpg | 29.62dB
22-11-07 05:41:25.556 : ---4-->   0004.jpg | 28.52dB
22-11-07 05:41:25.733 : ---5-->   0005.jpg | 29.32dB
22-11-07 05:41:25.907 : ---6-->   0006.jpg | 33.51dB
22-11-07 05:41:26.070 : ---7-->   0007.jpg | 35.83dB
22-11-07 05:41:26.465 : ---8-->   0008.jpg | 24.35dB
22-11-07 05:41:26.815 : ---9-->   0009.jpg | 23.55dB
22-11-07 05:41:27.157 : --10-->   0010.jpg | 28.30dB
22-11-07 05:41:27.523 : --11-->   0011.jpg | 26.55dB
22-11-07 05:41:27.879 : --12-->   0012.jpg | 24.27dB
22-11-07 05:41:28.034 : --13-->   0013.jpg | 24.25dB
22-11-07 05:41:28.194 : --14-->   0014.jpg | 23.56dB
22-11-07 05:41:28.521 : --15-->   0015.jpg | 21.97dB
22-11-07 05:41:28.723 : --16-->   0016.jpg | 30.26dB
22-11-07 05:41:28.929 : --17-->   0017.jpg | 29.77dB
22-11-07 05:41:29.155 : --18-->   0018.jpg | 27.88dB
22-11-07 05:41:29.470 : --19-->   0019.jpg | 32.28dB
22-11-07 05:41:29.636 : --20-->   0020.jpg | 23.95dB
22-11-07 05:41:29.964 : --21-->   0021.jpg | 26.68dB
22-11-07 05:41:30.154 : --22-->   0022.jpg | 38.58dB
22-11-07 05:41:30.300 : --23-->   0023.jpg | 25.70dB
22-11-07 05:41:30.498 : --24-->   0024.jpg | 34.26dB
22-11-07 05:41:30.651 : --25-->   0025.jpg | 26.15dB
22-11-07 05:41:30.834 : --26-->   0026.jpg | 27.26dB
22-11-07 05:41:31.160 : --27-->   0027.jpg | 29.00dB
22-11-07 05:41:31.463 : --28-->   0028.jpg | 30.47dB
22-11-07 05:41:31.614 : --29-->   0029.jpg | 24.24dB
22-11-07 05:41:31.761 : --30-->   0030.jpg | 29.35dB
22-11-07 05:41:31.961 : --31-->   0031.jpg | 28.31dB
22-11-07 05:41:32.156 : --32-->   0032.jpg | 26.24dB
22-11-07 05:41:32.318 : --33-->   0033.jpg | 23.81dB
22-11-07 05:41:32.780 : --34-->   0034.jpg | 23.44dB
22-11-07 05:41:33.278 : --35-->   0035.jpg | 27.21dB
22-11-07 05:41:33.693 : --36-->   0036.jpg | 23.37dB
22-11-07 05:41:33.861 : --37-->   0037.jpg | 23.08dB
22-11-07 05:41:34.061 : --38-->   0038.jpg | 26.97dB
22-11-07 05:41:34.248 : --39-->   0039.jpg | 24.18dB
22-11-07 05:41:34.459 : --40-->   0040.jpg | 28.21dB
22-11-07 05:41:34.619 : --41-->   0041.jpg | 21.21dB
22-11-07 05:41:34.975 : --42-->   0042.jpg | 26.62dB
22-11-07 05:41:35.371 : --43-->   0043.jpg | 27.53dB
22-11-07 05:41:35.787 : --44-->   0044.jpg | 24.74dB
22-11-07 05:41:36.072 : --45-->   0045.jpg | 21.76dB
22-11-07 05:41:36.292 : --46-->   0046.jpg | 23.43dB
22-11-07 05:41:36.655 : --47-->   0047.jpg | 24.47dB
22-11-07 05:41:36.799 : --48-->   0048.jpg | 28.79dB
22-11-07 05:41:37.051 : --49-->   0049.jpg | 28.58dB
22-11-07 05:41:37.208 : --50-->   0050.jpg | 21.30dB
22-11-07 05:41:37.357 : --51-->   0051.jpg | 26.27dB
22-11-07 05:41:37.513 : --52-->   0052.jpg | 23.62dB
22-11-07 05:41:37.661 : --53-->   0053.jpg | 24.69dB
22-11-07 05:41:37.882 : --54-->   0054.jpg | 22.19dB
22-11-07 05:41:38.172 : --55-->   0055.jpg | 36.68dB
22-11-07 05:41:38.321 : --56-->   0056.jpg | 24.54dB
22-11-07 05:41:38.462 : --57-->   0057.jpg | 30.84dB
22-11-07 05:41:38.822 : --58-->   0058.jpg | 26.69dB
22-11-07 05:41:39.016 : --59-->   0059.jpg | 28.05dB
22-11-07 05:41:39.187 : --60-->   0060.jpg | 24.89dB
22-11-07 05:41:39.358 : --61-->   0061.jpg | 26.56dB
22-11-07 05:41:39.531 : --62-->   0062.jpg | 24.49dB
22-11-07 05:41:39.712 : --63-->   0063.jpg | 23.28dB
22-11-07 05:41:40.098 : --64-->   0064.jpg | 29.35dB
22-11-07 05:41:40.437 : --65-->   0065.jpg | 35.20dB
22-11-07 05:41:40.603 : --66-->   0066.jpg | 30.34dB
22-11-07 05:41:40.961 : --67-->   0067.jpg | 26.70dB
22-11-07 05:41:41.154 : --68-->   0068.jpg | 25.29dB
22-11-07 05:41:41.358 : --69-->   0069.jpg | 19.47dB
22-11-07 05:41:41.561 : --70-->   0070.jpg | 21.22dB
22-11-07 05:41:41.732 : --71-->   0071.jpg | 22.58dB
22-11-07 05:41:41.908 : --72-->   0072.jpg | 28.51dB
22-11-07 05:41:42.249 : --73-->   0073.jpg | 27.48dB
22-11-07 05:41:42.449 : --74-->   0074.jpg | 24.66dB
22-11-07 05:41:42.663 : --75-->   0075.jpg | 24.63dB
22-11-07 05:41:42.838 : --76-->   0076.jpg | 25.33dB
22-11-07 05:41:43.186 : --77-->   0077.jpg | 22.53dB
22-11-07 05:41:43.347 : --78-->   0078.jpg | 28.07dB
22-11-07 05:41:43.696 : --79-->   0079.jpg | 20.51dB
22-11-07 05:41:43.869 : --80-->   0080.jpg | 30.36dB
22-11-07 05:41:44.085 : --81-->   0081.jpg | 28.72dB
22-11-07 05:41:44.243 : --82-->   0082.jpg | 28.57dB
22-11-07 05:41:44.411 : --83-->   0083.jpg | 23.27dB
22-11-07 05:41:44.884 : --84-->   0084.jpg | 29.04dB
22-11-07 05:41:45.039 : --85-->   0085.jpg | 21.01dB
22-11-07 05:41:45.204 : --86-->   0086.jpg | 24.38dB
22-11-07 05:41:45.368 : --87-->   0087.jpg | 21.05dB
22-11-07 05:41:45.853 : --88-->   0088.jpg | 27.02dB
22-11-07 05:41:46.019 : --89-->   0089.jpg | 23.79dB
22-11-07 05:41:46.250 : --90-->   0090.jpg | 21.81dB
22-11-07 05:41:46.421 : --91-->   0091.jpg | 31.39dB
22-11-07 05:41:46.589 : --92-->   0092.jpg | 22.60dB
22-11-07 05:41:46.768 : --93-->   0093.jpg | 27.84dB
22-11-07 05:41:47.221 : --94-->   0094.jpg | 30.67dB
22-11-07 05:41:47.378 : --95-->   0095.jpg | 25.21dB
22-11-07 05:41:47.817 : --96-->   0096.jpg | 20.90dB
22-11-07 05:41:48.017 : --97-->   0097.jpg | 32.61dB
22-11-07 05:41:48.165 : --98-->   0098.jpg | 25.64dB
22-11-07 05:41:48.314 : --99-->   0099.jpg | 24.96dB
22-11-07 05:41:48.468 : -100-->   0100.jpg | 23.59dB
22-11-07 05:41:48.560 : <epoch:673, iter: 200,000, Average PSNR : 26.43dB

22-11-07 05:42:29.645 : <epoch:674, iter: 200,200, lr:2.000e-04> G_loss: 3.331e-02 
22-11-07 05:43:09.425 : <epoch:674, iter: 200,400, lr:2.000e-04> G_loss: 3.273e-02 
22-11-07 05:43:50.828 : <epoch:675, iter: 200,600, lr:2.000e-04> G_loss: 2.769e-02 
22-11-07 05:44:32.105 : <epoch:676, iter: 200,800, lr:2.000e-04> G_loss: 3.417e-02 
22-11-07 05:45:11.967 : <epoch:676, iter: 201,000, lr:2.000e-04> G_loss: 3.599e-02 
22-11-07 05:45:53.499 : <epoch:677, iter: 201,200, lr:2.000e-04> G_loss: 3.318e-02 
22-11-07 05:46:34.842 : <epoch:678, iter: 201,400, lr:2.000e-04> G_loss: 3.563e-02 
22-11-07 05:47:14.250 : <epoch:678, iter: 201,600, lr:2.000e-04> G_loss: 3.013e-02 
22-11-07 05:47:55.684 : <epoch:679, iter: 201,800, lr:2.000e-04> G_loss: 3.742e-02 
22-11-07 05:48:36.924 : <epoch:680, iter: 202,000, lr:2.000e-04> G_loss: 3.141e-02 
22-11-07 05:49:16.634 : <epoch:680, iter: 202,200, lr:2.000e-04> G_loss: 2.860e-02 
22-11-07 05:49:57.870 : <epoch:681, iter: 202,400, lr:2.000e-04> G_loss: 3.686e-02 
22-11-07 05:50:39.244 : <epoch:682, iter: 202,600, lr:2.000e-04> G_loss: 3.915e-02 
22-11-07 05:51:19.381 : <epoch:682, iter: 202,800, lr:2.000e-04> G_loss: 3.737e-02 
22-11-07 05:52:00.745 : <epoch:683, iter: 203,000, lr:2.000e-04> G_loss: 3.873e-02 
22-11-07 05:52:42.224 : <epoch:684, iter: 203,200, lr:2.000e-04> G_loss: 2.765e-02 
22-11-07 05:53:21.740 : <epoch:684, iter: 203,400, lr:2.000e-04> G_loss: 3.297e-02 
22-11-07 05:54:02.842 : <epoch:685, iter: 203,600, lr:2.000e-04> G_loss: 2.737e-02 
22-11-07 05:54:44.170 : <epoch:686, iter: 203,800, lr:2.000e-04> G_loss: 3.413e-02 
22-11-07 05:55:23.654 : <epoch:686, iter: 204,000, lr:2.000e-04> G_loss: 3.890e-02 
22-11-07 05:56:05.138 : <epoch:687, iter: 204,200, lr:2.000e-04> G_loss: 3.259e-02 
22-11-07 05:56:46.554 : <epoch:688, iter: 204,400, lr:2.000e-04> G_loss: 3.469e-02 
22-11-07 05:57:26.276 : <epoch:688, iter: 204,600, lr:2.000e-04> G_loss: 3.736e-02 
22-11-07 05:58:07.906 : <epoch:689, iter: 204,800, lr:2.000e-04> G_loss: 3.099e-02 
22-11-07 05:58:49.335 : <epoch:690, iter: 205,000, lr:2.000e-04> G_loss: 2.751e-02 
22-11-07 05:58:49.336 : Saving the model.
22-11-07 05:58:50.123 : ---1-->   0001.jpg | 25.25dB
22-11-07 05:58:50.640 : ---2-->   0002.jpg | 25.83dB
22-11-07 05:58:50.937 : ---3-->   0003.jpg | 29.89dB
22-11-07 05:58:51.135 : ---4-->   0004.jpg | 28.54dB
22-11-07 05:58:51.284 : ---5-->   0005.jpg | 29.34dB
22-11-07 05:58:51.447 : ---6-->   0006.jpg | 34.20dB
22-11-07 05:58:51.699 : ---7-->   0007.jpg | 35.96dB
22-11-07 05:58:52.088 : ---8-->   0008.jpg | 24.37dB
22-11-07 05:58:52.438 : ---9-->   0009.jpg | 23.60dB
22-11-07 05:58:52.810 : --10-->   0010.jpg | 28.45dB
22-11-07 05:58:53.286 : --11-->   0011.jpg | 25.72dB
22-11-07 05:58:53.617 : --12-->   0012.jpg | 24.16dB
22-11-07 05:58:53.881 : --13-->   0013.jpg | 24.23dB
22-11-07 05:58:54.049 : --14-->   0014.jpg | 23.49dB
22-11-07 05:58:54.478 : --15-->   0015.jpg | 22.01dB
22-11-07 05:58:54.631 : --16-->   0016.jpg | 30.20dB
22-11-07 05:58:54.838 : --17-->   0017.jpg | 29.71dB
22-11-07 05:58:55.096 : --18-->   0018.jpg | 27.03dB
22-11-07 05:58:55.568 : --19-->   0019.jpg | 32.68dB
22-11-07 05:58:55.750 : --20-->   0020.jpg | 24.02dB
22-11-07 05:58:56.180 : --21-->   0021.jpg | 26.69dB
22-11-07 05:58:56.561 : --22-->   0022.jpg | 39.43dB
22-11-07 05:58:56.735 : --23-->   0023.jpg | 25.74dB
22-11-07 05:58:57.194 : --24-->   0024.jpg | 34.69dB
22-11-07 05:58:57.369 : --25-->   0025.jpg | 26.17dB
22-11-07 05:58:57.541 : --26-->   0026.jpg | 27.20dB
22-11-07 05:58:58.018 : --27-->   0027.jpg | 29.16dB
22-11-07 05:58:58.326 : --28-->   0028.jpg | 30.30dB
22-11-07 05:58:58.500 : --29-->   0029.jpg | 24.28dB
22-11-07 05:58:58.649 : --30-->   0030.jpg | 29.29dB
22-11-07 05:58:58.847 : --31-->   0031.jpg | 28.35dB
22-11-07 05:58:59.023 : --32-->   0032.jpg | 26.37dB
22-11-07 05:58:59.181 : --33-->   0033.jpg | 23.91dB
22-11-07 05:58:59.504 : --34-->   0034.jpg | 23.48dB
22-11-07 05:58:59.822 : --35-->   0035.jpg | 26.95dB
22-11-07 05:59:00.239 : --36-->   0036.jpg | 23.38dB
22-11-07 05:59:00.404 : --37-->   0037.jpg | 23.09dB
22-11-07 05:59:00.552 : --38-->   0038.jpg | 26.93dB
22-11-07 05:59:00.706 : --39-->   0039.jpg | 24.39dB
22-11-07 05:59:00.913 : --40-->   0040.jpg | 28.22dB
22-11-07 05:59:01.114 : --41-->   0041.jpg | 21.16dB
22-11-07 05:59:01.489 : --42-->   0042.jpg | 26.70dB
22-11-07 05:59:02.164 : --43-->   0043.jpg | 27.43dB
22-11-07 05:59:02.637 : --44-->   0044.jpg | 24.75dB
22-11-07 05:59:02.804 : --45-->   0045.jpg | 21.71dB
22-11-07 05:59:03.049 : --46-->   0046.jpg | 23.33dB
22-11-07 05:59:03.445 : --47-->   0047.jpg | 24.39dB
22-11-07 05:59:03.588 : --48-->   0048.jpg | 28.77dB
22-11-07 05:59:03.739 : --49-->   0049.jpg | 28.67dB
22-11-07 05:59:04.023 : --50-->   0050.jpg | 21.29dB
22-11-07 05:59:04.180 : --51-->   0051.jpg | 26.26dB
22-11-07 05:59:04.335 : --52-->   0052.jpg | 23.61dB
22-11-07 05:59:04.483 : --53-->   0053.jpg | 24.71dB
22-11-07 05:59:04.725 : --54-->   0054.jpg | 22.21dB
22-11-07 05:59:05.180 : --55-->   0055.jpg | 36.83dB
22-11-07 05:59:05.349 : --56-->   0056.jpg | 24.48dB
22-11-07 05:59:05.505 : --57-->   0057.jpg | 30.62dB
22-11-07 05:59:06.006 : --58-->   0058.jpg | 26.73dB
22-11-07 05:59:06.157 : --59-->   0059.jpg | 28.19dB
22-11-07 05:59:06.307 : --60-->   0060.jpg | 25.00dB
22-11-07 05:59:06.536 : --61-->   0061.jpg | 26.69dB
22-11-07 05:59:06.686 : --62-->   0062.jpg | 24.52dB
22-11-07 05:59:06.969 : --63-->   0063.jpg | 23.24dB
22-11-07 05:59:07.632 : --64-->   0064.jpg | 29.37dB
22-11-07 05:59:08.260 : --65-->   0065.jpg | 35.07dB
22-11-07 05:59:08.412 : --66-->   0066.jpg | 30.27dB
22-11-07 05:59:09.020 : --67-->   0067.jpg | 26.88dB
22-11-07 05:59:09.201 : --68-->   0068.jpg | 25.30dB
22-11-07 05:59:09.374 : --69-->   0069.jpg | 19.46dB
22-11-07 05:59:09.532 : --70-->   0070.jpg | 21.24dB
22-11-07 05:59:09.703 : --71-->   0071.jpg | 22.55dB
22-11-07 05:59:09.880 : --72-->   0072.jpg | 28.41dB
22-11-07 05:59:10.269 : --73-->   0073.jpg | 27.40dB
22-11-07 05:59:10.451 : --74-->   0074.jpg | 24.63dB
22-11-07 05:59:10.631 : --75-->   0075.jpg | 24.57dB
22-11-07 05:59:10.811 : --76-->   0076.jpg | 25.50dB
22-11-07 05:59:11.228 : --77-->   0077.jpg | 22.51dB
22-11-07 05:59:11.400 : --78-->   0078.jpg | 28.06dB
22-11-07 05:59:11.767 : --79-->   0079.jpg | 20.50dB
22-11-07 05:59:11.987 : --80-->   0080.jpg | 30.21dB
22-11-07 05:59:12.234 : --81-->   0081.jpg | 28.72dB
22-11-07 05:59:12.410 : --82-->   0082.jpg | 28.53dB
22-11-07 05:59:12.564 : --83-->   0083.jpg | 23.43dB
22-11-07 05:59:12.975 : --84-->   0084.jpg | 29.02dB
22-11-07 05:59:13.132 : --85-->   0085.jpg | 21.08dB
22-11-07 05:59:13.308 : --86-->   0086.jpg | 24.56dB
22-11-07 05:59:13.481 : --87-->   0087.jpg | 21.04dB
22-11-07 05:59:13.845 : --88-->   0088.jpg | 27.32dB
22-11-07 05:59:14.085 : --89-->   0089.jpg | 23.94dB
22-11-07 05:59:14.367 : --90-->   0090.jpg | 21.84dB
22-11-07 05:59:14.514 : --91-->   0091.jpg | 31.28dB
22-11-07 05:59:14.669 : --92-->   0092.jpg | 22.53dB
22-11-07 05:59:14.814 : --93-->   0093.jpg | 27.95dB
22-11-07 05:59:15.234 : --94-->   0094.jpg | 30.78dB
22-11-07 05:59:15.403 : --95-->   0095.jpg | 25.24dB
22-11-07 05:59:15.760 : --96-->   0096.jpg | 20.95dB
22-11-07 05:59:15.976 : --97-->   0097.jpg | 32.64dB
22-11-07 05:59:16.143 : --98-->   0098.jpg | 25.72dB
22-11-07 05:59:16.342 : --99-->   0099.jpg | 24.94dB
22-11-07 05:59:16.493 : -100-->   0100.jpg | 23.58dB
22-11-07 05:59:16.587 : <epoch:690, iter: 205,000, Average PSNR : 26.45dB

22-11-07 05:59:56.002 : <epoch:690, iter: 205,200, lr:2.000e-04> G_loss: 2.845e-02 
22-11-07 06:00:37.363 : <epoch:691, iter: 205,400, lr:2.000e-04> G_loss: 3.228e-02 
22-11-07 06:01:18.495 : <epoch:692, iter: 205,600, lr:2.000e-04> G_loss: 2.365e-02 
22-11-07 06:01:57.834 : <epoch:692, iter: 205,800, lr:2.000e-04> G_loss: 3.516e-02 
22-11-07 06:02:39.154 : <epoch:693, iter: 206,000, lr:2.000e-04> G_loss: 3.049e-02 
22-11-07 06:03:20.748 : <epoch:694, iter: 206,200, lr:2.000e-04> G_loss: 3.228e-02 
22-11-07 06:03:59.878 : <epoch:694, iter: 206,400, lr:2.000e-04> G_loss: 3.206e-02 
22-11-07 06:04:41.808 : <epoch:695, iter: 206,600, lr:2.000e-04> G_loss: 3.180e-02 
22-11-07 06:05:23.002 : <epoch:696, iter: 206,800, lr:2.000e-04> G_loss: 3.636e-02 
22-11-07 06:06:02.176 : <epoch:696, iter: 207,000, lr:2.000e-04> G_loss: 2.627e-02 
22-11-07 06:06:43.940 : <epoch:697, iter: 207,200, lr:2.000e-04> G_loss: 3.099e-02 
22-11-07 06:07:25.872 : <epoch:698, iter: 207,400, lr:2.000e-04> G_loss: 3.171e-02 
22-11-07 06:08:05.321 : <epoch:698, iter: 207,600, lr:2.000e-04> G_loss: 2.756e-02 
22-11-07 06:08:47.203 : <epoch:699, iter: 207,800, lr:2.000e-04> G_loss: 4.020e-02 
22-11-07 06:09:28.582 : <epoch:700, iter: 208,000, lr:2.000e-04> G_loss: 2.943e-02 
22-11-07 06:10:10.481 : <epoch:701, iter: 208,200, lr:2.000e-04> G_loss: 3.011e-02 
22-11-07 06:10:50.604 : <epoch:701, iter: 208,400, lr:2.000e-04> G_loss: 2.954e-02 
22-11-07 06:11:32.418 : <epoch:702, iter: 208,600, lr:2.000e-04> G_loss: 2.963e-02 
22-11-07 06:12:14.635 : <epoch:703, iter: 208,800, lr:2.000e-04> G_loss: 3.660e-02 
22-11-07 06:12:54.671 : <epoch:703, iter: 209,000, lr:2.000e-04> G_loss: 3.161e-02 
22-11-07 06:13:36.414 : <epoch:704, iter: 209,200, lr:2.000e-04> G_loss: 2.971e-02 
22-11-07 06:14:18.338 : <epoch:705, iter: 209,400, lr:2.000e-04> G_loss: 3.077e-02 
22-11-07 06:14:58.144 : <epoch:705, iter: 209,600, lr:2.000e-04> G_loss: 2.598e-02 
22-11-07 06:15:40.043 : <epoch:706, iter: 209,800, lr:2.000e-04> G_loss: 2.744e-02 
22-11-07 06:16:22.116 : <epoch:707, iter: 210,000, lr:2.000e-04> G_loss: 3.017e-02 
22-11-07 06:16:22.117 : Saving the model.
22-11-07 06:16:22.870 : ---1-->   0001.jpg | 25.37dB
22-11-07 06:16:23.347 : ---2-->   0002.jpg | 25.94dB
22-11-07 06:16:23.726 : ---3-->   0003.jpg | 29.76dB
22-11-07 06:16:23.925 : ---4-->   0004.jpg | 28.46dB
22-11-07 06:16:24.078 : ---5-->   0005.jpg | 29.38dB
22-11-07 06:16:24.270 : ---6-->   0006.jpg | 33.49dB
22-11-07 06:16:24.435 : ---7-->   0007.jpg | 35.85dB
22-11-07 06:16:24.782 : ---8-->   0008.jpg | 24.36dB
22-11-07 06:16:25.198 : ---9-->   0009.jpg | 23.64dB
22-11-07 06:16:25.570 : --10-->   0010.jpg | 28.37dB
22-11-07 06:16:25.881 : --11-->   0011.jpg | 26.49dB
22-11-07 06:16:26.244 : --12-->   0012.jpg | 24.20dB
22-11-07 06:16:26.396 : --13-->   0013.jpg | 24.26dB
22-11-07 06:16:26.558 : --14-->   0014.jpg | 23.63dB
22-11-07 06:16:26.875 : --15-->   0015.jpg | 22.08dB
22-11-07 06:16:27.025 : --16-->   0016.jpg | 30.27dB
22-11-07 06:16:27.254 : --17-->   0017.jpg | 29.67dB
22-11-07 06:16:27.536 : --18-->   0018.jpg | 27.24dB
22-11-07 06:16:27.946 : --19-->   0019.jpg | 32.56dB
22-11-07 06:16:28.136 : --20-->   0020.jpg | 24.07dB
22-11-07 06:16:28.578 : --21-->   0021.jpg | 26.69dB
22-11-07 06:16:28.903 : --22-->   0022.jpg | 39.03dB
22-11-07 06:16:29.067 : --23-->   0023.jpg | 25.66dB
22-11-07 06:16:29.300 : --24-->   0024.jpg | 34.25dB
22-11-07 06:16:29.455 : --25-->   0025.jpg | 26.26dB
22-11-07 06:16:29.610 : --26-->   0026.jpg | 27.38dB
22-11-07 06:16:29.915 : --27-->   0027.jpg | 29.24dB
22-11-07 06:16:30.259 : --28-->   0028.jpg | 30.60dB
22-11-07 06:16:30.437 : --29-->   0029.jpg | 24.28dB
22-11-07 06:16:30.598 : --30-->   0030.jpg | 29.47dB
22-11-07 06:16:30.762 : --31-->   0031.jpg | 28.38dB
22-11-07 06:16:30.964 : --32-->   0032.jpg | 26.53dB
22-11-07 06:16:31.130 : --33-->   0033.jpg | 23.95dB
22-11-07 06:16:31.474 : --34-->   0034.jpg | 23.44dB
22-11-07 06:16:31.830 : --35-->   0035.jpg | 27.43dB
22-11-07 06:16:32.201 : --36-->   0036.jpg | 23.45dB
22-11-07 06:16:32.380 : --37-->   0037.jpg | 23.09dB
22-11-07 06:16:32.545 : --38-->   0038.jpg | 27.01dB
22-11-07 06:16:32.803 : --39-->   0039.jpg | 24.24dB
22-11-07 06:16:33.021 : --40-->   0040.jpg | 28.29dB
22-11-07 06:16:33.191 : --41-->   0041.jpg | 21.34dB
22-11-07 06:16:33.538 : --42-->   0042.jpg | 26.80dB
22-11-07 06:16:34.005 : --43-->   0043.jpg | 27.62dB
22-11-07 06:16:34.495 : --44-->   0044.jpg | 24.74dB
22-11-07 06:16:34.662 : --45-->   0045.jpg | 21.71dB
22-11-07 06:16:34.954 : --46-->   0046.jpg | 23.41dB
22-11-07 06:16:35.483 : --47-->   0047.jpg | 24.52dB
22-11-07 06:16:35.641 : --48-->   0048.jpg | 28.79dB
22-11-07 06:16:35.819 : --49-->   0049.jpg | 28.76dB
22-11-07 06:16:35.986 : --50-->   0050.jpg | 21.33dB
22-11-07 06:16:36.154 : --51-->   0051.jpg | 26.39dB
22-11-07 06:16:36.376 : --52-->   0052.jpg | 23.66dB
22-11-07 06:16:36.540 : --53-->   0053.jpg | 24.73dB
22-11-07 06:16:36.824 : --54-->   0054.jpg | 22.16dB
22-11-07 06:16:37.292 : --55-->   0055.jpg | 36.55dB
22-11-07 06:16:37.449 : --56-->   0056.jpg | 24.82dB
22-11-07 06:16:37.660 : --57-->   0057.jpg | 30.75dB
22-11-07 06:16:38.110 : --58-->   0058.jpg | 26.93dB
22-11-07 06:16:38.284 : --59-->   0059.jpg | 28.19dB
22-11-07 06:16:38.435 : --60-->   0060.jpg | 25.07dB
22-11-07 06:16:38.596 : --61-->   0061.jpg | 26.71dB
22-11-07 06:16:38.767 : --62-->   0062.jpg | 24.51dB
22-11-07 06:16:38.968 : --63-->   0063.jpg | 23.31dB
22-11-07 06:16:39.354 : --64-->   0064.jpg | 29.34dB
22-11-07 06:16:39.673 : --65-->   0065.jpg | 35.28dB
22-11-07 06:16:39.840 : --66-->   0066.jpg | 30.24dB
22-11-07 06:16:40.176 : --67-->   0067.jpg | 26.98dB
22-11-07 06:16:40.371 : --68-->   0068.jpg | 25.36dB
22-11-07 06:16:40.639 : --69-->   0069.jpg | 19.48dB
22-11-07 06:16:40.828 : --70-->   0070.jpg | 21.30dB
22-11-07 06:16:41.001 : --71-->   0071.jpg | 22.61dB
22-11-07 06:16:41.177 : --72-->   0072.jpg | 28.28dB
22-11-07 06:16:41.496 : --73-->   0073.jpg | 27.50dB
22-11-07 06:16:41.682 : --74-->   0074.jpg | 25.23dB
22-11-07 06:16:41.878 : --75-->   0075.jpg | 24.65dB
22-11-07 06:16:42.094 : --76-->   0076.jpg | 25.44dB
22-11-07 06:16:42.457 : --77-->   0077.jpg | 22.59dB
22-11-07 06:16:42.620 : --78-->   0078.jpg | 28.11dB
22-11-07 06:16:42.969 : --79-->   0079.jpg | 20.51dB
22-11-07 06:16:43.203 : --80-->   0080.jpg | 30.41dB
22-11-07 06:16:43.432 : --81-->   0081.jpg | 28.72dB
22-11-07 06:16:43.589 : --82-->   0082.jpg | 28.66dB
22-11-07 06:16:43.742 : --83-->   0083.jpg | 23.43dB
22-11-07 06:16:44.298 : --84-->   0084.jpg | 29.28dB
22-11-07 06:16:44.459 : --85-->   0085.jpg | 21.01dB
22-11-07 06:16:44.617 : --86-->   0086.jpg | 24.42dB
22-11-07 06:16:44.770 : --87-->   0087.jpg | 21.10dB
22-11-07 06:16:45.405 : --88-->   0088.jpg | 27.35dB
22-11-07 06:16:45.570 : --89-->   0089.jpg | 23.99dB
22-11-07 06:16:45.955 : --90-->   0090.jpg | 21.83dB
22-11-07 06:16:46.139 : --91-->   0091.jpg | 31.44dB
22-11-07 06:16:46.300 : --92-->   0092.jpg | 22.66dB
22-11-07 06:16:46.470 : --93-->   0093.jpg | 27.99dB
22-11-07 06:16:46.871 : --94-->   0094.jpg | 30.74dB
22-11-07 06:16:47.215 : --95-->   0095.jpg | 25.24dB
22-11-07 06:16:47.615 : --96-->   0096.jpg | 20.93dB
22-11-07 06:16:47.932 : --97-->   0097.jpg | 32.62dB
22-11-07 06:16:48.104 : --98-->   0098.jpg | 25.70dB
22-11-07 06:16:48.307 : --99-->   0099.jpg | 24.99dB
22-11-07 06:16:48.468 : -100-->   0100.jpg | 23.59dB
22-11-07 06:16:48.562 : <epoch:707, iter: 210,000, Average PSNR : 26.50dB

22-11-07 06:17:28.741 : <epoch:707, iter: 210,200, lr:2.000e-04> G_loss: 2.501e-02 
22-11-07 06:18:10.720 : <epoch:708, iter: 210,400, lr:2.000e-04> G_loss: 3.126e-02 
22-11-07 06:18:52.458 : <epoch:709, iter: 210,600, lr:2.000e-04> G_loss: 2.775e-02 
22-11-07 06:19:32.662 : <epoch:709, iter: 210,800, lr:2.000e-04> G_loss: 4.102e-02 
22-11-07 06:20:14.766 : <epoch:710, iter: 211,000, lr:2.000e-04> G_loss: 2.942e-02 
22-11-07 06:20:56.823 : <epoch:711, iter: 211,200, lr:2.000e-04> G_loss: 4.188e-02 
22-11-07 06:21:36.971 : <epoch:711, iter: 211,400, lr:2.000e-04> G_loss: 3.448e-02 
22-11-07 06:22:19.604 : <epoch:712, iter: 211,600, lr:2.000e-04> G_loss: 2.614e-02 
22-11-07 06:23:02.588 : <epoch:713, iter: 211,800, lr:2.000e-04> G_loss: 2.871e-02 
22-11-07 06:23:42.596 : <epoch:713, iter: 212,000, lr:2.000e-04> G_loss: 3.855e-02 
22-11-07 06:24:24.895 : <epoch:714, iter: 212,200, lr:2.000e-04> G_loss: 3.291e-02 
22-11-07 06:25:07.447 : <epoch:715, iter: 212,400, lr:2.000e-04> G_loss: 3.338e-02 
22-11-07 06:25:47.744 : <epoch:715, iter: 212,600, lr:2.000e-04> G_loss: 3.008e-02 
22-11-07 06:26:30.210 : <epoch:716, iter: 212,800, lr:2.000e-04> G_loss: 2.951e-02 
22-11-07 06:27:13.034 : <epoch:717, iter: 213,000, lr:2.000e-04> G_loss: 2.703e-02 
22-11-07 06:27:53.609 : <epoch:717, iter: 213,200, lr:2.000e-04> G_loss: 3.526e-02 
22-11-07 06:28:36.113 : <epoch:718, iter: 213,400, lr:2.000e-04> G_loss: 3.023e-02 
22-11-07 06:29:18.885 : <epoch:719, iter: 213,600, lr:2.000e-04> G_loss: 3.834e-02 
22-11-07 06:29:59.361 : <epoch:719, iter: 213,800, lr:2.000e-04> G_loss: 3.776e-02 
22-11-07 06:30:41.829 : <epoch:720, iter: 214,000, lr:2.000e-04> G_loss: 3.450e-02 
22-11-07 06:31:24.459 : <epoch:721, iter: 214,200, lr:2.000e-04> G_loss: 4.112e-02 
22-11-07 06:32:05.137 : <epoch:721, iter: 214,400, lr:2.000e-04> G_loss: 3.432e-02 
22-11-07 06:32:47.991 : <epoch:722, iter: 214,600, lr:2.000e-04> G_loss: 3.708e-02 
22-11-07 06:33:31.171 : <epoch:723, iter: 214,800, lr:2.000e-04> G_loss: 4.549e-02 
22-11-07 06:34:11.542 : <epoch:723, iter: 215,000, lr:2.000e-04> G_loss: 3.518e-02 
22-11-07 06:34:11.543 : Saving the model.
22-11-07 06:34:12.341 : ---1-->   0001.jpg | 25.34dB
22-11-07 06:34:12.761 : ---2-->   0002.jpg | 25.95dB
22-11-07 06:34:13.174 : ---3-->   0003.jpg | 29.72dB
22-11-07 06:34:13.393 : ---4-->   0004.jpg | 28.44dB
22-11-07 06:34:13.575 : ---5-->   0005.jpg | 29.27dB
22-11-07 06:34:13.796 : ---6-->   0006.jpg | 33.46dB
22-11-07 06:34:13.952 : ---7-->   0007.jpg | 35.53dB
22-11-07 06:34:14.353 : ---8-->   0008.jpg | 24.35dB
22-11-07 06:34:14.729 : ---9-->   0009.jpg | 23.62dB
22-11-07 06:34:15.088 : --10-->   0010.jpg | 28.32dB
22-11-07 06:34:15.596 : --11-->   0011.jpg | 26.45dB
22-11-07 06:34:16.013 : --12-->   0012.jpg | 24.41dB
22-11-07 06:34:16.168 : --13-->   0013.jpg | 24.19dB
22-11-07 06:34:16.325 : --14-->   0014.jpg | 23.59dB
22-11-07 06:34:16.919 : --15-->   0015.jpg | 21.80dB
22-11-07 06:34:17.068 : --16-->   0016.jpg | 30.19dB
22-11-07 06:34:17.233 : --17-->   0017.jpg | 29.65dB
22-11-07 06:34:17.488 : --18-->   0018.jpg | 27.35dB
22-11-07 06:34:18.264 : --19-->   0019.jpg | 32.51dB
22-11-07 06:34:18.420 : --20-->   0020.jpg | 24.08dB
22-11-07 06:34:18.925 : --21-->   0021.jpg | 26.42dB
22-11-07 06:34:19.127 : --22-->   0022.jpg | 38.76dB
22-11-07 06:34:19.286 : --23-->   0023.jpg | 25.67dB
22-11-07 06:34:19.527 : --24-->   0024.jpg | 34.06dB
22-11-07 06:34:19.706 : --25-->   0025.jpg | 26.15dB
22-11-07 06:34:19.873 : --26-->   0026.jpg | 27.35dB
22-11-07 06:34:20.249 : --27-->   0027.jpg | 29.29dB
22-11-07 06:34:20.534 : --28-->   0028.jpg | 30.29dB
22-11-07 06:34:20.698 : --29-->   0029.jpg | 24.26dB
22-11-07 06:34:20.859 : --30-->   0030.jpg | 29.27dB
22-11-07 06:34:21.098 : --31-->   0031.jpg | 28.30dB
22-11-07 06:34:21.266 : --32-->   0032.jpg | 26.35dB
22-11-07 06:34:21.419 : --33-->   0033.jpg | 23.99dB
22-11-07 06:34:21.794 : --34-->   0034.jpg | 23.42dB
22-11-07 06:34:22.103 : --35-->   0035.jpg | 27.31dB
22-11-07 06:34:22.463 : --36-->   0036.jpg | 23.35dB
22-11-07 06:34:22.628 : --37-->   0037.jpg | 23.00dB
22-11-07 06:34:22.775 : --38-->   0038.jpg | 26.89dB
22-11-07 06:34:22.926 : --39-->   0039.jpg | 24.34dB
22-11-07 06:34:23.152 : --40-->   0040.jpg | 28.18dB
22-11-07 06:34:23.318 : --41-->   0041.jpg | 21.37dB
22-11-07 06:34:23.700 : --42-->   0042.jpg | 26.71dB
22-11-07 06:34:24.081 : --43-->   0043.jpg | 27.49dB
22-11-07 06:34:24.400 : --44-->   0044.jpg | 24.75dB
22-11-07 06:34:24.656 : --45-->   0045.jpg | 21.57dB
22-11-07 06:34:24.870 : --46-->   0046.jpg | 23.31dB
22-11-07 06:34:25.459 : --47-->   0047.jpg | 24.48dB
22-11-07 06:34:25.606 : --48-->   0048.jpg | 28.78dB
22-11-07 06:34:25.794 : --49-->   0049.jpg | 28.65dB
22-11-07 06:34:25.977 : --50-->   0050.jpg | 21.29dB
22-11-07 06:34:26.150 : --51-->   0051.jpg | 26.28dB
22-11-07 06:34:26.311 : --52-->   0052.jpg | 23.69dB
22-11-07 06:34:26.462 : --53-->   0053.jpg | 24.62dB
22-11-07 06:34:26.713 : --54-->   0054.jpg | 22.14dB
22-11-07 06:34:27.113 : --55-->   0055.jpg | 36.94dB
22-11-07 06:34:27.278 : --56-->   0056.jpg | 24.98dB
22-11-07 06:34:27.457 : --57-->   0057.jpg | 30.82dB
22-11-07 06:34:27.972 : --58-->   0058.jpg | 26.84dB
22-11-07 06:34:28.172 : --59-->   0059.jpg | 28.13dB
22-11-07 06:34:28.341 : --60-->   0060.jpg | 25.05dB
22-11-07 06:34:28.489 : --61-->   0061.jpg | 26.53dB
22-11-07 06:34:28.649 : --62-->   0062.jpg | 24.54dB
22-11-07 06:34:28.908 : --63-->   0063.jpg | 23.28dB
22-11-07 06:34:29.224 : --64-->   0064.jpg | 29.08dB
22-11-07 06:34:29.531 : --65-->   0065.jpg | 35.07dB
22-11-07 06:34:29.734 : --66-->   0066.jpg | 30.23dB
22-11-07 06:34:30.032 : --67-->   0067.jpg | 26.90dB
22-11-07 06:34:30.209 : --68-->   0068.jpg | 25.29dB
22-11-07 06:34:30.484 : --69-->   0069.jpg | 19.47dB
22-11-07 06:34:30.646 : --70-->   0070.jpg | 21.26dB
22-11-07 06:34:30.808 : --71-->   0071.jpg | 22.58dB
22-11-07 06:34:30.960 : --72-->   0072.jpg | 28.35dB
22-11-07 06:34:31.271 : --73-->   0073.jpg | 27.52dB
22-11-07 06:34:31.437 : --74-->   0074.jpg | 24.61dB
22-11-07 06:34:31.603 : --75-->   0075.jpg | 24.58dB
22-11-07 06:34:31.774 : --76-->   0076.jpg | 25.46dB
22-11-07 06:34:32.104 : --77-->   0077.jpg | 22.60dB
22-11-07 06:34:32.260 : --78-->   0078.jpg | 28.00dB
22-11-07 06:34:32.780 : --79-->   0079.jpg | 20.49dB
22-11-07 06:34:32.949 : --80-->   0080.jpg | 30.27dB
22-11-07 06:34:33.180 : --81-->   0081.jpg | 28.73dB
22-11-07 06:34:33.417 : --82-->   0082.jpg | 28.52dB
22-11-07 06:34:33.570 : --83-->   0083.jpg | 23.38dB
22-11-07 06:34:34.107 : --84-->   0084.jpg | 29.21dB
22-11-07 06:34:34.266 : --85-->   0085.jpg | 21.03dB
22-11-07 06:34:34.430 : --86-->   0086.jpg | 24.51dB
22-11-07 06:34:34.592 : --87-->   0087.jpg | 21.00dB
22-11-07 06:34:35.083 : --88-->   0088.jpg | 27.35dB
22-11-07 06:34:35.251 : --89-->   0089.jpg | 23.93dB
22-11-07 06:34:35.648 : --90-->   0090.jpg | 21.82dB
22-11-07 06:34:35.824 : --91-->   0091.jpg | 31.44dB
22-11-07 06:34:36.156 : --92-->   0092.jpg | 22.49dB
22-11-07 06:34:36.655 : --93-->   0093.jpg | 27.86dB
22-11-07 06:34:37.312 : --94-->   0094.jpg | 30.61dB
22-11-07 06:34:37.469 : --95-->   0095.jpg | 25.24dB
22-11-07 06:34:37.937 : --96-->   0096.jpg | 20.92dB
22-11-07 06:34:38.155 : --97-->   0097.jpg | 32.59dB
22-11-07 06:34:38.652 : --98-->   0098.jpg | 25.70dB
22-11-07 06:34:38.810 : --99-->   0099.jpg | 24.94dB
22-11-07 06:34:38.969 : -100-->   0100.jpg | 23.54dB
22-11-07 06:34:39.090 : <epoch:723, iter: 215,000, Average PSNR : 26.44dB

22-11-07 06:35:21.538 : <epoch:724, iter: 215,200, lr:2.000e-04> G_loss: 3.058e-02 
22-11-07 06:36:04.384 : <epoch:725, iter: 215,400, lr:2.000e-04> G_loss: 2.930e-02 
22-11-07 06:36:44.655 : <epoch:725, iter: 215,600, lr:2.000e-04> G_loss: 3.560e-02 
22-11-07 06:37:27.479 : <epoch:726, iter: 215,800, lr:2.000e-04> G_loss: 3.316e-02 
22-11-07 06:38:10.089 : <epoch:727, iter: 216,000, lr:2.000e-04> G_loss: 3.166e-02 
22-11-07 06:38:50.422 : <epoch:727, iter: 216,200, lr:2.000e-04> G_loss: 3.142e-02 
22-11-07 06:39:33.225 : <epoch:728, iter: 216,400, lr:2.000e-04> G_loss: 3.750e-02 
22-11-07 06:40:16.228 : <epoch:729, iter: 216,600, lr:2.000e-04> G_loss: 2.363e-02 
22-11-07 06:40:56.366 : <epoch:729, iter: 216,800, lr:2.000e-04> G_loss: 3.019e-02 
22-11-07 06:41:39.505 : <epoch:730, iter: 217,000, lr:2.000e-04> G_loss: 3.337e-02 
22-11-07 06:42:22.398 : <epoch:731, iter: 217,200, lr:2.000e-04> G_loss: 3.446e-02 
22-11-07 06:43:02.624 : <epoch:731, iter: 217,400, lr:2.000e-04> G_loss: 3.125e-02 
22-11-07 06:43:45.881 : <epoch:732, iter: 217,600, lr:2.000e-04> G_loss: 2.920e-02 
22-11-07 06:44:28.471 : <epoch:733, iter: 217,800, lr:2.000e-04> G_loss: 3.240e-02 
22-11-07 06:45:10.781 : <epoch:734, iter: 218,000, lr:2.000e-04> G_loss: 3.455e-02 
22-11-07 06:45:51.736 : <epoch:734, iter: 218,200, lr:2.000e-04> G_loss: 3.564e-02 
22-11-07 06:46:34.436 : <epoch:735, iter: 218,400, lr:2.000e-04> G_loss: 3.304e-02 
22-11-07 06:47:17.715 : <epoch:736, iter: 218,600, lr:2.000e-04> G_loss: 3.545e-02 
22-11-07 06:47:58.673 : <epoch:736, iter: 218,800, lr:2.000e-04> G_loss: 3.750e-02 
22-11-07 06:48:41.191 : <epoch:737, iter: 219,000, lr:2.000e-04> G_loss: 3.362e-02 
22-11-07 06:49:23.902 : <epoch:738, iter: 219,200, lr:2.000e-04> G_loss: 2.683e-02 
22-11-07 06:50:04.845 : <epoch:738, iter: 219,400, lr:2.000e-04> G_loss: 2.801e-02 
22-11-07 06:50:48.083 : <epoch:739, iter: 219,600, lr:2.000e-04> G_loss: 3.374e-02 
22-11-07 06:51:31.581 : <epoch:740, iter: 219,800, lr:2.000e-04> G_loss: 3.185e-02 
22-11-07 06:52:13.346 : <epoch:740, iter: 220,000, lr:2.000e-04> G_loss: 3.060e-02 
22-11-07 06:52:13.347 : Saving the model.
22-11-07 06:52:14.235 : ---1-->   0001.jpg | 25.30dB
22-11-07 06:52:14.715 : ---2-->   0002.jpg | 25.84dB
22-11-07 06:52:15.065 : ---3-->   0003.jpg | 29.90dB
22-11-07 06:52:15.319 : ---4-->   0004.jpg | 28.40dB
22-11-07 06:52:15.468 : ---5-->   0005.jpg | 29.41dB
22-11-07 06:52:15.612 : ---6-->   0006.jpg | 33.35dB
22-11-07 06:52:15.814 : ---7-->   0007.jpg | 35.89dB
22-11-07 06:52:16.178 : ---8-->   0008.jpg | 24.39dB
22-11-07 06:52:16.607 : ---9-->   0009.jpg | 23.63dB
22-11-07 06:52:16.908 : --10-->   0010.jpg | 28.44dB
22-11-07 06:52:17.296 : --11-->   0011.jpg | 26.53dB
22-11-07 06:52:17.768 : --12-->   0012.jpg | 24.74dB
22-11-07 06:52:17.934 : --13-->   0013.jpg | 24.23dB
22-11-07 06:52:18.090 : --14-->   0014.jpg | 23.64dB
22-11-07 06:52:18.500 : --15-->   0015.jpg | 22.03dB
22-11-07 06:52:18.657 : --16-->   0016.jpg | 30.21dB
22-11-07 06:52:18.820 : --17-->   0017.jpg | 29.73dB
22-11-07 06:52:19.314 : --18-->   0018.jpg | 27.59dB
22-11-07 06:52:19.708 : --19-->   0019.jpg | 32.63dB
22-11-07 06:52:19.877 : --20-->   0020.jpg | 24.11dB
22-11-07 06:52:20.232 : --21-->   0021.jpg | 26.72dB
22-11-07 06:52:20.578 : --22-->   0022.jpg | 39.26dB
22-11-07 06:52:20.736 : --23-->   0023.jpg | 25.80dB
22-11-07 06:52:20.949 : --24-->   0024.jpg | 34.44dB
22-11-07 06:52:21.121 : --25-->   0025.jpg | 26.23dB
22-11-07 06:52:21.283 : --26-->   0026.jpg | 27.24dB
22-11-07 06:52:21.594 : --27-->   0027.jpg | 29.17dB
22-11-07 06:52:21.912 : --28-->   0028.jpg | 30.27dB
22-11-07 06:52:22.078 : --29-->   0029.jpg | 24.25dB
22-11-07 06:52:22.251 : --30-->   0030.jpg | 29.34dB
22-11-07 06:52:22.436 : --31-->   0031.jpg | 28.34dB
22-11-07 06:52:22.621 : --32-->   0032.jpg | 26.33dB
22-11-07 06:52:22.795 : --33-->   0033.jpg | 24.10dB
22-11-07 06:52:23.229 : --34-->   0034.jpg | 23.40dB
22-11-07 06:52:23.623 : --35-->   0035.jpg | 27.34dB
22-11-07 06:52:23.997 : --36-->   0036.jpg | 23.34dB
22-11-07 06:52:24.289 : --37-->   0037.jpg | 23.08dB
22-11-07 06:52:24.435 : --38-->   0038.jpg | 26.87dB
22-11-07 06:52:24.584 : --39-->   0039.jpg | 24.43dB
22-11-07 06:52:24.858 : --40-->   0040.jpg | 28.29dB
22-11-07 06:52:25.012 : --41-->   0041.jpg | 21.25dB
22-11-07 06:52:25.458 : --42-->   0042.jpg | 26.85dB
22-11-07 06:52:25.912 : --43-->   0043.jpg | 27.50dB
22-11-07 06:52:26.486 : --44-->   0044.jpg | 24.74dB
22-11-07 06:52:26.742 : --45-->   0045.jpg | 21.67dB
22-11-07 06:52:26.984 : --46-->   0046.jpg | 23.34dB
22-11-07 06:52:27.512 : --47-->   0047.jpg | 24.52dB
22-11-07 06:52:27.664 : --48-->   0048.jpg | 28.83dB
22-11-07 06:52:27.827 : --49-->   0049.jpg | 28.74dB
22-11-07 06:52:28.215 : --50-->   0050.jpg | 21.24dB
22-11-07 06:52:28.375 : --51-->   0051.jpg | 26.36dB
22-11-07 06:52:28.585 : --52-->   0052.jpg | 23.72dB
22-11-07 06:52:28.847 : --53-->   0053.jpg | 24.86dB
22-11-07 06:52:29.025 : --54-->   0054.jpg | 22.23dB
22-11-07 06:52:29.309 : --55-->   0055.jpg | 36.30dB
22-11-07 06:52:29.549 : --56-->   0056.jpg | 24.77dB
22-11-07 06:52:29.706 : --57-->   0057.jpg | 30.68dB
22-11-07 06:52:30.028 : --58-->   0058.jpg | 27.01dB
22-11-07 06:52:30.289 : --59-->   0059.jpg | 28.11dB
22-11-07 06:52:30.471 : --60-->   0060.jpg | 25.19dB
22-11-07 06:52:30.643 : --61-->   0061.jpg | 26.53dB
22-11-07 06:52:30.834 : --62-->   0062.jpg | 24.52dB
22-11-07 06:52:31.081 : --63-->   0063.jpg | 23.28dB
22-11-07 06:52:31.449 : --64-->   0064.jpg | 29.25dB
22-11-07 06:52:31.784 : --65-->   0065.jpg | 35.21dB
22-11-07 06:52:32.067 : --66-->   0066.jpg | 30.21dB
22-11-07 06:52:32.363 : --67-->   0067.jpg | 26.91dB
22-11-07 06:52:32.518 : --68-->   0068.jpg | 25.36dB
22-11-07 06:52:32.801 : --69-->   0069.jpg | 19.46dB
22-11-07 06:52:32.960 : --70-->   0070.jpg | 21.23dB
22-11-07 06:52:33.113 : --71-->   0071.jpg | 22.59dB
22-11-07 06:52:33.394 : --72-->   0072.jpg | 28.42dB
22-11-07 06:52:33.692 : --73-->   0073.jpg | 27.38dB
22-11-07 06:52:33.850 : --74-->   0074.jpg | 24.80dB
22-11-07 06:52:34.137 : --75-->   0075.jpg | 24.65dB
22-11-07 06:52:34.299 : --76-->   0076.jpg | 25.45dB
22-11-07 06:52:34.820 : --77-->   0077.jpg | 22.62dB
22-11-07 06:52:34.978 : --78-->   0078.jpg | 28.06dB
22-11-07 06:52:35.627 : --79-->   0079.jpg | 20.49dB
22-11-07 06:52:35.789 : --80-->   0080.jpg | 30.36dB
22-11-07 06:52:36.075 : --81-->   0081.jpg | 28.74dB
22-11-07 06:52:36.259 : --82-->   0082.jpg | 28.49dB
22-11-07 06:52:36.451 : --83-->   0083.jpg | 23.35dB
22-11-07 06:52:36.898 : --84-->   0084.jpg | 29.34dB
22-11-07 06:52:37.057 : --85-->   0085.jpg | 21.02dB
22-11-07 06:52:37.291 : --86-->   0086.jpg | 24.39dB
22-11-07 06:52:37.453 : --87-->   0087.jpg | 20.96dB
22-11-07 06:52:37.951 : --88-->   0088.jpg | 27.38dB
22-11-07 06:52:38.108 : --89-->   0089.jpg | 23.96dB
22-11-07 06:52:38.535 : --90-->   0090.jpg | 21.86dB
22-11-07 06:52:38.783 : --91-->   0091.jpg | 31.42dB
22-11-07 06:52:38.953 : --92-->   0092.jpg | 22.56dB
22-11-07 06:52:39.154 : --93-->   0093.jpg | 27.94dB
22-11-07 06:52:39.552 : --94-->   0094.jpg | 30.68dB
22-11-07 06:52:39.836 : --95-->   0095.jpg | 25.21dB
22-11-07 06:52:40.173 : --96-->   0096.jpg | 20.97dB
22-11-07 06:52:40.465 : --97-->   0097.jpg | 32.64dB
22-11-07 06:52:40.626 : --98-->   0098.jpg | 25.74dB
22-11-07 06:52:40.789 : --99-->   0099.jpg | 25.01dB
22-11-07 06:52:40.981 : -100-->   0100.jpg | 23.61dB
22-11-07 06:52:41.087 : <epoch:740, iter: 220,000, Average PSNR : 26.48dB

22-11-07 06:53:24.266 : <epoch:741, iter: 220,200, lr:2.000e-04> G_loss: 3.109e-02 
22-11-07 06:54:07.502 : <epoch:742, iter: 220,400, lr:2.000e-04> G_loss: 3.204e-02 
22-11-07 06:54:49.298 : <epoch:742, iter: 220,600, lr:2.000e-04> G_loss: 2.764e-02 
22-11-07 06:55:32.577 : <epoch:743, iter: 220,800, lr:2.000e-04> G_loss: 4.015e-02 
22-11-07 06:56:15.541 : <epoch:744, iter: 221,000, lr:2.000e-04> G_loss: 2.951e-02 
22-11-07 06:56:56.397 : <epoch:744, iter: 221,200, lr:2.000e-04> G_loss: 2.897e-02 
22-11-07 06:57:39.461 : <epoch:745, iter: 221,400, lr:2.000e-04> G_loss: 3.335e-02 
22-11-07 06:58:35.599 : <epoch:746, iter: 221,600, lr:2.000e-04> G_loss: 3.080e-02 
22-11-07 06:59:33.037 : <epoch:746, iter: 221,800, lr:2.000e-04> G_loss: 3.023e-02 
22-11-07 07:00:34.207 : <epoch:747, iter: 222,000, lr:2.000e-04> G_loss: 3.025e-02 
22-11-07 07:01:34.747 : <epoch:748, iter: 222,200, lr:2.000e-04> G_loss: 3.803e-02 
22-11-07 07:02:31.630 : <epoch:748, iter: 222,400, lr:2.000e-04> G_loss: 3.065e-02 
22-11-07 07:03:32.603 : <epoch:749, iter: 222,600, lr:2.000e-04> G_loss: 3.055e-02 
22-11-07 07:04:33.469 : <epoch:750, iter: 222,800, lr:2.000e-04> G_loss: 4.012e-02 
22-11-07 07:05:32.611 : <epoch:750, iter: 223,000, lr:2.000e-04> G_loss: 3.328e-02 
22-11-07 07:06:32.568 : <epoch:751, iter: 223,200, lr:2.000e-04> G_loss: 3.340e-02 
22-11-07 07:07:34.120 : <epoch:752, iter: 223,400, lr:2.000e-04> G_loss: 2.844e-02 
22-11-07 07:08:31.004 : <epoch:752, iter: 223,600, lr:2.000e-04> G_loss: 2.823e-02 
22-11-07 07:09:30.587 : <epoch:753, iter: 223,800, lr:2.000e-04> G_loss: 2.729e-02 
22-11-07 07:10:28.085 : <epoch:754, iter: 224,000, lr:2.000e-04> G_loss: 3.257e-02 
22-11-07 07:12:17.490 : <epoch:754, iter: 224,200, lr:2.000e-04> G_loss: 2.808e-02 
22-11-07 07:14:54.877 : <epoch:755, iter: 224,400, lr:2.000e-04> G_loss: 3.926e-02 
22-11-07 07:17:32.849 : <epoch:756, iter: 224,600, lr:2.000e-04> G_loss: 2.638e-02 
22-11-07 07:20:17.746 : <epoch:756, iter: 224,800, lr:2.000e-04> G_loss: 3.080e-02 
22-11-07 07:22:58.482 : <epoch:757, iter: 225,000, lr:2.000e-04> G_loss: 3.680e-02 
22-11-07 07:22:58.483 : Saving the model.
22-11-07 07:23:04.555 : ---1-->   0001.jpg | 25.31dB
22-11-07 07:23:08.236 : ---2-->   0002.jpg | 25.82dB
22-11-07 07:23:18.504 : ---3-->   0003.jpg | 29.89dB
22-11-07 07:23:25.768 : ---4-->   0004.jpg | 28.42dB
22-11-07 07:23:31.657 : ---5-->   0005.jpg | 29.26dB
22-11-07 07:23:37.932 : ---6-->   0006.jpg | 33.45dB
22-11-07 07:23:47.469 : ---7-->   0007.jpg | 36.06dB
22-11-07 07:23:53.807 : ---8-->   0008.jpg | 24.35dB
22-11-07 07:23:56.267 : ---9-->   0009.jpg | 23.52dB
22-11-07 07:24:02.592 : --10-->   0010.jpg | 28.40dB
22-11-07 07:24:09.927 : --11-->   0011.jpg | 26.37dB
22-11-07 07:24:15.520 : --12-->   0012.jpg | 24.26dB
22-11-07 07:24:18.878 : --13-->   0013.jpg | 24.26dB
22-11-07 07:24:21.525 : --14-->   0014.jpg | 23.60dB
22-11-07 07:24:24.403 : --15-->   0015.jpg | 22.10dB
22-11-07 07:24:29.591 : --16-->   0016.jpg | 30.24dB
22-11-07 07:24:34.982 : --17-->   0017.jpg | 29.83dB
22-11-07 07:24:42.143 : --18-->   0018.jpg | 27.84dB
22-11-07 07:24:52.107 : --19-->   0019.jpg | 32.58dB
22-11-07 07:24:59.732 : --20-->   0020.jpg | 24.09dB
22-11-07 07:25:10.291 : --21-->   0021.jpg | 26.65dB
22-11-07 07:25:13.149 : --22-->   0022.jpg | 39.23dB
22-11-07 07:25:17.357 : --23-->   0023.jpg | 25.58dB
22-11-07 07:25:23.823 : --24-->   0024.jpg | 34.38dB
22-11-07 07:25:27.585 : --25-->   0025.jpg | 26.14dB
22-11-07 07:25:31.332 : --26-->   0026.jpg | 27.23dB
22-11-07 07:25:37.494 : --27-->   0027.jpg | 29.21dB
22-11-07 07:25:41.939 : --28-->   0028.jpg | 30.41dB
22-11-07 07:25:44.274 : --29-->   0029.jpg | 24.25dB
22-11-07 07:25:47.621 : --30-->   0030.jpg | 29.41dB
22-11-07 07:25:48.203 : --31-->   0031.jpg | 28.26dB
22-11-07 07:25:54.548 : --32-->   0032.jpg | 26.26dB
22-11-07 07:26:04.355 : --33-->   0033.jpg | 23.93dB
22-11-07 07:26:14.118 : --34-->   0034.jpg | 23.42dB
22-11-07 07:26:20.859 : --35-->   0035.jpg | 27.26dB
22-11-07 07:26:26.592 : --36-->   0036.jpg | 23.39dB
22-11-07 07:26:33.618 : --37-->   0037.jpg | 23.09dB
22-11-07 07:26:42.610 : --38-->   0038.jpg | 26.94dB
22-11-07 07:26:48.551 : --39-->   0039.jpg | 24.32dB
22-11-07 07:26:53.846 : --40-->   0040.jpg | 28.26dB
22-11-07 07:27:01.266 : --41-->   0041.jpg | 21.36dB
22-11-07 07:27:10.482 : --42-->   0042.jpg | 26.83dB
22-11-07 07:27:23.309 : --43-->   0043.jpg | 27.57dB
22-11-07 07:27:31.042 : --44-->   0044.jpg | 24.72dB
22-11-07 07:27:33.138 : --45-->   0045.jpg | 21.74dB
22-11-07 07:27:38.598 : --46-->   0046.jpg | 23.40dB
22-11-07 07:27:43.367 : --47-->   0047.jpg | 24.52dB
22-11-07 07:27:49.554 : --48-->   0048.jpg | 28.89dB
22-11-07 07:27:54.784 : --49-->   0049.jpg | 28.73dB
22-11-07 07:28:02.630 : --50-->   0050.jpg | 21.31dB
22-11-07 07:28:06.041 : --51-->   0051.jpg | 26.38dB
22-11-07 07:28:08.131 : --52-->   0052.jpg | 23.66dB
22-11-07 07:28:16.065 : --53-->   0053.jpg | 24.74dB
22-11-07 07:28:23.382 : --54-->   0054.jpg | 22.15dB
22-11-07 07:28:31.996 : --55-->   0055.jpg | 36.56dB
22-11-07 07:28:37.300 : --56-->   0056.jpg | 24.88dB
22-11-07 07:28:43.022 : --57-->   0057.jpg | 30.81dB
22-11-07 07:28:47.472 : --58-->   0058.jpg | 27.00dB
22-11-07 07:28:52.382 : --59-->   0059.jpg | 28.04dB
22-11-07 07:28:59.517 : --60-->   0060.jpg | 24.98dB
22-11-07 07:29:07.086 : --61-->   0061.jpg | 26.60dB
22-11-07 07:29:13.868 : --62-->   0062.jpg | 24.56dB
22-11-07 07:29:19.665 : --63-->   0063.jpg | 23.30dB
22-11-07 07:29:23.894 : --64-->   0064.jpg | 29.20dB
22-11-07 07:29:34.123 : --65-->   0065.jpg | 35.24dB
22-11-07 07:29:37.293 : --66-->   0066.jpg | 30.32dB
22-11-07 07:29:44.131 : --67-->   0067.jpg | 27.00dB
22-11-07 07:29:46.453 : --68-->   0068.jpg | 25.33dB
22-11-07 07:29:50.159 : --69-->   0069.jpg | 19.49dB
22-11-07 07:29:55.871 : --70-->   0070.jpg | 21.29dB
22-11-07 07:29:58.577 : --71-->   0071.jpg | 22.59dB
22-11-07 07:30:05.830 : --72-->   0072.jpg | 28.27dB
22-11-07 07:30:14.345 : --73-->   0073.jpg | 27.36dB
22-11-07 07:30:19.919 : --74-->   0074.jpg | 24.71dB
22-11-07 07:30:29.788 : --75-->   0075.jpg | 24.63dB
22-11-07 07:30:38.011 : --76-->   0076.jpg | 25.26dB
22-11-07 07:30:44.011 : --77-->   0077.jpg | 22.65dB
22-11-07 07:30:48.809 : --78-->   0078.jpg | 28.08dB
22-11-07 07:30:52.760 : --79-->   0079.jpg | 20.51dB
22-11-07 07:30:54.189 : --80-->   0080.jpg | 30.50dB
22-11-07 07:31:00.824 : --81-->   0081.jpg | 28.70dB
22-11-07 07:31:05.911 : --82-->   0082.jpg | 28.57dB
22-11-07 07:31:11.457 : --83-->   0083.jpg | 23.37dB
22-11-07 07:31:15.173 : --84-->   0084.jpg | 29.21dB
22-11-07 07:31:23.420 : --85-->   0085.jpg | 21.07dB
22-11-07 07:31:32.494 : --86-->   0086.jpg | 24.49dB
22-11-07 07:31:36.893 : --87-->   0087.jpg | 21.01dB
22-11-07 07:31:42.283 : --88-->   0088.jpg | 27.38dB
22-11-07 07:31:44.860 : --89-->   0089.jpg | 23.91dB
22-11-07 07:31:50.266 : --90-->   0090.jpg | 21.85dB
22-11-07 07:31:54.761 : --91-->   0091.jpg | 31.25dB
22-11-07 07:31:57.631 : --92-->   0092.jpg | 22.64dB
22-11-07 07:32:04.268 : --93-->   0093.jpg | 27.80dB
22-11-07 07:32:08.048 : --94-->   0094.jpg | 30.72dB
22-11-07 07:32:13.724 : --95-->   0095.jpg | 25.18dB
22-11-07 07:32:22.792 : --96-->   0096.jpg | 20.89dB
22-11-07 07:32:26.350 : --97-->   0097.jpg | 32.63dB
22-11-07 07:32:30.419 : --98-->   0098.jpg | 25.76dB
22-11-07 07:32:33.178 : --99-->   0099.jpg | 25.02dB
22-11-07 07:32:36.808 : -100-->   0100.jpg | 23.61dB
22-11-07 07:32:37.093 : <epoch:757, iter: 225,000, Average PSNR : 26.48dB

22-11-07 07:35:18.779 : <epoch:758, iter: 225,200, lr:2.000e-04> G_loss: 2.927e-02 
22-11-07 07:38:01.190 : <epoch:758, iter: 225,400, lr:2.000e-04> G_loss: 3.051e-02 
22-11-07 07:40:39.718 : <epoch:759, iter: 225,600, lr:2.000e-04> G_loss: 3.093e-02 
22-11-07 07:43:16.673 : <epoch:760, iter: 225,800, lr:2.000e-04> G_loss: 3.147e-02 
22-11-07 07:45:56.661 : <epoch:760, iter: 226,000, lr:2.000e-04> G_loss: 3.203e-02 
22-11-07 07:48:55.457 : <epoch:761, iter: 226,200, lr:2.000e-04> G_loss: 3.250e-02 
22-11-07 07:51:32.677 : <epoch:762, iter: 226,400, lr:2.000e-04> G_loss: 3.371e-02 
22-11-07 07:54:05.110 : <epoch:762, iter: 226,600, lr:2.000e-04> G_loss: 2.969e-02 
22-11-07 07:57:03.241 : <epoch:763, iter: 226,800, lr:2.000e-04> G_loss: 3.242e-02 
22-11-07 07:59:42.069 : <epoch:764, iter: 227,000, lr:2.000e-04> G_loss: 3.314e-02 
22-11-07 08:02:24.646 : <epoch:764, iter: 227,200, lr:2.000e-04> G_loss: 2.958e-02 
22-11-07 08:05:12.837 : <epoch:765, iter: 227,400, lr:2.000e-04> G_loss: 3.624e-02 
22-11-07 08:08:09.294 : <epoch:766, iter: 227,600, lr:2.000e-04> G_loss: 3.239e-02 
22-11-07 08:10:56.479 : <epoch:767, iter: 227,800, lr:2.000e-04> G_loss: 2.960e-02 
22-11-07 08:13:45.290 : <epoch:767, iter: 228,000, lr:2.000e-04> G_loss: 3.834e-02 
22-11-07 08:16:26.947 : <epoch:768, iter: 228,200, lr:2.000e-04> G_loss: 3.325e-02 
22-11-07 08:19:13.850 : <epoch:769, iter: 228,400, lr:2.000e-04> G_loss: 2.818e-02 
22-11-07 08:21:48.847 : <epoch:769, iter: 228,600, lr:2.000e-04> G_loss: 3.612e-02 
22-11-07 08:24:26.476 : <epoch:770, iter: 228,800, lr:2.000e-04> G_loss: 2.919e-02 
22-11-07 08:27:05.573 : <epoch:771, iter: 229,000, lr:2.000e-04> G_loss: 3.308e-02 
22-11-07 08:29:39.436 : <epoch:771, iter: 229,200, lr:2.000e-04> G_loss: 3.003e-02 
22-11-07 08:32:19.894 : <epoch:772, iter: 229,400, lr:2.000e-04> G_loss: 3.462e-02 
22-11-07 08:35:03.402 : <epoch:773, iter: 229,600, lr:2.000e-04> G_loss: 2.892e-02 
22-11-07 08:37:39.489 : <epoch:773, iter: 229,800, lr:2.000e-04> G_loss: 3.285e-02 
22-11-07 08:40:28.243 : <epoch:774, iter: 230,000, lr:2.000e-04> G_loss: 3.378e-02 
22-11-07 08:40:28.289 : Saving the model.
22-11-07 08:40:39.559 : ---1-->   0001.jpg | 25.32dB
22-11-07 08:40:44.897 : ---2-->   0002.jpg | 25.91dB
22-11-07 08:40:49.626 : ---3-->   0003.jpg | 29.96dB
22-11-07 08:40:51.167 : ---4-->   0004.jpg | 28.52dB
22-11-07 08:40:55.622 : ---5-->   0005.jpg | 29.35dB
22-11-07 08:41:04.143 : ---6-->   0006.jpg | 33.49dB
22-11-07 08:41:08.428 : ---7-->   0007.jpg | 35.71dB
22-11-07 08:41:13.276 : ---8-->   0008.jpg | 24.33dB
22-11-07 08:41:19.233 : ---9-->   0009.jpg | 23.54dB
22-11-07 08:41:26.192 : --10-->   0010.jpg | 28.46dB
22-11-07 08:41:32.947 : --11-->   0011.jpg | 26.48dB
22-11-07 08:41:43.021 : --12-->   0012.jpg | 24.69dB
22-11-07 08:41:48.049 : --13-->   0013.jpg | 24.21dB
22-11-07 08:41:49.034 : --14-->   0014.jpg | 23.63dB
22-11-07 08:41:57.186 : --15-->   0015.jpg | 22.07dB
22-11-07 08:42:05.011 : --16-->   0016.jpg | 30.36dB
22-11-07 08:42:08.487 : --17-->   0017.jpg | 29.76dB
22-11-07 08:42:18.841 : --18-->   0018.jpg | 27.41dB
22-11-07 08:42:25.321 : --19-->   0019.jpg | 32.68dB
22-11-07 08:42:31.442 : --20-->   0020.jpg | 24.13dB
22-11-07 08:42:38.320 : --21-->   0021.jpg | 26.72dB
22-11-07 08:42:43.044 : --22-->   0022.jpg | 39.16dB
22-11-07 08:42:51.103 : --23-->   0023.jpg | 25.59dB
22-11-07 08:42:58.184 : --24-->   0024.jpg | 34.43dB
22-11-07 08:43:04.047 : --25-->   0025.jpg | 26.23dB
22-11-07 08:43:09.720 : --26-->   0026.jpg | 27.24dB
22-11-07 08:43:17.756 : --27-->   0027.jpg | 29.11dB
22-11-07 08:43:27.050 : --28-->   0028.jpg | 30.52dB
22-11-07 08:43:29.614 : --29-->   0029.jpg | 24.26dB
22-11-07 08:43:31.522 : --30-->   0030.jpg | 29.38dB
22-11-07 08:43:34.366 : --31-->   0031.jpg | 28.28dB
22-11-07 08:43:38.646 : --32-->   0032.jpg | 26.33dB
22-11-07 08:43:40.852 : --33-->   0033.jpg | 23.96dB
22-11-07 08:43:51.209 : --34-->   0034.jpg | 23.50dB
22-11-07 08:44:00.217 : --35-->   0035.jpg | 27.59dB
22-11-07 08:44:06.452 : --36-->   0036.jpg | 23.38dB
22-11-07 08:44:09.136 : --37-->   0037.jpg | 23.09dB
22-11-07 08:44:17.034 : --38-->   0038.jpg | 26.85dB
22-11-07 08:44:22.562 : --39-->   0039.jpg | 24.20dB
22-11-07 08:44:28.967 : --40-->   0040.jpg | 28.25dB
22-11-07 08:44:32.588 : --41-->   0041.jpg | 21.30dB
22-11-07 08:44:36.519 : --42-->   0042.jpg | 26.77dB
22-11-07 08:44:40.465 : --43-->   0043.jpg | 27.52dB
22-11-07 08:44:47.486 : --44-->   0044.jpg | 24.73dB
22-11-07 08:44:50.693 : --45-->   0045.jpg | 21.71dB
22-11-07 08:44:56.634 : --46-->   0046.jpg | 23.41dB
22-11-07 08:45:04.530 : --47-->   0047.jpg | 24.50dB
22-11-07 08:45:07.439 : --48-->   0048.jpg | 28.77dB
22-11-07 08:45:11.375 : --49-->   0049.jpg | 28.69dB
22-11-07 08:45:15.220 : --50-->   0050.jpg | 21.30dB
22-11-07 08:45:19.077 : --51-->   0051.jpg | 26.33dB
22-11-07 08:45:22.859 : --52-->   0052.jpg | 23.71dB
22-11-07 08:45:25.349 : --53-->   0053.jpg | 24.70dB
22-11-07 08:45:33.402 : --54-->   0054.jpg | 22.17dB
22-11-07 08:45:40.674 : --55-->   0055.jpg | 35.92dB
22-11-07 08:45:44.544 : --56-->   0056.jpg | 24.86dB
22-11-07 08:45:51.756 : --57-->   0057.jpg | 30.81dB
22-11-07 08:46:00.399 : --58-->   0058.jpg | 26.81dB
22-11-07 08:46:05.802 : --59-->   0059.jpg | 28.14dB
22-11-07 08:46:13.775 : --60-->   0060.jpg | 25.00dB
22-11-07 08:46:18.166 : --61-->   0061.jpg | 26.56dB
22-11-07 08:46:27.271 : --62-->   0062.jpg | 24.55dB
22-11-07 08:46:34.821 : --63-->   0063.jpg | 23.31dB
22-11-07 08:46:43.691 : --64-->   0064.jpg | 29.25dB
22-11-07 08:46:52.164 : --65-->   0065.jpg | 35.20dB
22-11-07 08:46:53.940 : --66-->   0066.jpg | 30.10dB
22-11-07 08:46:58.720 : --67-->   0067.jpg | 26.99dB
22-11-07 08:47:05.514 : --68-->   0068.jpg | 25.34dB
22-11-07 08:47:07.717 : --69-->   0069.jpg | 19.50dB
22-11-07 08:47:10.249 : --70-->   0070.jpg | 21.30dB
22-11-07 08:47:15.357 : --71-->   0071.jpg | 22.59dB
22-11-07 08:47:18.672 : --72-->   0072.jpg | 28.36dB
22-11-07 08:47:24.778 : --73-->   0073.jpg | 27.46dB
22-11-07 08:47:30.416 : --74-->   0074.jpg | 24.88dB
22-11-07 08:47:36.619 : --75-->   0075.jpg | 24.63dB
22-11-07 08:47:41.964 : --76-->   0076.jpg | 25.41dB
22-11-07 08:47:45.504 : --77-->   0077.jpg | 22.56dB
22-11-07 08:47:47.593 : --78-->   0078.jpg | 28.12dB
22-11-07 08:47:56.462 : --79-->   0079.jpg | 20.52dB
22-11-07 08:47:59.472 : --80-->   0080.jpg | 30.40dB
22-11-07 08:48:06.259 : --81-->   0081.jpg | 28.72dB
22-11-07 08:48:12.525 : --82-->   0082.jpg | 28.60dB
22-11-07 08:48:15.900 : --83-->   0083.jpg | 23.44dB
22-11-07 08:48:21.994 : --84-->   0084.jpg | 29.28dB
22-11-07 08:48:26.943 : --85-->   0085.jpg | 21.11dB
22-11-07 08:48:32.213 : --86-->   0086.jpg | 24.46dB
22-11-07 08:48:38.295 : --87-->   0087.jpg | 20.98dB
22-11-07 08:48:47.732 : --88-->   0088.jpg | 27.11dB
22-11-07 08:48:56.445 : --89-->   0089.jpg | 23.91dB
22-11-07 08:49:02.138 : --90-->   0090.jpg | 21.83dB
22-11-07 08:49:08.296 : --91-->   0091.jpg | 31.41dB
22-11-07 08:49:12.440 : --92-->   0092.jpg | 22.67dB
22-11-07 08:49:20.319 : --93-->   0093.jpg | 27.86dB
22-11-07 08:49:30.200 : --94-->   0094.jpg | 30.61dB
22-11-07 08:49:33.449 : --95-->   0095.jpg | 25.22dB
22-11-07 08:49:38.628 : --96-->   0096.jpg | 20.93dB
22-11-07 08:49:44.704 : --97-->   0097.jpg | 32.65dB
22-11-07 08:49:51.481 : --98-->   0098.jpg | 25.82dB
22-11-07 08:49:57.195 : --99-->   0099.jpg | 24.98dB
22-11-07 08:49:59.307 : -100-->   0100.jpg | 23.62dB
22-11-07 08:49:59.467 : <epoch:774, iter: 230,000, Average PSNR : 26.47dB

22-11-07 08:52:39.920 : <epoch:775, iter: 230,200, lr:2.000e-04> G_loss: 2.936e-02 
22-11-07 08:55:22.304 : <epoch:775, iter: 230,400, lr:2.000e-04> G_loss: 3.298e-02 
22-11-07 08:58:09.108 : <epoch:776, iter: 230,600, lr:2.000e-04> G_loss: 3.434e-02 
22-11-07 09:00:46.841 : <epoch:777, iter: 230,800, lr:2.000e-04> G_loss: 3.764e-02 
22-11-07 09:03:34.924 : <epoch:777, iter: 231,000, lr:2.000e-04> G_loss: 3.753e-02 
22-11-07 09:06:19.621 : <epoch:778, iter: 231,200, lr:2.000e-04> G_loss: 2.872e-02 
22-11-08 10:49:41.457 :   task: swinir_sr_classical_patch48_x8
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 8
  n_channels: 3
  path:[
    root: superresolution
    pretrained_netG: superresolution/swinir_sr_classical_patch48_x8/models/230000_G.pth
    pretrained_netE: superresolution/swinir_sr_classical_patch48_x8/models/230000_E.pth
    task: superresolution/swinir_sr_classical_patch48_x8
    log: superresolution/swinir_sr_classical_patch48_x8
    options: superresolution/swinir_sr_classical_patch48_x8/options
    models: superresolution/swinir_sr_classical_patch48_x8/models
    images: superresolution/swinir_sr_classical_patch48_x8/images
    pretrained_optimizerG: superresolution/swinir_sr_classical_patch48_x8/models/230000_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/WIN/train-HR
      dataroot_L: trainsets/WIN/train-LRx8
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 16
      dataloader_batch_size: 24
      phase: train
      scale: 8
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/WIN/test-HR-100
      dataroot_L: testsets/WIN/test-LRx8-100
      phase: test
      scale: 8
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 8
    in_chans: 3
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 8
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 5000
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/swinir/train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  use_static_graph: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-11-08 10:49:41.809 : Number of train images: 7,142, iters: 298
22-11-08 10:55:40.583 : 
Networks name: SwinIR
Params number: 12047911
Net structure:
SwinIR(
  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.003)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.006)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.011)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.014)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.017)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.020)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.023)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.026)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.029)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.031)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.034)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.037)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.040)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.043)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.046)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.049)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.051)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.054)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.057)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.060)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.063)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.066)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.069)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.071)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.074)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.077)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.080)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.083)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.086)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.089)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.091)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.094)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.097)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): PixelShuffle(upscale_factor=2)
    (4): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-11-08 10:55:40.998 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.001 | -0.455 |  0.438 |  0.144 | torch.Size([180, 3, 3, 3]) || conv_first.weight
 |  0.003 | -0.442 |  0.737 |  0.158 | torch.Size([180]) || conv_first.bias
 |  1.091 |  0.725 |  1.696 |  0.158 | torch.Size([180]) || patch_embed.norm.weight
 | -0.002 | -0.554 |  0.591 |  0.091 | torch.Size([180]) || patch_embed.norm.bias
 |  0.453 |  0.066 |  0.807 |  0.139 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.776 |  0.698 |  0.294 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.019 | -0.579 |  0.672 |  0.131 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.437 |  0.386 |  0.044 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 | -0.004 | -0.456 |  0.382 |  0.090 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.352 |  0.439 |  0.031 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.001 | -0.509 |  0.427 |  0.073 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  0.728 |  0.267 |  1.361 |  0.162 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.007 | -0.434 |  0.481 |  0.218 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.493 |  0.439 |  0.052 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 | -0.071 | -0.218 |  0.163 |  0.060 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.001 | -0.510 |  0.549 |  0.053 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.002 | -0.388 |  0.383 |  0.059 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  0.464 |  0.017 |  0.793 |  0.123 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 | -0.001 | -0.799 |  0.826 |  0.194 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 | -0.011 | -0.415 |  0.409 |  0.112 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.389 |  0.434 |  0.062 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.001 | -0.387 |  0.459 |  0.109 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.355 |  0.325 |  0.044 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.003 | -0.268 |  0.315 |  0.058 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  0.633 |  0.132 |  0.937 |  0.148 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.502 |  0.396 |  0.118 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 | -0.001 | -0.405 |  0.471 |  0.056 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 | -0.054 | -0.248 |  0.042 |  0.067 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.001 | -0.358 |  0.391 |  0.041 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.002 | -0.153 |  0.227 |  0.049 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  0.609 |  0.097 |  0.932 |  0.138 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 |  0.001 | -0.710 |  0.810 |  0.164 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 | -0.000 | -1.080 |  0.511 |  0.142 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.618 |  0.606 |  0.074 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 | -0.000 | -0.551 |  0.530 |  0.123 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.433 |  0.476 |  0.052 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.003 | -0.172 |  0.200 |  0.061 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  0.704 |  0.416 |  1.176 |  0.123 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 | -0.002 | -0.396 |  0.357 |  0.095 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.563 |  0.583 |  0.056 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 | -0.036 | -0.259 |  0.040 |  0.056 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.407 |  0.492 |  0.039 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.004 | -0.116 |  0.176 |  0.046 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  0.642 |  0.095 |  0.944 |  0.152 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 |  0.004 | -0.900 |  0.861 |  0.142 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.017 | -1.098 |  0.579 |  0.145 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.507 |  0.540 |  0.084 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 | -0.003 | -0.514 |  0.533 |  0.136 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.331 |  0.360 |  0.057 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.005 | -0.130 |  0.371 |  0.058 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  0.676 |  0.286 |  0.990 |  0.124 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.006 | -0.368 |  0.507 |  0.083 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 | -0.001 | -0.573 |  0.476 |  0.059 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 | -0.032 | -0.321 |  0.038 |  0.064 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.450 |  0.431 |  0.039 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.003 | -0.108 |  0.424 |  0.050 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.740 |  0.138 |  0.998 |  0.116 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.016 | -0.857 |  0.893 |  0.116 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 | -0.015 | -1.396 |  0.882 |  0.197 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.653 |  0.562 |  0.086 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 | -0.004 | -0.478 |  0.477 |  0.100 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.378 |  0.360 |  0.055 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.006 | -0.144 |  0.650 |  0.066 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  0.731 |  0.375 |  1.127 |  0.114 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 |  0.017 | -0.343 |  0.472 |  0.062 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 | -0.003 | -0.554 |  0.456 |  0.056 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 | -0.011 | -0.279 |  0.054 |  0.038 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.616 |  0.436 |  0.040 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.007 | -0.154 |  0.627 |  0.057 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  0.708 |  0.077 |  0.929 |  0.125 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 |  0.027 | -0.922 |  0.745 |  0.101 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 | -0.026 | -0.725 |  0.843 |  0.167 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.504 |  0.453 |  0.088 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 | -0.002 | -0.383 |  0.372 |  0.086 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.516 |  0.477 |  0.055 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.007 | -0.380 |  0.554 |  0.062 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  0.786 |  0.531 |  1.094 |  0.105 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 |  0.027 | -0.272 |  0.321 |  0.047 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 | -0.001 | -0.403 |  0.430 |  0.060 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 | -0.019 | -0.261 |  0.045 |  0.034 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.001 | -0.673 |  0.478 |  0.044 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.007 | -0.356 |  0.429 |  0.051 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.749 |  0.643 |  0.063 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 |  0.002 | -0.344 |  0.673 |  0.083 | torch.Size([180]) || layers.0.conv.bias
 |  0.669 |  0.289 |  0.978 |  0.137 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.005 | -0.874 |  0.664 |  0.189 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.009 | -1.646 |  1.562 |  0.220 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.543 |  0.445 |  0.071 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 | -0.001 | -0.244 |  0.240 |  0.062 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.363 |  0.293 |  0.065 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.002 | -0.312 |  0.362 |  0.076 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  0.948 |  0.442 |  1.264 |  0.145 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.004 | -0.259 |  0.323 |  0.088 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.001 | -0.543 |  0.441 |  0.085 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 | -0.062 | -0.254 |  0.069 |  0.048 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.417 |  0.479 |  0.076 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.387 |  0.214 |  0.070 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  0.774 |  0.528 |  1.012 |  0.099 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.007 | -0.811 |  0.740 |  0.194 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 | -0.042 | -2.366 |  1.638 |  0.305 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.563 |  0.688 |  0.079 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 | -0.001 | -0.230 |  0.242 |  0.051 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.455 |  0.462 |  0.072 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.001 | -0.433 |  0.318 |  0.082 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  0.935 |  0.545 |  1.216 |  0.124 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.006 | -0.393 |  0.218 |  0.083 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.001 | -0.469 |  0.456 |  0.091 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 | -0.059 | -0.200 |  0.033 |  0.030 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.461 |  0.433 |  0.081 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.444 |  0.278 |  0.071 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  0.815 |  0.394 |  1.060 |  0.123 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.006 | -0.936 |  0.728 |  0.159 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 | -0.012 | -1.888 |  1.205 |  0.216 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.624 |  0.528 |  0.081 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.004 | -0.256 |  0.269 |  0.054 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.395 |  0.340 |  0.079 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.311 |  0.317 |  0.072 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  0.958 |  0.618 |  1.264 |  0.113 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.005 | -0.400 |  0.288 |  0.082 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.519 |  0.416 |  0.094 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 | -0.066 | -0.249 |  0.007 |  0.034 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.409 |  0.470 |  0.081 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.001 | -0.315 |  0.264 |  0.062 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  0.792 |  0.165 |  1.021 |  0.113 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.004 | -0.976 |  0.830 |  0.140 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.768 |  0.849 |  0.173 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.492 |  0.455 |  0.085 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.002 | -0.285 |  0.249 |  0.056 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.358 |  0.374 |  0.077 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 | -0.002 | -0.433 |  0.283 |  0.066 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  0.989 |  0.617 |  1.270 |  0.096 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.008 | -0.402 |  0.212 |  0.073 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 | -0.001 | -0.523 |  0.428 |  0.096 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 | -0.068 | -0.267 |  0.051 |  0.037 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.457 |  0.446 |  0.083 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.437 |  0.250 |  0.061 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.903 |  0.299 |  1.091 |  0.106 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 |  0.005 | -0.981 |  0.848 |  0.129 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.066 | -2.334 |  1.494 |  0.300 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.442 |  0.477 |  0.091 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.002 | -0.198 |  0.258 |  0.044 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.377 |  0.335 |  0.078 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.744 |  0.220 |  0.073 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  0.998 |  0.547 |  1.189 |  0.093 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 |  0.008 | -0.498 |  0.228 |  0.072 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 | -0.001 | -0.425 |  0.409 |  0.096 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 | -0.068 | -0.284 |  0.008 |  0.033 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.423 |  0.548 |  0.087 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.728 |  0.188 |  0.070 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  0.910 |  0.333 |  1.077 |  0.095 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.002 | -0.988 |  0.933 |  0.131 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 | -0.082 | -1.927 |  1.840 |  0.333 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.600 |  0.480 |  0.091 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.001 | -0.178 |  0.288 |  0.040 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.682 |  0.824 |  0.078 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 | -0.005 | -1.174 |  0.323 |  0.096 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  0.921 |  0.566 |  1.145 |  0.072 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.006 | -0.384 |  0.218 |  0.069 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.001 | -0.584 |  0.609 |  0.091 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 | -0.068 | -0.151 |  0.431 |  0.044 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.001 | -1.418 |  1.311 |  0.093 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 | -0.006 | -1.563 |  0.665 |  0.132 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.484 |  0.394 |  0.060 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 |  0.001 | -0.415 |  0.496 |  0.097 | torch.Size([180]) || layers.1.conv.bias
 |  0.782 |  0.411 |  1.040 |  0.106 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 |  0.011 | -0.564 |  0.496 |  0.116 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 | -0.027 | -1.871 |  1.506 |  0.239 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.426 |  0.380 |  0.078 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.003 | -0.222 |  0.267 |  0.067 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.001 | -0.396 |  0.379 |  0.085 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 | -0.003 | -0.281 |  0.193 |  0.056 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.082 |  0.791 |  1.388 |  0.109 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 |  0.009 | -0.220 |  0.300 |  0.093 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.434 |  0.608 |  0.100 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 | -0.071 | -0.165 |  0.039 |  0.037 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 |  0.001 | -0.441 |  0.476 |  0.101 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 | -0.004 | -0.311 |  0.203 |  0.057 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  0.787 |  0.323 |  0.984 |  0.097 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.005 | -0.805 |  0.506 |  0.135 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 | -0.020 | -0.956 |  1.584 |  0.238 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.434 |  0.339 |  0.082 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 | -0.002 | -0.262 |  0.239 |  0.056 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.385 |  0.411 |  0.087 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 | -0.004 | -0.383 |  0.208 |  0.062 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.088 |  0.692 |  1.307 |  0.104 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.007 | -0.237 |  0.283 |  0.104 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 |  0.001 | -0.432 |  0.459 |  0.100 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 | -0.072 | -0.165 |  0.026 |  0.030 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 |  0.001 | -0.445 |  0.530 |  0.102 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 | -0.006 | -0.378 |  0.158 |  0.067 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  0.829 |  0.151 |  1.009 |  0.114 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.005 | -0.957 |  0.600 |  0.145 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.005 | -1.192 |  1.064 |  0.219 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.365 |  0.395 |  0.086 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.001 | -0.317 |  0.245 |  0.053 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.358 |  0.383 |  0.090 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 | -0.005 | -0.408 |  0.173 |  0.061 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.113 |  0.714 |  1.336 |  0.095 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.010 | -0.280 |  0.229 |  0.092 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.436 |  0.464 |  0.102 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 | -0.067 | -0.142 |  0.018 |  0.029 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.554 |  0.528 |  0.105 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 | -0.005 | -0.374 |  0.175 |  0.068 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  0.797 |  0.196 |  1.014 |  0.114 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.006 | -0.834 |  0.624 |  0.142 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.020 | -0.708 |  0.893 |  0.166 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.432 |  0.414 |  0.084 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.001 | -0.190 |  0.198 |  0.042 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.431 |  0.379 |  0.086 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 | -0.006 | -0.391 |  0.125 |  0.070 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.105 |  0.638 |  1.330 |  0.102 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.010 | -0.410 |  0.214 |  0.098 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.518 |  0.476 |  0.101 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 | -0.072 | -0.151 |  0.011 |  0.028 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.477 |  0.429 |  0.105 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 | -0.007 | -0.403 |  0.155 |  0.082 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  0.894 |  0.312 |  1.112 |  0.105 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.004 | -0.837 |  0.732 |  0.137 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 |  0.009 | -2.451 |  1.781 |  0.306 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.452 |  0.432 |  0.088 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 | -0.002 | -0.175 |  0.269 |  0.038 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.419 |  0.452 |  0.092 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 | -0.009 | -0.558 |  0.125 |  0.083 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.132 |  0.592 |  1.337 |  0.109 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 |  0.008 | -0.414 |  0.248 |  0.094 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 | -0.001 | -0.479 |  0.434 |  0.102 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 | -0.076 | -0.169 |  0.008 |  0.032 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.465 |  0.577 |  0.107 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 | -0.009 | -0.700 |  0.160 |  0.092 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  0.880 |  0.389 |  1.040 |  0.088 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.001 | -0.735 |  0.672 |  0.124 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 | -0.012 | -2.597 |  1.610 |  0.282 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.457 |  0.350 |  0.090 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.001 | -0.213 |  0.160 |  0.038 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.691 |  0.835 |  0.091 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 | -0.010 | -0.927 |  0.147 |  0.094 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.126 |  0.750 |  1.365 |  0.082 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 |  0.008 | -0.287 |  0.192 |  0.082 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 | -0.002 | -0.483 |  0.469 |  0.102 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 | -0.076 | -0.190 |  0.249 |  0.043 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.001 | -0.981 |  0.471 |  0.107 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 | -0.013 | -0.974 |  0.216 |  0.108 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.380 |  0.356 |  0.060 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 |  0.006 | -0.375 |  0.488 |  0.116 | torch.Size([180]) || layers.2.conv.bias
 |  0.846 |  0.469 |  1.070 |  0.115 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.006 | -0.441 |  0.278 |  0.085 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 |  0.018 | -2.354 |  1.467 |  0.238 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.444 |  0.453 |  0.087 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 | -0.002 | -0.226 |  0.285 |  0.067 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.429 |  0.409 |  0.101 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.002 | -0.197 |  0.222 |  0.063 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.316 |  0.887 |  1.648 |  0.121 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.336 |  0.384 |  0.131 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 |  0.001 | -0.563 |  0.537 |  0.112 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 | -0.091 | -0.237 |  0.070 |  0.054 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 | -0.001 | -0.539 |  0.580 |  0.114 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 | -0.005 | -0.161 |  0.135 |  0.050 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  0.922 |  0.558 |  1.136 |  0.102 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.006 | -0.446 |  0.341 |  0.102 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 |  0.026 | -2.396 |  1.351 |  0.302 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.473 |  0.522 |  0.091 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.001 | -0.239 |  0.246 |  0.059 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 |  0.001 | -0.467 |  0.450 |  0.105 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.004 | -0.213 |  0.210 |  0.066 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.349 |  1.000 |  1.598 |  0.113 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 | -0.001 | -0.251 |  0.270 |  0.099 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 |  0.001 | -0.539 |  0.559 |  0.114 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 | -0.080 | -0.251 |  0.064 |  0.052 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 | -0.001 | -0.488 |  0.526 |  0.119 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 | -0.003 | -0.194 |  0.188 |  0.061 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  0.953 |  0.540 |  1.205 |  0.103 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.006 | -0.537 |  0.470 |  0.113 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 |  0.015 | -3.050 |  1.869 |  0.319 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.487 |  0.468 |  0.093 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.003 | -0.210 |  0.211 |  0.054 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.590 |  0.561 |  0.110 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.009 | -0.247 |  0.286 |  0.088 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.355 |  0.814 |  1.573 |  0.122 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 | -0.001 | -0.213 |  0.287 |  0.092 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 |  0.001 | -0.498 |  0.586 |  0.114 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 | -0.077 | -0.246 |  0.074 |  0.050 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.001 | -0.608 |  0.570 |  0.122 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 | -0.002 | -0.257 |  0.214 |  0.078 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  0.933 |  0.453 |  1.163 |  0.114 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 |  0.005 | -0.588 |  0.412 |  0.107 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 |  0.014 | -1.816 |  1.799 |  0.306 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.503 |  0.436 |  0.094 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 | -0.004 | -0.271 |  0.227 |  0.054 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.468 |  0.450 |  0.109 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.003 | -0.183 |  0.211 |  0.079 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.358 |  0.879 |  1.645 |  0.131 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.004 | -0.282 |  0.250 |  0.095 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.001 | -0.472 |  0.522 |  0.114 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 | -0.079 | -0.269 |  0.061 |  0.046 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.601 |  0.620 |  0.126 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 | -0.002 | -0.251 |  0.275 |  0.093 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  0.934 |  0.477 |  1.125 |  0.100 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.008 | -0.583 |  0.476 |  0.106 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 |  0.011 | -2.413 |  1.631 |  0.341 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.497 |  0.468 |  0.094 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.233 |  0.177 |  0.051 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.554 |  0.506 |  0.114 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.007 | -0.354 |  0.243 |  0.097 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.391 |  0.977 |  1.628 |  0.122 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 | -0.221 |  0.263 |  0.082 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.002 | -0.542 |  0.573 |  0.116 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 | -0.077 | -0.241 |  0.075 |  0.043 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.817 |  0.879 |  0.134 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.257 |  0.261 |  0.099 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  0.974 |  0.519 |  1.142 |  0.089 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.007 | -0.445 |  0.245 |  0.089 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 | -0.003 | -2.971 |  2.243 |  0.372 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.473 |  0.547 |  0.097 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 | -0.003 | -0.210 |  0.187 |  0.054 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.501 |  0.514 |  0.116 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.012 | -0.279 |  0.349 |  0.109 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.364 |  1.001 |  1.599 |  0.118 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 |  0.002 | -0.193 |  0.250 |  0.076 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.002 | -0.466 |  0.518 |  0.116 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 | -0.072 | -0.265 |  0.089 |  0.044 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.625 |  0.601 |  0.137 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.001 | -0.232 |  0.245 |  0.084 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 |  0.001 | -0.333 |  0.332 |  0.064 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 |  0.022 | -0.407 |  0.485 |  0.152 | torch.Size([180]) || layers.3.conv.bias
 |  1.008 |  0.617 |  1.370 |  0.128 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 |  0.002 | -0.208 |  0.263 |  0.084 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 | -0.025 | -2.483 |  1.960 |  0.293 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.676 |  0.653 |  0.102 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.233 |  0.222 |  0.060 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 | -0.001 | -0.596 |  0.595 |  0.132 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 |  0.024 | -0.184 |  0.239 |  0.090 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.688 |  1.163 |  2.083 |  0.163 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.310 |  0.247 |  0.097 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 |  0.001 | -0.823 |  0.666 |  0.144 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 | -0.046 | -0.381 |  0.171 |  0.086 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 | -0.002 | -0.792 |  0.716 |  0.143 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 |  0.003 | -0.140 |  0.159 |  0.057 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.029 |  0.556 |  1.477 |  0.146 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 |  0.003 | -0.293 |  0.326 |  0.110 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 | -0.017 | -2.284 |  2.064 |  0.295 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.641 |  0.610 |  0.103 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 |  0.006 | -0.187 |  0.216 |  0.065 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.598 |  0.628 |  0.138 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.019 | -0.248 |  0.280 |  0.111 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.711 |  1.136 |  2.149 |  0.174 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 |  0.002 | -0.222 |  0.250 |  0.077 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 |  0.002 | -0.738 |  0.587 |  0.144 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 | -0.044 | -0.322 |  0.160 |  0.083 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 | -0.001 | -0.789 |  0.821 |  0.156 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 |  0.001 | -0.175 |  0.186 |  0.072 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.011 |  0.484 |  1.279 |  0.126 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 |  0.005 | -0.341 |  0.345 |  0.122 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 | -0.017 | -1.837 |  1.571 |  0.277 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.715 |  0.614 |  0.103 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 | -0.002 | -0.246 |  0.217 |  0.064 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.690 |  0.709 |  0.138 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 |  0.020 | -0.249 |  0.275 |  0.112 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.706 |  1.142 |  2.105 |  0.179 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.002 | -0.119 |  0.155 |  0.055 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 |  0.002 | -0.723 |  0.652 |  0.145 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 | -0.034 | -0.222 |  0.149 |  0.073 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 | -0.002 | -0.817 |  0.969 |  0.171 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 | -0.003 | -0.215 |  0.229 |  0.075 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.036 |  0.539 |  1.348 |  0.132 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 |  0.010 | -0.382 |  0.388 |  0.145 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 | -0.030 | -1.812 |  1.674 |  0.260 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.642 |  0.601 |  0.100 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 |  0.001 | -0.209 |  0.202 |  0.068 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.696 |  0.625 |  0.134 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 |  0.011 | -0.234 |  0.262 |  0.120 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.629 |  0.971 |  1.992 |  0.184 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 |  0.003 | -0.170 |  0.110 |  0.054 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 |  0.001 | -0.730 |  0.709 |  0.141 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 | -0.027 | -0.200 |  0.161 |  0.063 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 | -0.001 | -0.770 |  0.956 |  0.181 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.251 |  0.258 |  0.084 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.027 |  0.508 |  1.315 |  0.121 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.009 | -0.468 |  0.424 |  0.166 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 | -0.032 | -1.941 |  1.758 |  0.270 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.617 |  0.713 |  0.102 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 |  0.003 | -0.207 |  0.179 |  0.065 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.720 |  0.829 |  0.140 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 |  0.011 | -0.217 |  0.269 |  0.129 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.554 |  1.060 |  2.007 |  0.158 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 |  0.005 | -0.234 |  0.175 |  0.068 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 |  0.001 | -0.651 |  0.656 |  0.138 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 | -0.015 | -0.290 |  0.103 |  0.050 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 | -0.002 | -0.982 |  1.152 |  0.193 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 |  0.003 | -0.238 |  0.244 |  0.078 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.090 |  0.625 |  1.320 |  0.118 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 |  0.006 | -0.340 |  0.338 |  0.168 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 | -0.031 | -2.376 |  1.739 |  0.254 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.768 |  0.627 |  0.107 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 |  0.001 | -0.204 |  0.213 |  0.063 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 |  0.001 | -0.735 |  0.719 |  0.150 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 |  0.011 | -0.318 |  0.314 |  0.169 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.526 |  0.994 |  1.948 |  0.174 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 |  0.009 | -0.274 |  0.241 |  0.097 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 |  0.001 | -0.819 |  0.743 |  0.138 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 | -0.007 | -0.170 |  0.133 |  0.048 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -1.069 |  1.059 |  0.195 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 |  0.006 | -0.243 |  0.183 |  0.065 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.487 |  0.580 |  0.086 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 | -0.019 | -0.589 |  0.592 |  0.249 | torch.Size([180]) || layers.4.conv.bias
 |  1.006 |  0.723 |  1.244 |  0.110 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.443 |  0.399 |  0.173 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.150 |  0.190 |  0.040 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.373 |  0.382 |  0.059 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 | -0.004 | -0.166 |  0.162 |  0.067 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.307 |  0.335 |  0.060 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 | -0.003 | -0.187 |  0.197 |  0.080 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.343 |  0.853 |  2.092 |  0.239 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 | -0.003 | -0.460 |  0.484 |  0.234 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.003 | -0.640 |  0.648 |  0.118 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.083 | -0.023 |  0.253 |  0.056 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 |  0.001 | -0.740 |  0.752 |  0.124 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 | -0.002 | -0.182 |  0.153 |  0.070 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.000 |  0.712 |  1.330 |  0.108 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 | -0.002 | -0.505 |  0.367 |  0.182 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.152 |  0.765 |  0.056 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.374 |  0.386 |  0.060 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 | -0.001 | -0.140 |  0.135 |  0.072 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.314 |  0.361 |  0.062 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 | -0.002 | -0.196 |  0.203 |  0.090 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.317 |  0.857 |  2.040 |  0.232 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 | -0.001 | -0.438 |  0.448 |  0.225 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 | -0.002 | -0.788 |  0.705 |  0.114 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.081 | -0.031 |  0.247 |  0.052 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 |  0.001 | -0.687 |  0.942 |  0.122 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 | -0.001 | -0.191 |  0.167 |  0.075 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.004 |  0.718 |  1.259 |  0.107 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 | -0.004 | -0.326 |  0.359 |  0.159 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.137 |  0.234 |  0.034 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.374 |  0.357 |  0.059 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 |  0.004 | -0.143 |  0.140 |  0.068 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.416 |  0.344 |  0.062 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 | -0.003 | -0.200 |  0.210 |  0.100 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.285 |  0.825 |  1.879 |  0.221 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 | -0.001 | -0.485 |  0.432 |  0.227 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.002 | -0.747 |  0.619 |  0.110 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.078 | -0.031 |  0.212 |  0.048 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.690 |  0.864 |  0.121 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 | -0.001 | -0.162 |  0.170 |  0.071 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  0.999 |  0.692 |  1.278 |  0.111 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 |  0.013 | -0.489 |  0.511 |  0.194 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.136 |  0.543 |  0.045 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.365 |  0.358 |  0.057 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 | -0.002 | -0.153 |  0.146 |  0.069 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.247 |  0.289 |  0.060 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.201 |  0.206 |  0.101 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.256 |  0.769 |  1.822 |  0.214 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.430 |  0.453 |  0.225 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 | -0.002 | -0.618 |  0.667 |  0.107 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.083 | -0.024 |  0.202 |  0.043 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 |  0.001 | -0.653 |  0.794 |  0.120 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 | -0.001 | -0.158 |  0.178 |  0.072 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  0.988 |  0.564 |  1.315 |  0.119 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 |  0.006 | -0.416 |  0.382 |  0.179 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.214 |  0.220 |  0.035 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.369 |  0.364 |  0.054 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 | -0.003 | -0.159 |  0.131 |  0.063 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.257 |  0.273 |  0.054 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 | -0.001 | -0.196 |  0.200 |  0.096 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.225 |  0.793 |  1.827 |  0.193 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 | -0.007 | -0.460 |  0.386 |  0.225 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 | -0.003 | -0.600 |  0.665 |  0.106 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.081 | -0.022 |  0.186 |  0.046 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 |  0.001 | -0.770 |  0.726 |  0.121 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.179 |  0.178 |  0.072 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.001 |  0.754 |  1.270 |  0.104 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 | -0.001 | -0.473 |  0.661 |  0.198 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 | -0.003 | -0.163 |  0.504 |  0.048 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.331 |  0.356 |  0.059 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 | -0.002 | -0.143 |  0.142 |  0.067 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.270 |  0.256 |  0.060 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 | -0.001 | -0.219 |  0.221 |  0.103 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.238 |  0.766 |  1.726 |  0.192 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 | -0.003 | -0.419 |  0.377 |  0.216 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 | -0.002 | -0.553 |  0.591 |  0.103 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.073 | -0.020 |  0.174 |  0.042 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.001 | -0.654 |  0.694 |  0.119 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.193 |  0.210 |  0.077 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.847 |  0.800 |  0.088 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 |  0.004 | -0.326 |  0.625 |  0.063 | torch.Size([180]) || layers.5.conv.bias
 |  0.222 |  0.016 |  0.927 |  0.147 | torch.Size([180]) || norm.weight
 |  0.021 | -0.800 |  0.347 |  0.075 | torch.Size([180]) || norm.bias
 | -0.000 | -0.482 |  0.491 |  0.075 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.002 | -0.139 |  0.086 |  0.036 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.428 |  0.402 |  0.061 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 | -0.062 | -0.153 |  0.074 |  0.056 | torch.Size([64]) || conv_before_upsample.0.bias
 |  0.000 | -0.693 |  0.662 |  0.062 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 |  0.000 | -0.080 |  0.068 |  0.017 | torch.Size([256]) || upsample.0.bias
 |  0.000 | -0.267 |  0.273 |  0.031 | torch.Size([256, 64, 3, 3]) || upsample.2.weight
 | -0.000 | -0.043 |  0.034 |  0.008 | torch.Size([256]) || upsample.2.bias
 | -0.000 | -0.316 |  0.357 |  0.025 | torch.Size([256, 64, 3, 3]) || upsample.4.weight
 | -0.001 | -0.084 |  0.045 |  0.011 | torch.Size([256]) || upsample.4.bias
 | -0.000 | -0.053 |  0.045 |  0.006 | torch.Size([3, 64, 3, 3]) || conv_last.weight
 |  0.051 |  0.041 |  0.069 |  0.016 | torch.Size([3]) || conv_last.bias

22-11-09 22:56:00.675 :   task: swinir_sr_classical_patch48_x8
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 8
  n_channels: 3
  path:[
    root: superresolution
    pretrained_netG: superresolution/swinir_sr_classical_patch48_x8/models/230000_G.pth
    pretrained_netE: superresolution/swinir_sr_classical_patch48_x8/models/230000_E.pth
    task: superresolution/swinir_sr_classical_patch48_x8
    log: superresolution/swinir_sr_classical_patch48_x8
    options: superresolution/swinir_sr_classical_patch48_x8/options
    models: superresolution/swinir_sr_classical_patch48_x8/models
    images: superresolution/swinir_sr_classical_patch48_x8/images
    pretrained_optimizerG: superresolution/swinir_sr_classical_patch48_x8/models/230000_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/WIN/train-HR
      dataroot_L: trainsets/WIN/train-LRx8
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 16
      dataloader_batch_size: 24
      phase: train
      scale: 8
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/WIN/test-HR-100
      dataroot_L: testsets/WIN/test-LRx8-100
      phase: test
      scale: 8
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 8
    in_chans: 3
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 8
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 1
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/swinir/train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  use_static_graph: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-11-09 22:56:01.485 : Number of train images: 7,142, iters: 298
22-11-09 22:56:41.903 :   task: swinir_sr_classical_patch48_x8
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 8
  n_channels: 3
  path:[
    root: superresolution
    pretrained_netG: superresolution/swinir_sr_classical_patch48_x8/models/230000_G.pth
    pretrained_netE: superresolution/swinir_sr_classical_patch48_x8/models/230000_E.pth
    task: superresolution/swinir_sr_classical_patch48_x8
    log: superresolution/swinir_sr_classical_patch48_x8
    options: superresolution/swinir_sr_classical_patch48_x8/options
    models: superresolution/swinir_sr_classical_patch48_x8/models
    images: superresolution/swinir_sr_classical_patch48_x8/images
    pretrained_optimizerG: superresolution/swinir_sr_classical_patch48_x8/models/230000_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/WIN/train-HR
      dataroot_L: trainsets/WIN/train-LRx8
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 16
      dataloader_batch_size: 24
      phase: train
      scale: 8
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/WIN/test-HR-100
      dataroot_L: testsets/WIN/test-LRx8-100
      phase: test
      scale: 8
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 8
    in_chans: 3
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 8
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 1
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/swinir/train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  use_static_graph: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-11-09 22:56:42.103 : Number of train images: 7,142, iters: 298
22-11-09 22:58:39.863 :   task: swinir_sr_classical_patch48_x8
  model: plain
  gpu_ids: [0]
  dist: False
  scale: 8
  n_channels: 3
  path:[
    root: superresolution
    pretrained_netG: superresolution/swinir_sr_classical_patch48_x8/models/230000_G.pth
    pretrained_netE: superresolution/swinir_sr_classical_patch48_x8/models/230000_E.pth
    task: superresolution/swinir_sr_classical_patch48_x8
    log: superresolution/swinir_sr_classical_patch48_x8
    options: superresolution/swinir_sr_classical_patch48_x8/options
    models: superresolution/swinir_sr_classical_patch48_x8/models
    images: superresolution/swinir_sr_classical_patch48_x8/images
    pretrained_optimizerG: superresolution/swinir_sr_classical_patch48_x8/models/230000_optimizerG.pth
  ]
  datasets:[
    train:[
      name: train_dataset
      dataset_type: sr
      dataroot_H: trainsets/WIN/train-HR
      dataroot_L: trainsets/WIN/train-LRx8
      H_size: 96
      dataloader_shuffle: True
      dataloader_num_workers: 16
      dataloader_batch_size: 24
      phase: train
      scale: 8
      n_channels: 3
    ]
    test:[
      name: test_dataset
      dataset_type: sr
      dataroot_H: testsets/WIN/test-HR-100
      dataroot_L: testsets/WIN/test-LRx8-100
      phase: test
      scale: 8
      n_channels: 3
    ]
  ]
  netG:[
    net_type: swinir
    upscale: 8
    in_chans: 3
    img_size: 48
    window_size: 8
    img_range: 1.0
    depths: [6, 6, 6, 6, 6, 6]
    embed_dim: 180
    num_heads: [6, 6, 6, 6, 6, 6]
    mlp_ratio: 2
    upsampler: pixelshuffle
    resi_connection: 1conv
    init_type: default
    scale: 8
  ]
  train:[
    G_lossfn_type: l1
    G_lossfn_weight: 1.0
    E_decay: 0.999
    G_optimizer_type: adam
    G_optimizer_lr: 0.0002
    G_optimizer_wd: 0
    G_optimizer_clipgrad: None
    G_optimizer_reuse: True
    G_scheduler_type: MultiStepLR
    G_scheduler_milestones: [250000, 400000, 450000, 475000, 500000]
    G_scheduler_gamma: 0.5
    G_regularizer_orthstep: None
    G_regularizer_clipstep: None
    G_param_strict: True
    E_param_strict: True
    checkpoint_test: 1
    checkpoint_save: 5000
    checkpoint_print: 200
    F_feature_layer: 34
    F_weights: 1.0
    F_lossfn_type: l1
    F_use_input_norm: True
    F_use_range_norm: False
    G_optimizer_betas: [0.9, 0.999]
    G_scheduler_restart_weights: 1
  ]
  opt_path: options/swinir/train_swinir_sr_classical.json
  is_train: True
  merge_bn: False
  merge_bn_startpoint: -1
  find_unused_parameters: True
  use_static_graph: False
  num_gpu: 1
  rank: 0
  world_size: 1

22-11-09 22:58:39.974 : Number of train images: 7,142, iters: 298
22-11-09 23:03:02.944 : 
Networks name: SwinIR
Params number: 12047911
Net structure:
SwinIR(
  (conv_first): Conv2d(3, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (patch_embed): PatchEmbed(
    (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  )
  (patch_unembed): PatchUnEmbed()
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): Identity()
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.003)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.006)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.009)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.011)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.014)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (1): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.017)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.020)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.023)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.026)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.029)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.031)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (2): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.034)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.037)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.040)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.043)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.046)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.049)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (3): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.051)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.054)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.057)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.060)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.063)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.066)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (4): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.069)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.071)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.074)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.077)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.080)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.083)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
    (5): RSTB(
      (residual_group): BasicLayer(
        dim=180, input_resolution=(48, 48), depth=6
        (blocks): ModuleList(
          (0): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.086)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (1): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.089)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (2): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.091)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (3): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.094)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (4): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=0, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.097)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
          (5): SwinTransformerBlock(
            dim=180, input_resolution=(48, 48), num_heads=6, window_size=8, shift_size=4, mlp_ratio=2
            (norm1): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (attn): WindowAttention(
              dim=180, window_size=(8, 8), num_heads=6
              (qkv): Linear(in_features=180, out_features=540, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=180, out_features=180, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
              (softmax): Softmax(dim=-1)
            )
            (drop_path): DropPath(drop_prob=0.100)
            (norm2): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
            (mlp): Mlp(
              (fc1): Linear(in_features=180, out_features=360, bias=True)
              (act): GELU(approximate=none)
              (fc2): Linear(in_features=360, out_features=180, bias=True)
              (drop): Dropout(p=0.0, inplace=False)
            )
          )
        )
      )
      (conv): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (patch_embed): PatchEmbed()
      (patch_unembed): PatchUnEmbed()
    )
  )
  (norm): LayerNorm((180,), eps=1e-05, elementwise_affine=True)
  (conv_after_body): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (conv_before_upsample): Sequential(
    (0): Conv2d(180, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): LeakyReLU(negative_slope=0.01, inplace=True)
  )
  (upsample): Upsample(
    (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): PixelShuffle(upscale_factor=2)
    (2): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): PixelShuffle(upscale_factor=2)
    (4): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (5): PixelShuffle(upscale_factor=2)
  )
  (conv_last): Conv2d(64, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
)

22-11-09 23:03:03.152 : 
 |  mean  |  min   |  max   |  std   || shape               
 |  0.001 | -0.455 |  0.438 |  0.144 | torch.Size([180, 3, 3, 3]) || conv_first.weight
 |  0.003 | -0.442 |  0.737 |  0.158 | torch.Size([180]) || conv_first.bias
 |  1.091 |  0.725 |  1.696 |  0.158 | torch.Size([180]) || patch_embed.norm.weight
 | -0.002 | -0.554 |  0.591 |  0.091 | torch.Size([180]) || patch_embed.norm.bias
 |  0.453 |  0.066 |  0.807 |  0.139 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.776 |  0.698 |  0.294 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm1.bias
 | -0.019 | -0.579 |  0.672 |  0.131 | torch.Size([225, 6]) || layers.0.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.437 |  0.386 |  0.044 | torch.Size([540, 180]) || layers.0.residual_group.blocks.0.attn.qkv.weight
 | -0.004 | -0.456 |  0.382 |  0.090 | torch.Size([540]) || layers.0.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.352 |  0.439 |  0.031 | torch.Size([180, 180]) || layers.0.residual_group.blocks.0.attn.proj.weight
 |  0.001 | -0.509 |  0.427 |  0.073 | torch.Size([180]) || layers.0.residual_group.blocks.0.attn.proj.bias
 |  0.728 |  0.267 |  1.361 |  0.162 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.weight
 |  0.007 | -0.434 |  0.481 |  0.218 | torch.Size([180]) || layers.0.residual_group.blocks.0.norm2.bias
 | -0.000 | -0.493 |  0.439 |  0.052 | torch.Size([360, 180]) || layers.0.residual_group.blocks.0.mlp.fc1.weight
 | -0.071 | -0.218 |  0.163 |  0.060 | torch.Size([360]) || layers.0.residual_group.blocks.0.mlp.fc1.bias
 |  0.001 | -0.510 |  0.549 |  0.053 | torch.Size([180, 360]) || layers.0.residual_group.blocks.0.mlp.fc2.weight
 |  0.002 | -0.388 |  0.383 |  0.059 | torch.Size([180]) || layers.0.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.1.attn_mask
 |  0.464 |  0.017 |  0.793 |  0.123 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.weight
 | -0.001 | -0.799 |  0.826 |  0.194 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm1.bias
 | -0.011 | -0.415 |  0.409 |  0.112 | torch.Size([225, 6]) || layers.0.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.389 |  0.434 |  0.062 | torch.Size([540, 180]) || layers.0.residual_group.blocks.1.attn.qkv.weight
 |  0.001 | -0.387 |  0.459 |  0.109 | torch.Size([540]) || layers.0.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.355 |  0.325 |  0.044 | torch.Size([180, 180]) || layers.0.residual_group.blocks.1.attn.proj.weight
 |  0.003 | -0.268 |  0.315 |  0.058 | torch.Size([180]) || layers.0.residual_group.blocks.1.attn.proj.bias
 |  0.633 |  0.132 |  0.937 |  0.148 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.weight
 |  0.001 | -0.502 |  0.396 |  0.118 | torch.Size([180]) || layers.0.residual_group.blocks.1.norm2.bias
 | -0.001 | -0.405 |  0.471 |  0.056 | torch.Size([360, 180]) || layers.0.residual_group.blocks.1.mlp.fc1.weight
 | -0.054 | -0.248 |  0.042 |  0.067 | torch.Size([360]) || layers.0.residual_group.blocks.1.mlp.fc1.bias
 |  0.001 | -0.358 |  0.391 |  0.041 | torch.Size([180, 360]) || layers.0.residual_group.blocks.1.mlp.fc2.weight
 |  0.002 | -0.153 |  0.227 |  0.049 | torch.Size([180]) || layers.0.residual_group.blocks.1.mlp.fc2.bias
 |  0.609 |  0.097 |  0.932 |  0.138 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.weight
 |  0.001 | -0.710 |  0.810 |  0.164 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm1.bias
 | -0.000 | -1.080 |  0.511 |  0.142 | torch.Size([225, 6]) || layers.0.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.618 |  0.606 |  0.074 | torch.Size([540, 180]) || layers.0.residual_group.blocks.2.attn.qkv.weight
 | -0.000 | -0.551 |  0.530 |  0.123 | torch.Size([540]) || layers.0.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.433 |  0.476 |  0.052 | torch.Size([180, 180]) || layers.0.residual_group.blocks.2.attn.proj.weight
 |  0.003 | -0.172 |  0.200 |  0.061 | torch.Size([180]) || layers.0.residual_group.blocks.2.attn.proj.bias
 |  0.704 |  0.416 |  1.176 |  0.123 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.weight
 | -0.002 | -0.396 |  0.357 |  0.095 | torch.Size([180]) || layers.0.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.563 |  0.583 |  0.056 | torch.Size([360, 180]) || layers.0.residual_group.blocks.2.mlp.fc1.weight
 | -0.036 | -0.259 |  0.040 |  0.056 | torch.Size([360]) || layers.0.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.407 |  0.492 |  0.039 | torch.Size([180, 360]) || layers.0.residual_group.blocks.2.mlp.fc2.weight
 |  0.004 | -0.116 |  0.176 |  0.046 | torch.Size([180]) || layers.0.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.3.attn_mask
 |  0.642 |  0.095 |  0.944 |  0.152 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.weight
 |  0.004 | -0.900 |  0.861 |  0.142 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm1.bias
 | -0.017 | -1.098 |  0.579 |  0.145 | torch.Size([225, 6]) || layers.0.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.507 |  0.540 |  0.084 | torch.Size([540, 180]) || layers.0.residual_group.blocks.3.attn.qkv.weight
 | -0.003 | -0.514 |  0.533 |  0.136 | torch.Size([540]) || layers.0.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.331 |  0.360 |  0.057 | torch.Size([180, 180]) || layers.0.residual_group.blocks.3.attn.proj.weight
 |  0.005 | -0.130 |  0.371 |  0.058 | torch.Size([180]) || layers.0.residual_group.blocks.3.attn.proj.bias
 |  0.676 |  0.286 |  0.990 |  0.124 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.weight
 |  0.006 | -0.368 |  0.507 |  0.083 | torch.Size([180]) || layers.0.residual_group.blocks.3.norm2.bias
 | -0.001 | -0.573 |  0.476 |  0.059 | torch.Size([360, 180]) || layers.0.residual_group.blocks.3.mlp.fc1.weight
 | -0.032 | -0.321 |  0.038 |  0.064 | torch.Size([360]) || layers.0.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.450 |  0.431 |  0.039 | torch.Size([180, 360]) || layers.0.residual_group.blocks.3.mlp.fc2.weight
 |  0.003 | -0.108 |  0.424 |  0.050 | torch.Size([180]) || layers.0.residual_group.blocks.3.mlp.fc2.bias
 |  0.740 |  0.138 |  0.998 |  0.116 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.weight
 |  0.016 | -0.857 |  0.893 |  0.116 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm1.bias
 | -0.015 | -1.396 |  0.882 |  0.197 | torch.Size([225, 6]) || layers.0.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.653 |  0.562 |  0.086 | torch.Size([540, 180]) || layers.0.residual_group.blocks.4.attn.qkv.weight
 | -0.004 | -0.478 |  0.477 |  0.100 | torch.Size([540]) || layers.0.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.378 |  0.360 |  0.055 | torch.Size([180, 180]) || layers.0.residual_group.blocks.4.attn.proj.weight
 |  0.006 | -0.144 |  0.650 |  0.066 | torch.Size([180]) || layers.0.residual_group.blocks.4.attn.proj.bias
 |  0.731 |  0.375 |  1.127 |  0.114 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.weight
 |  0.017 | -0.343 |  0.472 |  0.062 | torch.Size([180]) || layers.0.residual_group.blocks.4.norm2.bias
 | -0.003 | -0.554 |  0.456 |  0.056 | torch.Size([360, 180]) || layers.0.residual_group.blocks.4.mlp.fc1.weight
 | -0.011 | -0.279 |  0.054 |  0.038 | torch.Size([360]) || layers.0.residual_group.blocks.4.mlp.fc1.bias
 | -0.000 | -0.616 |  0.436 |  0.040 | torch.Size([180, 360]) || layers.0.residual_group.blocks.4.mlp.fc2.weight
 |  0.007 | -0.154 |  0.627 |  0.057 | torch.Size([180]) || layers.0.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.0.residual_group.blocks.5.attn_mask
 |  0.708 |  0.077 |  0.929 |  0.125 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.weight
 |  0.027 | -0.922 |  0.745 |  0.101 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm1.bias
 | -0.026 | -0.725 |  0.843 |  0.167 | torch.Size([225, 6]) || layers.0.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.0.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.504 |  0.453 |  0.088 | torch.Size([540, 180]) || layers.0.residual_group.blocks.5.attn.qkv.weight
 | -0.002 | -0.383 |  0.372 |  0.086 | torch.Size([540]) || layers.0.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.516 |  0.477 |  0.055 | torch.Size([180, 180]) || layers.0.residual_group.blocks.5.attn.proj.weight
 |  0.007 | -0.380 |  0.554 |  0.062 | torch.Size([180]) || layers.0.residual_group.blocks.5.attn.proj.bias
 |  0.786 |  0.531 |  1.094 |  0.105 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.weight
 |  0.027 | -0.272 |  0.321 |  0.047 | torch.Size([180]) || layers.0.residual_group.blocks.5.norm2.bias
 | -0.001 | -0.403 |  0.430 |  0.060 | torch.Size([360, 180]) || layers.0.residual_group.blocks.5.mlp.fc1.weight
 | -0.019 | -0.261 |  0.045 |  0.034 | torch.Size([360]) || layers.0.residual_group.blocks.5.mlp.fc1.bias
 | -0.001 | -0.673 |  0.478 |  0.044 | torch.Size([180, 360]) || layers.0.residual_group.blocks.5.mlp.fc2.weight
 |  0.007 | -0.356 |  0.429 |  0.051 | torch.Size([180]) || layers.0.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.749 |  0.643 |  0.063 | torch.Size([180, 180, 3, 3]) || layers.0.conv.weight
 |  0.002 | -0.344 |  0.673 |  0.083 | torch.Size([180]) || layers.0.conv.bias
 |  0.669 |  0.289 |  0.978 |  0.137 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.weight
 |  0.005 | -0.874 |  0.664 |  0.189 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm1.bias
 | -0.009 | -1.646 |  1.562 |  0.220 | torch.Size([225, 6]) || layers.1.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.543 |  0.445 |  0.071 | torch.Size([540, 180]) || layers.1.residual_group.blocks.0.attn.qkv.weight
 | -0.001 | -0.244 |  0.240 |  0.062 | torch.Size([540]) || layers.1.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.363 |  0.293 |  0.065 | torch.Size([180, 180]) || layers.1.residual_group.blocks.0.attn.proj.weight
 |  0.002 | -0.312 |  0.362 |  0.076 | torch.Size([180]) || layers.1.residual_group.blocks.0.attn.proj.bias
 |  0.948 |  0.442 |  1.264 |  0.145 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.weight
 |  0.004 | -0.259 |  0.323 |  0.088 | torch.Size([180]) || layers.1.residual_group.blocks.0.norm2.bias
 | -0.001 | -0.543 |  0.441 |  0.085 | torch.Size([360, 180]) || layers.1.residual_group.blocks.0.mlp.fc1.weight
 | -0.062 | -0.254 |  0.069 |  0.048 | torch.Size([360]) || layers.1.residual_group.blocks.0.mlp.fc1.bias
 |  0.000 | -0.417 |  0.479 |  0.076 | torch.Size([180, 360]) || layers.1.residual_group.blocks.0.mlp.fc2.weight
 | -0.000 | -0.387 |  0.214 |  0.070 | torch.Size([180]) || layers.1.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.1.attn_mask
 |  0.774 |  0.528 |  1.012 |  0.099 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.weight
 |  0.007 | -0.811 |  0.740 |  0.194 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm1.bias
 | -0.042 | -2.366 |  1.638 |  0.305 | torch.Size([225, 6]) || layers.1.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.563 |  0.688 |  0.079 | torch.Size([540, 180]) || layers.1.residual_group.blocks.1.attn.qkv.weight
 | -0.001 | -0.230 |  0.242 |  0.051 | torch.Size([540]) || layers.1.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.455 |  0.462 |  0.072 | torch.Size([180, 180]) || layers.1.residual_group.blocks.1.attn.proj.weight
 |  0.001 | -0.433 |  0.318 |  0.082 | torch.Size([180]) || layers.1.residual_group.blocks.1.attn.proj.bias
 |  0.935 |  0.545 |  1.216 |  0.124 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.weight
 |  0.006 | -0.393 |  0.218 |  0.083 | torch.Size([180]) || layers.1.residual_group.blocks.1.norm2.bias
 | -0.001 | -0.469 |  0.456 |  0.091 | torch.Size([360, 180]) || layers.1.residual_group.blocks.1.mlp.fc1.weight
 | -0.059 | -0.200 |  0.033 |  0.030 | torch.Size([360]) || layers.1.residual_group.blocks.1.mlp.fc1.bias
 |  0.000 | -0.461 |  0.433 |  0.081 | torch.Size([180, 360]) || layers.1.residual_group.blocks.1.mlp.fc2.weight
 |  0.000 | -0.444 |  0.278 |  0.071 | torch.Size([180]) || layers.1.residual_group.blocks.1.mlp.fc2.bias
 |  0.815 |  0.394 |  1.060 |  0.123 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.weight
 |  0.006 | -0.936 |  0.728 |  0.159 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm1.bias
 | -0.012 | -1.888 |  1.205 |  0.216 | torch.Size([225, 6]) || layers.1.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.624 |  0.528 |  0.081 | torch.Size([540, 180]) || layers.1.residual_group.blocks.2.attn.qkv.weight
 |  0.004 | -0.256 |  0.269 |  0.054 | torch.Size([540]) || layers.1.residual_group.blocks.2.attn.qkv.bias
 | -0.001 | -0.395 |  0.340 |  0.079 | torch.Size([180, 180]) || layers.1.residual_group.blocks.2.attn.proj.weight
 | -0.000 | -0.311 |  0.317 |  0.072 | torch.Size([180]) || layers.1.residual_group.blocks.2.attn.proj.bias
 |  0.958 |  0.618 |  1.264 |  0.113 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.weight
 |  0.005 | -0.400 |  0.288 |  0.082 | torch.Size([180]) || layers.1.residual_group.blocks.2.norm2.bias
 | -0.000 | -0.519 |  0.416 |  0.094 | torch.Size([360, 180]) || layers.1.residual_group.blocks.2.mlp.fc1.weight
 | -0.066 | -0.249 |  0.007 |  0.034 | torch.Size([360]) || layers.1.residual_group.blocks.2.mlp.fc1.bias
 | -0.000 | -0.409 |  0.470 |  0.081 | torch.Size([180, 360]) || layers.1.residual_group.blocks.2.mlp.fc2.weight
 |  0.001 | -0.315 |  0.264 |  0.062 | torch.Size([180]) || layers.1.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.3.attn_mask
 |  0.792 |  0.165 |  1.021 |  0.113 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.weight
 |  0.004 | -0.976 |  0.830 |  0.140 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm1.bias
 |  0.002 | -0.768 |  0.849 |  0.173 | torch.Size([225, 6]) || layers.1.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.492 |  0.455 |  0.085 | torch.Size([540, 180]) || layers.1.residual_group.blocks.3.attn.qkv.weight
 |  0.002 | -0.285 |  0.249 |  0.056 | torch.Size([540]) || layers.1.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.358 |  0.374 |  0.077 | torch.Size([180, 180]) || layers.1.residual_group.blocks.3.attn.proj.weight
 | -0.002 | -0.433 |  0.283 |  0.066 | torch.Size([180]) || layers.1.residual_group.blocks.3.attn.proj.bias
 |  0.989 |  0.617 |  1.270 |  0.096 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.weight
 |  0.008 | -0.402 |  0.212 |  0.073 | torch.Size([180]) || layers.1.residual_group.blocks.3.norm2.bias
 | -0.001 | -0.523 |  0.428 |  0.096 | torch.Size([360, 180]) || layers.1.residual_group.blocks.3.mlp.fc1.weight
 | -0.068 | -0.267 |  0.051 |  0.037 | torch.Size([360]) || layers.1.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.457 |  0.446 |  0.083 | torch.Size([180, 360]) || layers.1.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.437 |  0.250 |  0.061 | torch.Size([180]) || layers.1.residual_group.blocks.3.mlp.fc2.bias
 |  0.903 |  0.299 |  1.091 |  0.106 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.weight
 |  0.005 | -0.981 |  0.848 |  0.129 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm1.bias
 | -0.066 | -2.334 |  1.494 |  0.300 | torch.Size([225, 6]) || layers.1.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.442 |  0.477 |  0.091 | torch.Size([540, 180]) || layers.1.residual_group.blocks.4.attn.qkv.weight
 |  0.002 | -0.198 |  0.258 |  0.044 | torch.Size([540]) || layers.1.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.377 |  0.335 |  0.078 | torch.Size([180, 180]) || layers.1.residual_group.blocks.4.attn.proj.weight
 | -0.000 | -0.744 |  0.220 |  0.073 | torch.Size([180]) || layers.1.residual_group.blocks.4.attn.proj.bias
 |  0.998 |  0.547 |  1.189 |  0.093 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.weight
 |  0.008 | -0.498 |  0.228 |  0.072 | torch.Size([180]) || layers.1.residual_group.blocks.4.norm2.bias
 | -0.001 | -0.425 |  0.409 |  0.096 | torch.Size([360, 180]) || layers.1.residual_group.blocks.4.mlp.fc1.weight
 | -0.068 | -0.284 |  0.008 |  0.033 | torch.Size([360]) || layers.1.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.423 |  0.548 |  0.087 | torch.Size([180, 360]) || layers.1.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.728 |  0.188 |  0.070 | torch.Size([180]) || layers.1.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.1.residual_group.blocks.5.attn_mask
 |  0.910 |  0.333 |  1.077 |  0.095 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.weight
 |  0.002 | -0.988 |  0.933 |  0.131 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm1.bias
 | -0.082 | -1.927 |  1.840 |  0.333 | torch.Size([225, 6]) || layers.1.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.1.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.600 |  0.480 |  0.091 | torch.Size([540, 180]) || layers.1.residual_group.blocks.5.attn.qkv.weight
 |  0.001 | -0.178 |  0.288 |  0.040 | torch.Size([540]) || layers.1.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.682 |  0.824 |  0.078 | torch.Size([180, 180]) || layers.1.residual_group.blocks.5.attn.proj.weight
 | -0.005 | -1.174 |  0.323 |  0.096 | torch.Size([180]) || layers.1.residual_group.blocks.5.attn.proj.bias
 |  0.921 |  0.566 |  1.145 |  0.072 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.weight
 |  0.006 | -0.384 |  0.218 |  0.069 | torch.Size([180]) || layers.1.residual_group.blocks.5.norm2.bias
 | -0.001 | -0.584 |  0.609 |  0.091 | torch.Size([360, 180]) || layers.1.residual_group.blocks.5.mlp.fc1.weight
 | -0.068 | -0.151 |  0.431 |  0.044 | torch.Size([360]) || layers.1.residual_group.blocks.5.mlp.fc1.bias
 |  0.001 | -1.418 |  1.311 |  0.093 | torch.Size([180, 360]) || layers.1.residual_group.blocks.5.mlp.fc2.weight
 | -0.006 | -1.563 |  0.665 |  0.132 | torch.Size([180]) || layers.1.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.484 |  0.394 |  0.060 | torch.Size([180, 180, 3, 3]) || layers.1.conv.weight
 |  0.001 | -0.415 |  0.496 |  0.097 | torch.Size([180]) || layers.1.conv.bias
 |  0.782 |  0.411 |  1.040 |  0.106 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.weight
 |  0.011 | -0.564 |  0.496 |  0.116 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm1.bias
 | -0.027 | -1.871 |  1.506 |  0.239 | torch.Size([225, 6]) || layers.2.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.0.attn.relative_position_index
 | -0.000 | -0.426 |  0.380 |  0.078 | torch.Size([540, 180]) || layers.2.residual_group.blocks.0.attn.qkv.weight
 |  0.003 | -0.222 |  0.267 |  0.067 | torch.Size([540]) || layers.2.residual_group.blocks.0.attn.qkv.bias
 | -0.001 | -0.396 |  0.379 |  0.085 | torch.Size([180, 180]) || layers.2.residual_group.blocks.0.attn.proj.weight
 | -0.003 | -0.281 |  0.193 |  0.056 | torch.Size([180]) || layers.2.residual_group.blocks.0.attn.proj.bias
 |  1.082 |  0.791 |  1.388 |  0.109 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.weight
 |  0.009 | -0.220 |  0.300 |  0.093 | torch.Size([180]) || layers.2.residual_group.blocks.0.norm2.bias
 |  0.000 | -0.434 |  0.608 |  0.100 | torch.Size([360, 180]) || layers.2.residual_group.blocks.0.mlp.fc1.weight
 | -0.071 | -0.165 |  0.039 |  0.037 | torch.Size([360]) || layers.2.residual_group.blocks.0.mlp.fc1.bias
 |  0.001 | -0.441 |  0.476 |  0.101 | torch.Size([180, 360]) || layers.2.residual_group.blocks.0.mlp.fc2.weight
 | -0.004 | -0.311 |  0.203 |  0.057 | torch.Size([180]) || layers.2.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.1.attn_mask
 |  0.787 |  0.323 |  0.984 |  0.097 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.weight
 |  0.005 | -0.805 |  0.506 |  0.135 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm1.bias
 | -0.020 | -0.956 |  1.584 |  0.238 | torch.Size([225, 6]) || layers.2.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.434 |  0.339 |  0.082 | torch.Size([540, 180]) || layers.2.residual_group.blocks.1.attn.qkv.weight
 | -0.002 | -0.262 |  0.239 |  0.056 | torch.Size([540]) || layers.2.residual_group.blocks.1.attn.qkv.bias
 | -0.000 | -0.385 |  0.411 |  0.087 | torch.Size([180, 180]) || layers.2.residual_group.blocks.1.attn.proj.weight
 | -0.004 | -0.383 |  0.208 |  0.062 | torch.Size([180]) || layers.2.residual_group.blocks.1.attn.proj.bias
 |  1.088 |  0.692 |  1.307 |  0.104 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.weight
 |  0.007 | -0.237 |  0.283 |  0.104 | torch.Size([180]) || layers.2.residual_group.blocks.1.norm2.bias
 |  0.001 | -0.432 |  0.459 |  0.100 | torch.Size([360, 180]) || layers.2.residual_group.blocks.1.mlp.fc1.weight
 | -0.072 | -0.165 |  0.026 |  0.030 | torch.Size([360]) || layers.2.residual_group.blocks.1.mlp.fc1.bias
 |  0.001 | -0.445 |  0.530 |  0.102 | torch.Size([180, 360]) || layers.2.residual_group.blocks.1.mlp.fc2.weight
 | -0.006 | -0.378 |  0.158 |  0.067 | torch.Size([180]) || layers.2.residual_group.blocks.1.mlp.fc2.bias
 |  0.829 |  0.151 |  1.009 |  0.114 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.weight
 |  0.005 | -0.957 |  0.600 |  0.145 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm1.bias
 | -0.005 | -1.192 |  1.064 |  0.219 | torch.Size([225, 6]) || layers.2.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.365 |  0.395 |  0.086 | torch.Size([540, 180]) || layers.2.residual_group.blocks.2.attn.qkv.weight
 |  0.001 | -0.317 |  0.245 |  0.053 | torch.Size([540]) || layers.2.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.358 |  0.383 |  0.090 | torch.Size([180, 180]) || layers.2.residual_group.blocks.2.attn.proj.weight
 | -0.005 | -0.408 |  0.173 |  0.061 | torch.Size([180]) || layers.2.residual_group.blocks.2.attn.proj.bias
 |  1.113 |  0.714 |  1.336 |  0.095 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.weight
 |  0.010 | -0.280 |  0.229 |  0.092 | torch.Size([180]) || layers.2.residual_group.blocks.2.norm2.bias
 |  0.000 | -0.436 |  0.464 |  0.102 | torch.Size([360, 180]) || layers.2.residual_group.blocks.2.mlp.fc1.weight
 | -0.067 | -0.142 |  0.018 |  0.029 | torch.Size([360]) || layers.2.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.554 |  0.528 |  0.105 | torch.Size([180, 360]) || layers.2.residual_group.blocks.2.mlp.fc2.weight
 | -0.005 | -0.374 |  0.175 |  0.068 | torch.Size([180]) || layers.2.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.3.attn_mask
 |  0.797 |  0.196 |  1.014 |  0.114 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.weight
 |  0.006 | -0.834 |  0.624 |  0.142 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm1.bias
 |  0.020 | -0.708 |  0.893 |  0.166 | torch.Size([225, 6]) || layers.2.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.432 |  0.414 |  0.084 | torch.Size([540, 180]) || layers.2.residual_group.blocks.3.attn.qkv.weight
 |  0.001 | -0.190 |  0.198 |  0.042 | torch.Size([540]) || layers.2.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.431 |  0.379 |  0.086 | torch.Size([180, 180]) || layers.2.residual_group.blocks.3.attn.proj.weight
 | -0.006 | -0.391 |  0.125 |  0.070 | torch.Size([180]) || layers.2.residual_group.blocks.3.attn.proj.bias
 |  1.105 |  0.638 |  1.330 |  0.102 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.weight
 |  0.010 | -0.410 |  0.214 |  0.098 | torch.Size([180]) || layers.2.residual_group.blocks.3.norm2.bias
 | -0.000 | -0.518 |  0.476 |  0.101 | torch.Size([360, 180]) || layers.2.residual_group.blocks.3.mlp.fc1.weight
 | -0.072 | -0.151 |  0.011 |  0.028 | torch.Size([360]) || layers.2.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.477 |  0.429 |  0.105 | torch.Size([180, 360]) || layers.2.residual_group.blocks.3.mlp.fc2.weight
 | -0.007 | -0.403 |  0.155 |  0.082 | torch.Size([180]) || layers.2.residual_group.blocks.3.mlp.fc2.bias
 |  0.894 |  0.312 |  1.112 |  0.105 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.weight
 |  0.004 | -0.837 |  0.732 |  0.137 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm1.bias
 |  0.009 | -2.451 |  1.781 |  0.306 | torch.Size([225, 6]) || layers.2.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.452 |  0.432 |  0.088 | torch.Size([540, 180]) || layers.2.residual_group.blocks.4.attn.qkv.weight
 | -0.002 | -0.175 |  0.269 |  0.038 | torch.Size([540]) || layers.2.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.419 |  0.452 |  0.092 | torch.Size([180, 180]) || layers.2.residual_group.blocks.4.attn.proj.weight
 | -0.009 | -0.558 |  0.125 |  0.083 | torch.Size([180]) || layers.2.residual_group.blocks.4.attn.proj.bias
 |  1.132 |  0.592 |  1.337 |  0.109 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.weight
 |  0.008 | -0.414 |  0.248 |  0.094 | torch.Size([180]) || layers.2.residual_group.blocks.4.norm2.bias
 | -0.001 | -0.479 |  0.434 |  0.102 | torch.Size([360, 180]) || layers.2.residual_group.blocks.4.mlp.fc1.weight
 | -0.076 | -0.169 |  0.008 |  0.032 | torch.Size([360]) || layers.2.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.465 |  0.577 |  0.107 | torch.Size([180, 360]) || layers.2.residual_group.blocks.4.mlp.fc2.weight
 | -0.009 | -0.700 |  0.160 |  0.092 | torch.Size([180]) || layers.2.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.2.residual_group.blocks.5.attn_mask
 |  0.880 |  0.389 |  1.040 |  0.088 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.weight
 |  0.001 | -0.735 |  0.672 |  0.124 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm1.bias
 | -0.012 | -2.597 |  1.610 |  0.282 | torch.Size([225, 6]) || layers.2.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.2.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.457 |  0.350 |  0.090 | torch.Size([540, 180]) || layers.2.residual_group.blocks.5.attn.qkv.weight
 |  0.001 | -0.213 |  0.160 |  0.038 | torch.Size([540]) || layers.2.residual_group.blocks.5.attn.qkv.bias
 | -0.000 | -0.691 |  0.835 |  0.091 | torch.Size([180, 180]) || layers.2.residual_group.blocks.5.attn.proj.weight
 | -0.010 | -0.927 |  0.147 |  0.094 | torch.Size([180]) || layers.2.residual_group.blocks.5.attn.proj.bias
 |  1.126 |  0.750 |  1.365 |  0.082 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.weight
 |  0.008 | -0.287 |  0.192 |  0.082 | torch.Size([180]) || layers.2.residual_group.blocks.5.norm2.bias
 | -0.002 | -0.483 |  0.469 |  0.102 | torch.Size([360, 180]) || layers.2.residual_group.blocks.5.mlp.fc1.weight
 | -0.076 | -0.190 |  0.249 |  0.043 | torch.Size([360]) || layers.2.residual_group.blocks.5.mlp.fc1.bias
 | -0.001 | -0.981 |  0.471 |  0.107 | torch.Size([180, 360]) || layers.2.residual_group.blocks.5.mlp.fc2.weight
 | -0.013 | -0.974 |  0.216 |  0.108 | torch.Size([180]) || layers.2.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.380 |  0.356 |  0.060 | torch.Size([180, 180, 3, 3]) || layers.2.conv.weight
 |  0.006 | -0.375 |  0.488 |  0.116 | torch.Size([180]) || layers.2.conv.bias
 |  0.846 |  0.469 |  1.070 |  0.115 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.weight
 |  0.006 | -0.441 |  0.278 |  0.085 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm1.bias
 |  0.018 | -2.354 |  1.467 |  0.238 | torch.Size([225, 6]) || layers.3.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.444 |  0.453 |  0.087 | torch.Size([540, 180]) || layers.3.residual_group.blocks.0.attn.qkv.weight
 | -0.002 | -0.226 |  0.285 |  0.067 | torch.Size([540]) || layers.3.residual_group.blocks.0.attn.qkv.bias
 | -0.000 | -0.429 |  0.409 |  0.101 | torch.Size([180, 180]) || layers.3.residual_group.blocks.0.attn.proj.weight
 |  0.002 | -0.197 |  0.222 |  0.063 | torch.Size([180]) || layers.3.residual_group.blocks.0.attn.proj.bias
 |  1.316 |  0.887 |  1.648 |  0.121 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.weight
 | -0.000 | -0.336 |  0.384 |  0.131 | torch.Size([180]) || layers.3.residual_group.blocks.0.norm2.bias
 |  0.001 | -0.563 |  0.537 |  0.112 | torch.Size([360, 180]) || layers.3.residual_group.blocks.0.mlp.fc1.weight
 | -0.091 | -0.237 |  0.070 |  0.054 | torch.Size([360]) || layers.3.residual_group.blocks.0.mlp.fc1.bias
 | -0.001 | -0.539 |  0.580 |  0.114 | torch.Size([180, 360]) || layers.3.residual_group.blocks.0.mlp.fc2.weight
 | -0.005 | -0.161 |  0.135 |  0.050 | torch.Size([180]) || layers.3.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.1.attn_mask
 |  0.922 |  0.558 |  1.136 |  0.102 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.weight
 |  0.006 | -0.446 |  0.341 |  0.102 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm1.bias
 |  0.026 | -2.396 |  1.351 |  0.302 | torch.Size([225, 6]) || layers.3.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.473 |  0.522 |  0.091 | torch.Size([540, 180]) || layers.3.residual_group.blocks.1.attn.qkv.weight
 |  0.001 | -0.239 |  0.246 |  0.059 | torch.Size([540]) || layers.3.residual_group.blocks.1.attn.qkv.bias
 |  0.001 | -0.467 |  0.450 |  0.105 | torch.Size([180, 180]) || layers.3.residual_group.blocks.1.attn.proj.weight
 |  0.004 | -0.213 |  0.210 |  0.066 | torch.Size([180]) || layers.3.residual_group.blocks.1.attn.proj.bias
 |  1.349 |  1.000 |  1.598 |  0.113 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.weight
 | -0.001 | -0.251 |  0.270 |  0.099 | torch.Size([180]) || layers.3.residual_group.blocks.1.norm2.bias
 |  0.001 | -0.539 |  0.559 |  0.114 | torch.Size([360, 180]) || layers.3.residual_group.blocks.1.mlp.fc1.weight
 | -0.080 | -0.251 |  0.064 |  0.052 | torch.Size([360]) || layers.3.residual_group.blocks.1.mlp.fc1.bias
 | -0.001 | -0.488 |  0.526 |  0.119 | torch.Size([180, 360]) || layers.3.residual_group.blocks.1.mlp.fc2.weight
 | -0.003 | -0.194 |  0.188 |  0.061 | torch.Size([180]) || layers.3.residual_group.blocks.1.mlp.fc2.bias
 |  0.953 |  0.540 |  1.205 |  0.103 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.weight
 |  0.006 | -0.537 |  0.470 |  0.113 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm1.bias
 |  0.015 | -3.050 |  1.869 |  0.319 | torch.Size([225, 6]) || layers.3.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.487 |  0.468 |  0.093 | torch.Size([540, 180]) || layers.3.residual_group.blocks.2.attn.qkv.weight
 |  0.003 | -0.210 |  0.211 |  0.054 | torch.Size([540]) || layers.3.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.590 |  0.561 |  0.110 | torch.Size([180, 180]) || layers.3.residual_group.blocks.2.attn.proj.weight
 |  0.009 | -0.247 |  0.286 |  0.088 | torch.Size([180]) || layers.3.residual_group.blocks.2.attn.proj.bias
 |  1.355 |  0.814 |  1.573 |  0.122 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.weight
 | -0.001 | -0.213 |  0.287 |  0.092 | torch.Size([180]) || layers.3.residual_group.blocks.2.norm2.bias
 |  0.001 | -0.498 |  0.586 |  0.114 | torch.Size([360, 180]) || layers.3.residual_group.blocks.2.mlp.fc1.weight
 | -0.077 | -0.246 |  0.074 |  0.050 | torch.Size([360]) || layers.3.residual_group.blocks.2.mlp.fc1.bias
 | -0.001 | -0.608 |  0.570 |  0.122 | torch.Size([180, 360]) || layers.3.residual_group.blocks.2.mlp.fc2.weight
 | -0.002 | -0.257 |  0.214 |  0.078 | torch.Size([180]) || layers.3.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.3.attn_mask
 |  0.933 |  0.453 |  1.163 |  0.114 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.weight
 |  0.005 | -0.588 |  0.412 |  0.107 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm1.bias
 |  0.014 | -1.816 |  1.799 |  0.306 | torch.Size([225, 6]) || layers.3.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.3.attn.relative_position_index
 | -0.000 | -0.503 |  0.436 |  0.094 | torch.Size([540, 180]) || layers.3.residual_group.blocks.3.attn.qkv.weight
 | -0.004 | -0.271 |  0.227 |  0.054 | torch.Size([540]) || layers.3.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.468 |  0.450 |  0.109 | torch.Size([180, 180]) || layers.3.residual_group.blocks.3.attn.proj.weight
 |  0.003 | -0.183 |  0.211 |  0.079 | torch.Size([180]) || layers.3.residual_group.blocks.3.attn.proj.bias
 |  1.358 |  0.879 |  1.645 |  0.131 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.weight
 |  0.004 | -0.282 |  0.250 |  0.095 | torch.Size([180]) || layers.3.residual_group.blocks.3.norm2.bias
 |  0.001 | -0.472 |  0.522 |  0.114 | torch.Size([360, 180]) || layers.3.residual_group.blocks.3.mlp.fc1.weight
 | -0.079 | -0.269 |  0.061 |  0.046 | torch.Size([360]) || layers.3.residual_group.blocks.3.mlp.fc1.bias
 |  0.000 | -0.601 |  0.620 |  0.126 | torch.Size([180, 360]) || layers.3.residual_group.blocks.3.mlp.fc2.weight
 | -0.002 | -0.251 |  0.275 |  0.093 | torch.Size([180]) || layers.3.residual_group.blocks.3.mlp.fc2.bias
 |  0.934 |  0.477 |  1.125 |  0.100 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.weight
 |  0.008 | -0.583 |  0.476 |  0.106 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm1.bias
 |  0.011 | -2.413 |  1.631 |  0.341 | torch.Size([225, 6]) || layers.3.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.497 |  0.468 |  0.094 | torch.Size([540, 180]) || layers.3.residual_group.blocks.4.attn.qkv.weight
 | -0.000 | -0.233 |  0.177 |  0.051 | torch.Size([540]) || layers.3.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.554 |  0.506 |  0.114 | torch.Size([180, 180]) || layers.3.residual_group.blocks.4.attn.proj.weight
 |  0.007 | -0.354 |  0.243 |  0.097 | torch.Size([180]) || layers.3.residual_group.blocks.4.attn.proj.bias
 |  1.391 |  0.977 |  1.628 |  0.122 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.weight
 |  0.000 | -0.221 |  0.263 |  0.082 | torch.Size([180]) || layers.3.residual_group.blocks.4.norm2.bias
 |  0.002 | -0.542 |  0.573 |  0.116 | torch.Size([360, 180]) || layers.3.residual_group.blocks.4.mlp.fc1.weight
 | -0.077 | -0.241 |  0.075 |  0.043 | torch.Size([360]) || layers.3.residual_group.blocks.4.mlp.fc1.bias
 |  0.000 | -0.817 |  0.879 |  0.134 | torch.Size([180, 360]) || layers.3.residual_group.blocks.4.mlp.fc2.weight
 |  0.000 | -0.257 |  0.261 |  0.099 | torch.Size([180]) || layers.3.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.3.residual_group.blocks.5.attn_mask
 |  0.974 |  0.519 |  1.142 |  0.089 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.weight
 |  0.007 | -0.445 |  0.245 |  0.089 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm1.bias
 | -0.003 | -2.971 |  2.243 |  0.372 | torch.Size([225, 6]) || layers.3.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.3.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.473 |  0.547 |  0.097 | torch.Size([540, 180]) || layers.3.residual_group.blocks.5.attn.qkv.weight
 | -0.003 | -0.210 |  0.187 |  0.054 | torch.Size([540]) || layers.3.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.501 |  0.514 |  0.116 | torch.Size([180, 180]) || layers.3.residual_group.blocks.5.attn.proj.weight
 |  0.012 | -0.279 |  0.349 |  0.109 | torch.Size([180]) || layers.3.residual_group.blocks.5.attn.proj.bias
 |  1.364 |  1.001 |  1.599 |  0.118 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.weight
 |  0.002 | -0.193 |  0.250 |  0.076 | torch.Size([180]) || layers.3.residual_group.blocks.5.norm2.bias
 |  0.002 | -0.466 |  0.518 |  0.116 | torch.Size([360, 180]) || layers.3.residual_group.blocks.5.mlp.fc1.weight
 | -0.072 | -0.265 |  0.089 |  0.044 | torch.Size([360]) || layers.3.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -0.625 |  0.601 |  0.137 | torch.Size([180, 360]) || layers.3.residual_group.blocks.5.mlp.fc2.weight
 |  0.001 | -0.232 |  0.245 |  0.084 | torch.Size([180]) || layers.3.residual_group.blocks.5.mlp.fc2.bias
 |  0.001 | -0.333 |  0.332 |  0.064 | torch.Size([180, 180, 3, 3]) || layers.3.conv.weight
 |  0.022 | -0.407 |  0.485 |  0.152 | torch.Size([180]) || layers.3.conv.bias
 |  1.008 |  0.617 |  1.370 |  0.128 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.weight
 |  0.002 | -0.208 |  0.263 |  0.084 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm1.bias
 | -0.025 | -2.483 |  1.960 |  0.293 | torch.Size([225, 6]) || layers.4.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.676 |  0.653 |  0.102 | torch.Size([540, 180]) || layers.4.residual_group.blocks.0.attn.qkv.weight
 |  0.000 | -0.233 |  0.222 |  0.060 | torch.Size([540]) || layers.4.residual_group.blocks.0.attn.qkv.bias
 | -0.001 | -0.596 |  0.595 |  0.132 | torch.Size([180, 180]) || layers.4.residual_group.blocks.0.attn.proj.weight
 |  0.024 | -0.184 |  0.239 |  0.090 | torch.Size([180]) || layers.4.residual_group.blocks.0.attn.proj.bias
 |  1.688 |  1.163 |  2.083 |  0.163 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.weight
 |  0.000 | -0.310 |  0.247 |  0.097 | torch.Size([180]) || layers.4.residual_group.blocks.0.norm2.bias
 |  0.001 | -0.823 |  0.666 |  0.144 | torch.Size([360, 180]) || layers.4.residual_group.blocks.0.mlp.fc1.weight
 | -0.046 | -0.381 |  0.171 |  0.086 | torch.Size([360]) || layers.4.residual_group.blocks.0.mlp.fc1.bias
 | -0.002 | -0.792 |  0.716 |  0.143 | torch.Size([180, 360]) || layers.4.residual_group.blocks.0.mlp.fc2.weight
 |  0.003 | -0.140 |  0.159 |  0.057 | torch.Size([180]) || layers.4.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.1.attn_mask
 |  1.029 |  0.556 |  1.477 |  0.146 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.weight
 |  0.003 | -0.293 |  0.326 |  0.110 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm1.bias
 | -0.017 | -2.284 |  2.064 |  0.295 | torch.Size([225, 6]) || layers.4.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.1.attn.relative_position_index
 | -0.000 | -0.641 |  0.610 |  0.103 | torch.Size([540, 180]) || layers.4.residual_group.blocks.1.attn.qkv.weight
 |  0.006 | -0.187 |  0.216 |  0.065 | torch.Size([540]) || layers.4.residual_group.blocks.1.attn.qkv.bias
 | -0.001 | -0.598 |  0.628 |  0.138 | torch.Size([180, 180]) || layers.4.residual_group.blocks.1.attn.proj.weight
 |  0.019 | -0.248 |  0.280 |  0.111 | torch.Size([180]) || layers.4.residual_group.blocks.1.attn.proj.bias
 |  1.711 |  1.136 |  2.149 |  0.174 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.weight
 |  0.002 | -0.222 |  0.250 |  0.077 | torch.Size([180]) || layers.4.residual_group.blocks.1.norm2.bias
 |  0.002 | -0.738 |  0.587 |  0.144 | torch.Size([360, 180]) || layers.4.residual_group.blocks.1.mlp.fc1.weight
 | -0.044 | -0.322 |  0.160 |  0.083 | torch.Size([360]) || layers.4.residual_group.blocks.1.mlp.fc1.bias
 | -0.001 | -0.789 |  0.821 |  0.156 | torch.Size([180, 360]) || layers.4.residual_group.blocks.1.mlp.fc2.weight
 |  0.001 | -0.175 |  0.186 |  0.072 | torch.Size([180]) || layers.4.residual_group.blocks.1.mlp.fc2.bias
 |  1.011 |  0.484 |  1.279 |  0.126 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.weight
 |  0.005 | -0.341 |  0.345 |  0.122 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm1.bias
 | -0.017 | -1.837 |  1.571 |  0.277 | torch.Size([225, 6]) || layers.4.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.2.attn.relative_position_index
 |  0.000 | -0.715 |  0.614 |  0.103 | torch.Size([540, 180]) || layers.4.residual_group.blocks.2.attn.qkv.weight
 | -0.002 | -0.246 |  0.217 |  0.064 | torch.Size([540]) || layers.4.residual_group.blocks.2.attn.qkv.bias
 |  0.000 | -0.690 |  0.709 |  0.138 | torch.Size([180, 180]) || layers.4.residual_group.blocks.2.attn.proj.weight
 |  0.020 | -0.249 |  0.275 |  0.112 | torch.Size([180]) || layers.4.residual_group.blocks.2.attn.proj.bias
 |  1.706 |  1.142 |  2.105 |  0.179 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.weight
 |  0.002 | -0.119 |  0.155 |  0.055 | torch.Size([180]) || layers.4.residual_group.blocks.2.norm2.bias
 |  0.002 | -0.723 |  0.652 |  0.145 | torch.Size([360, 180]) || layers.4.residual_group.blocks.2.mlp.fc1.weight
 | -0.034 | -0.222 |  0.149 |  0.073 | torch.Size([360]) || layers.4.residual_group.blocks.2.mlp.fc1.bias
 | -0.002 | -0.817 |  0.969 |  0.171 | torch.Size([180, 360]) || layers.4.residual_group.blocks.2.mlp.fc2.weight
 | -0.003 | -0.215 |  0.229 |  0.075 | torch.Size([180]) || layers.4.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.3.attn_mask
 |  1.036 |  0.539 |  1.348 |  0.132 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.weight
 |  0.010 | -0.382 |  0.388 |  0.145 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm1.bias
 | -0.030 | -1.812 |  1.674 |  0.260 | torch.Size([225, 6]) || layers.4.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.642 |  0.601 |  0.100 | torch.Size([540, 180]) || layers.4.residual_group.blocks.3.attn.qkv.weight
 |  0.001 | -0.209 |  0.202 |  0.068 | torch.Size([540]) || layers.4.residual_group.blocks.3.attn.qkv.bias
 |  0.000 | -0.696 |  0.625 |  0.134 | torch.Size([180, 180]) || layers.4.residual_group.blocks.3.attn.proj.weight
 |  0.011 | -0.234 |  0.262 |  0.120 | torch.Size([180]) || layers.4.residual_group.blocks.3.attn.proj.bias
 |  1.629 |  0.971 |  1.992 |  0.184 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.weight
 |  0.003 | -0.170 |  0.110 |  0.054 | torch.Size([180]) || layers.4.residual_group.blocks.3.norm2.bias
 |  0.001 | -0.730 |  0.709 |  0.141 | torch.Size([360, 180]) || layers.4.residual_group.blocks.3.mlp.fc1.weight
 | -0.027 | -0.200 |  0.161 |  0.063 | torch.Size([360]) || layers.4.residual_group.blocks.3.mlp.fc1.bias
 | -0.001 | -0.770 |  0.956 |  0.181 | torch.Size([180, 360]) || layers.4.residual_group.blocks.3.mlp.fc2.weight
 | -0.000 | -0.251 |  0.258 |  0.084 | torch.Size([180]) || layers.4.residual_group.blocks.3.mlp.fc2.bias
 |  1.027 |  0.508 |  1.315 |  0.121 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.weight
 |  0.009 | -0.468 |  0.424 |  0.166 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm1.bias
 | -0.032 | -1.941 |  1.758 |  0.270 | torch.Size([225, 6]) || layers.4.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.4.attn.relative_position_index
 | -0.000 | -0.617 |  0.713 |  0.102 | torch.Size([540, 180]) || layers.4.residual_group.blocks.4.attn.qkv.weight
 |  0.003 | -0.207 |  0.179 |  0.065 | torch.Size([540]) || layers.4.residual_group.blocks.4.attn.qkv.bias
 |  0.000 | -0.720 |  0.829 |  0.140 | torch.Size([180, 180]) || layers.4.residual_group.blocks.4.attn.proj.weight
 |  0.011 | -0.217 |  0.269 |  0.129 | torch.Size([180]) || layers.4.residual_group.blocks.4.attn.proj.bias
 |  1.554 |  1.060 |  2.007 |  0.158 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.weight
 |  0.005 | -0.234 |  0.175 |  0.068 | torch.Size([180]) || layers.4.residual_group.blocks.4.norm2.bias
 |  0.001 | -0.651 |  0.656 |  0.138 | torch.Size([360, 180]) || layers.4.residual_group.blocks.4.mlp.fc1.weight
 | -0.015 | -0.290 |  0.103 |  0.050 | torch.Size([360]) || layers.4.residual_group.blocks.4.mlp.fc1.bias
 | -0.002 | -0.982 |  1.152 |  0.193 | torch.Size([180, 360]) || layers.4.residual_group.blocks.4.mlp.fc2.weight
 |  0.003 | -0.238 |  0.244 |  0.078 | torch.Size([180]) || layers.4.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.4.residual_group.blocks.5.attn_mask
 |  1.090 |  0.625 |  1.320 |  0.118 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.weight
 |  0.006 | -0.340 |  0.338 |  0.168 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm1.bias
 | -0.031 | -2.376 |  1.739 |  0.254 | torch.Size([225, 6]) || layers.4.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.4.residual_group.blocks.5.attn.relative_position_index
 | -0.000 | -0.768 |  0.627 |  0.107 | torch.Size([540, 180]) || layers.4.residual_group.blocks.5.attn.qkv.weight
 |  0.001 | -0.204 |  0.213 |  0.063 | torch.Size([540]) || layers.4.residual_group.blocks.5.attn.qkv.bias
 |  0.001 | -0.735 |  0.719 |  0.150 | torch.Size([180, 180]) || layers.4.residual_group.blocks.5.attn.proj.weight
 |  0.011 | -0.318 |  0.314 |  0.169 | torch.Size([180]) || layers.4.residual_group.blocks.5.attn.proj.bias
 |  1.526 |  0.994 |  1.948 |  0.174 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.weight
 |  0.009 | -0.274 |  0.241 |  0.097 | torch.Size([180]) || layers.4.residual_group.blocks.5.norm2.bias
 |  0.001 | -0.819 |  0.743 |  0.138 | torch.Size([360, 180]) || layers.4.residual_group.blocks.5.mlp.fc1.weight
 | -0.007 | -0.170 |  0.133 |  0.048 | torch.Size([360]) || layers.4.residual_group.blocks.5.mlp.fc1.bias
 | -0.000 | -1.069 |  1.059 |  0.195 | torch.Size([180, 360]) || layers.4.residual_group.blocks.5.mlp.fc2.weight
 |  0.006 | -0.243 |  0.183 |  0.065 | torch.Size([180]) || layers.4.residual_group.blocks.5.mlp.fc2.bias
 | -0.000 | -0.487 |  0.580 |  0.086 | torch.Size([180, 180, 3, 3]) || layers.4.conv.weight
 | -0.019 | -0.589 |  0.592 |  0.249 | torch.Size([180]) || layers.4.conv.bias
 |  1.006 |  0.723 |  1.244 |  0.110 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.weight
 |  0.000 | -0.443 |  0.399 |  0.173 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm1.bias
 | -0.001 | -0.150 |  0.190 |  0.040 | torch.Size([225, 6]) || layers.5.residual_group.blocks.0.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.0.attn.relative_position_index
 |  0.000 | -0.373 |  0.382 |  0.059 | torch.Size([540, 180]) || layers.5.residual_group.blocks.0.attn.qkv.weight
 | -0.004 | -0.166 |  0.162 |  0.067 | torch.Size([540]) || layers.5.residual_group.blocks.0.attn.qkv.bias
 |  0.000 | -0.307 |  0.335 |  0.060 | torch.Size([180, 180]) || layers.5.residual_group.blocks.0.attn.proj.weight
 | -0.003 | -0.187 |  0.197 |  0.080 | torch.Size([180]) || layers.5.residual_group.blocks.0.attn.proj.bias
 |  1.343 |  0.853 |  2.092 |  0.239 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.weight
 | -0.003 | -0.460 |  0.484 |  0.234 | torch.Size([180]) || layers.5.residual_group.blocks.0.norm2.bias
 | -0.003 | -0.640 |  0.648 |  0.118 | torch.Size([360, 180]) || layers.5.residual_group.blocks.0.mlp.fc1.weight
 |  0.083 | -0.023 |  0.253 |  0.056 | torch.Size([360]) || layers.5.residual_group.blocks.0.mlp.fc1.bias
 |  0.001 | -0.740 |  0.752 |  0.124 | torch.Size([180, 360]) || layers.5.residual_group.blocks.0.mlp.fc2.weight
 | -0.002 | -0.182 |  0.153 |  0.070 | torch.Size([180]) || layers.5.residual_group.blocks.0.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.1.attn_mask
 |  1.000 |  0.712 |  1.330 |  0.108 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.weight
 | -0.002 | -0.505 |  0.367 |  0.182 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm1.bias
 | -0.003 | -0.152 |  0.765 |  0.056 | torch.Size([225, 6]) || layers.5.residual_group.blocks.1.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.1.attn.relative_position_index
 |  0.000 | -0.374 |  0.386 |  0.060 | torch.Size([540, 180]) || layers.5.residual_group.blocks.1.attn.qkv.weight
 | -0.001 | -0.140 |  0.135 |  0.072 | torch.Size([540]) || layers.5.residual_group.blocks.1.attn.qkv.bias
 |  0.000 | -0.314 |  0.361 |  0.062 | torch.Size([180, 180]) || layers.5.residual_group.blocks.1.attn.proj.weight
 | -0.002 | -0.196 |  0.203 |  0.090 | torch.Size([180]) || layers.5.residual_group.blocks.1.attn.proj.bias
 |  1.317 |  0.857 |  2.040 |  0.232 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.weight
 | -0.001 | -0.438 |  0.448 |  0.225 | torch.Size([180]) || layers.5.residual_group.blocks.1.norm2.bias
 | -0.002 | -0.788 |  0.705 |  0.114 | torch.Size([360, 180]) || layers.5.residual_group.blocks.1.mlp.fc1.weight
 |  0.081 | -0.031 |  0.247 |  0.052 | torch.Size([360]) || layers.5.residual_group.blocks.1.mlp.fc1.bias
 |  0.001 | -0.687 |  0.942 |  0.122 | torch.Size([180, 360]) || layers.5.residual_group.blocks.1.mlp.fc2.weight
 | -0.001 | -0.191 |  0.167 |  0.075 | torch.Size([180]) || layers.5.residual_group.blocks.1.mlp.fc2.bias
 |  1.004 |  0.718 |  1.259 |  0.107 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.weight
 | -0.004 | -0.326 |  0.359 |  0.159 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm1.bias
 |  0.001 | -0.137 |  0.234 |  0.034 | torch.Size([225, 6]) || layers.5.residual_group.blocks.2.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.2.attn.relative_position_index
 | -0.000 | -0.374 |  0.357 |  0.059 | torch.Size([540, 180]) || layers.5.residual_group.blocks.2.attn.qkv.weight
 |  0.004 | -0.143 |  0.140 |  0.068 | torch.Size([540]) || layers.5.residual_group.blocks.2.attn.qkv.bias
 | -0.000 | -0.416 |  0.344 |  0.062 | torch.Size([180, 180]) || layers.5.residual_group.blocks.2.attn.proj.weight
 | -0.003 | -0.200 |  0.210 |  0.100 | torch.Size([180]) || layers.5.residual_group.blocks.2.attn.proj.bias
 |  1.285 |  0.825 |  1.879 |  0.221 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.weight
 | -0.001 | -0.485 |  0.432 |  0.227 | torch.Size([180]) || layers.5.residual_group.blocks.2.norm2.bias
 | -0.002 | -0.747 |  0.619 |  0.110 | torch.Size([360, 180]) || layers.5.residual_group.blocks.2.mlp.fc1.weight
 |  0.078 | -0.031 |  0.212 |  0.048 | torch.Size([360]) || layers.5.residual_group.blocks.2.mlp.fc1.bias
 |  0.000 | -0.690 |  0.864 |  0.121 | torch.Size([180, 360]) || layers.5.residual_group.blocks.2.mlp.fc2.weight
 | -0.001 | -0.162 |  0.170 |  0.071 | torch.Size([180]) || layers.5.residual_group.blocks.2.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.3.attn_mask
 |  0.999 |  0.692 |  1.278 |  0.111 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.weight
 |  0.013 | -0.489 |  0.511 |  0.194 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm1.bias
 | -0.001 | -0.136 |  0.543 |  0.045 | torch.Size([225, 6]) || layers.5.residual_group.blocks.3.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.3.attn.relative_position_index
 |  0.000 | -0.365 |  0.358 |  0.057 | torch.Size([540, 180]) || layers.5.residual_group.blocks.3.attn.qkv.weight
 | -0.002 | -0.153 |  0.146 |  0.069 | torch.Size([540]) || layers.5.residual_group.blocks.3.attn.qkv.bias
 | -0.000 | -0.247 |  0.289 |  0.060 | torch.Size([180, 180]) || layers.5.residual_group.blocks.3.attn.proj.weight
 | -0.001 | -0.201 |  0.206 |  0.101 | torch.Size([180]) || layers.5.residual_group.blocks.3.attn.proj.bias
 |  1.256 |  0.769 |  1.822 |  0.214 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.weight
 |  0.001 | -0.430 |  0.453 |  0.225 | torch.Size([180]) || layers.5.residual_group.blocks.3.norm2.bias
 | -0.002 | -0.618 |  0.667 |  0.107 | torch.Size([360, 180]) || layers.5.residual_group.blocks.3.mlp.fc1.weight
 |  0.083 | -0.024 |  0.202 |  0.043 | torch.Size([360]) || layers.5.residual_group.blocks.3.mlp.fc1.bias
 |  0.001 | -0.653 |  0.794 |  0.120 | torch.Size([180, 360]) || layers.5.residual_group.blocks.3.mlp.fc2.weight
 | -0.001 | -0.158 |  0.178 |  0.072 | torch.Size([180]) || layers.5.residual_group.blocks.3.mlp.fc2.bias
 |  0.988 |  0.564 |  1.315 |  0.119 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.weight
 |  0.006 | -0.416 |  0.382 |  0.179 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm1.bias
 |  0.001 | -0.214 |  0.220 |  0.035 | torch.Size([225, 6]) || layers.5.residual_group.blocks.4.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.4.attn.relative_position_index
 |  0.000 | -0.369 |  0.364 |  0.054 | torch.Size([540, 180]) || layers.5.residual_group.blocks.4.attn.qkv.weight
 | -0.003 | -0.159 |  0.131 |  0.063 | torch.Size([540]) || layers.5.residual_group.blocks.4.attn.qkv.bias
 | -0.000 | -0.257 |  0.273 |  0.054 | torch.Size([180, 180]) || layers.5.residual_group.blocks.4.attn.proj.weight
 | -0.001 | -0.196 |  0.200 |  0.096 | torch.Size([180]) || layers.5.residual_group.blocks.4.attn.proj.bias
 |  1.225 |  0.793 |  1.827 |  0.193 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.weight
 | -0.007 | -0.460 |  0.386 |  0.225 | torch.Size([180]) || layers.5.residual_group.blocks.4.norm2.bias
 | -0.003 | -0.600 |  0.665 |  0.106 | torch.Size([360, 180]) || layers.5.residual_group.blocks.4.mlp.fc1.weight
 |  0.081 | -0.022 |  0.186 |  0.046 | torch.Size([360]) || layers.5.residual_group.blocks.4.mlp.fc1.bias
 |  0.001 | -0.770 |  0.726 |  0.121 | torch.Size([180, 360]) || layers.5.residual_group.blocks.4.mlp.fc2.weight
 | -0.001 | -0.179 |  0.178 |  0.072 | torch.Size([180]) || layers.5.residual_group.blocks.4.mlp.fc2.bias
 | -15.972 | -100.000 |  0.000 | 36.635 | torch.Size([36, 64, 64]) || layers.5.residual_group.blocks.5.attn_mask
 |  1.001 |  0.754 |  1.270 |  0.104 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.weight
 | -0.001 | -0.473 |  0.661 |  0.198 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm1.bias
 | -0.003 | -0.163 |  0.504 |  0.048 | torch.Size([225, 6]) || layers.5.residual_group.blocks.5.attn.relative_position_bias_table
 | 112.000 |  0.000 | 224.000 | 48.719 | torch.Size([64, 64]) || layers.5.residual_group.blocks.5.attn.relative_position_index
 |  0.000 | -0.331 |  0.356 |  0.059 | torch.Size([540, 180]) || layers.5.residual_group.blocks.5.attn.qkv.weight
 | -0.002 | -0.143 |  0.142 |  0.067 | torch.Size([540]) || layers.5.residual_group.blocks.5.attn.qkv.bias
 |  0.000 | -0.270 |  0.256 |  0.060 | torch.Size([180, 180]) || layers.5.residual_group.blocks.5.attn.proj.weight
 | -0.001 | -0.219 |  0.221 |  0.103 | torch.Size([180]) || layers.5.residual_group.blocks.5.attn.proj.bias
 |  1.238 |  0.766 |  1.726 |  0.192 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.weight
 | -0.003 | -0.419 |  0.377 |  0.216 | torch.Size([180]) || layers.5.residual_group.blocks.5.norm2.bias
 | -0.002 | -0.553 |  0.591 |  0.103 | torch.Size([360, 180]) || layers.5.residual_group.blocks.5.mlp.fc1.weight
 |  0.073 | -0.020 |  0.174 |  0.042 | torch.Size([360]) || layers.5.residual_group.blocks.5.mlp.fc1.bias
 |  0.001 | -0.654 |  0.694 |  0.119 | torch.Size([180, 360]) || layers.5.residual_group.blocks.5.mlp.fc2.weight
 | -0.000 | -0.193 |  0.210 |  0.077 | torch.Size([180]) || layers.5.residual_group.blocks.5.mlp.fc2.bias
 |  0.000 | -0.847 |  0.800 |  0.088 | torch.Size([180, 180, 3, 3]) || layers.5.conv.weight
 |  0.004 | -0.326 |  0.625 |  0.063 | torch.Size([180]) || layers.5.conv.bias
 |  0.222 |  0.016 |  0.927 |  0.147 | torch.Size([180]) || norm.weight
 |  0.021 | -0.800 |  0.347 |  0.075 | torch.Size([180]) || norm.bias
 | -0.000 | -0.482 |  0.491 |  0.075 | torch.Size([180, 180, 3, 3]) || conv_after_body.weight
 | -0.002 | -0.139 |  0.086 |  0.036 | torch.Size([180]) || conv_after_body.bias
 |  0.000 | -0.428 |  0.402 |  0.061 | torch.Size([64, 180, 3, 3]) || conv_before_upsample.0.weight
 | -0.062 | -0.153 |  0.074 |  0.056 | torch.Size([64]) || conv_before_upsample.0.bias
 |  0.000 | -0.693 |  0.662 |  0.062 | torch.Size([256, 64, 3, 3]) || upsample.0.weight
 |  0.000 | -0.080 |  0.068 |  0.017 | torch.Size([256]) || upsample.0.bias
 |  0.000 | -0.267 |  0.273 |  0.031 | torch.Size([256, 64, 3, 3]) || upsample.2.weight
 | -0.000 | -0.043 |  0.034 |  0.008 | torch.Size([256]) || upsample.2.bias
 | -0.000 | -0.316 |  0.357 |  0.025 | torch.Size([256, 64, 3, 3]) || upsample.4.weight
 | -0.001 | -0.084 |  0.045 |  0.011 | torch.Size([256]) || upsample.4.bias
 | -0.000 | -0.053 |  0.045 |  0.006 | torch.Size([3, 64, 3, 3]) || conv_last.weight
 |  0.051 |  0.041 |  0.069 |  0.016 | torch.Size([3]) || conv_last.bias

22-11-09 23:03:29.844 : ---1-->   0001.jpg | 25.32dB
22-11-09 23:03:39.964 : ---2-->   0002.jpg | 25.91dB
22-11-09 23:03:46.337 : ---3-->   0003.jpg | 29.95dB
22-11-09 23:03:52.590 : ---4-->   0004.jpg | 28.52dB
22-11-09 23:03:58.929 : ---5-->   0005.jpg | 29.34dB
22-11-09 23:04:00.827 : ---6-->   0006.jpg | 33.48dB
22-11-09 23:04:05.106 : ---7-->   0007.jpg | 35.67dB
22-11-09 23:04:10.581 : ---8-->   0008.jpg | 24.32dB
22-11-09 23:04:17.108 : ---9-->   0009.jpg | 23.54dB
22-11-09 23:04:31.013 : --10-->   0010.jpg | 28.45dB
22-11-09 23:04:34.169 : --11-->   0011.jpg | 26.48dB
22-11-09 23:04:41.729 : --12-->   0012.jpg | 24.69dB
